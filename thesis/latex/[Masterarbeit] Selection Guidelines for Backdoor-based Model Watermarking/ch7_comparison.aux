\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Empirical comparison of existing watermarking methods}{61}{chapter.7}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:empirical_comparison}{{\M@TitleReference {7}{Empirical comparison of existing watermarking methods}}{61}{Empirical comparison of existing watermarking methods}{chapter.7}{}}
\newlabel{ch:empirical_comparison@cref}{{[chapter][7][]7}{[1][61][]61}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Implementation}{61}{section.7.1}\protected@file@percent }
\newlabel{sec:implementation}{{\M@TitleReference {7.1}{Implementation}}{61}{Implementation}{section.7.1}{}}
\newlabel{sec:implementation@cref}{{[section][1][7]7.1}{[1][61][]61}}
\citation{adi_turning_2018}
\citation{zhang_protecting_2018}
\citation{li_piracy_2020}
\citation{namba_robust_2019}
\citation{merrer_adversarial_2019}
\citation{guo_watermarking_2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Framework for watermarking methods}{62}{subsection.7.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {7.1}{\ignorespaces General framework\relax }}{62}{algocf.7.1}\protected@file@percent }
\newlabel{algo:framework}{{\M@TitleReference {7.1}{Framework for watermarking methods}}{62}{Framework for watermarking methods}{algocf.7.1}{}}
\newlabel{algo:framework@cref}{{[algorithm][1][7]7.1}{[1][61][]62}}
\@writefile{toc}{\contentsline {subsubsection}{WeaknessIntoStrength}{63}{section*.44}\protected@file@percent }
\newlabel{sec:weakness}{{\M@TitleReference {7.1.1}{WeaknessIntoStrength}}{63}{WeaknessIntoStrength}{section*.44}{}}
\newlabel{sec:weakness@cref}{{[subsection][1][7,1]7.1.1}{[1][63][]63}}
\@writefile{toc}{\contentsline {subsubsection}{ProtectingIP}{64}{section*.45}\protected@file@percent }
\newlabel{sec:protecting}{{\M@TitleReference {7.1.1}{ProtectingIP}}{64}{ProtectingIP}{section*.45}{}}
\newlabel{sec:protecting@cref}{{[subsection][1][7,1]7.1.1}{[1][63][]64}}
\@writefile{toc}{\contentsline {subsubsection}{PiracyResistant}{64}{section*.46}\protected@file@percent }
\newlabel{sec:piracy}{{\M@TitleReference {7.1.1}{PiracyResistant}}{64}{PiracyResistant}{section*.46}{}}
\newlabel{sec:piracy@cref}{{[subsection][1][7,1]7.1.1}{[1][64][]64}}
\@writefile{toc}{\contentsline {subsubsection}{ExponentialWeighting}{64}{section*.47}\protected@file@percent }
\newlabel{sec:exponential}{{\M@TitleReference {7.1.1}{ExponentialWeighting}}{64}{ExponentialWeighting}{section*.47}{}}
\newlabel{sec:exponential@cref}{{[subsection][1][7,1]7.1.1}{[1][64][]64}}
\citation{zhang_protecting_2018}
\citation{merrer_adversarial_2019}
\citation{rouhani_deepsigns_2019}
\@writefile{toc}{\contentsline {subsubsection}{FrontierStitching}{65}{section*.48}\protected@file@percent }
\newlabel{sec:frontier}{{\M@TitleReference {7.1.1}{FrontierStitching}}{65}{FrontierStitching}{section*.48}{}}
\newlabel{sec:frontier@cref}{{[subsection][1][7,1]7.1.1}{[1][65][]65}}
\@writefile{toc}{\contentsline {subsubsection}{WMEmbeddedSystems}{65}{section*.49}\protected@file@percent }
\newlabel{sec:embedded}{{\M@TitleReference {7.1.1}{WMEmbeddedSystems}}{65}{WMEmbeddedSystems}{section*.49}{}}
\newlabel{sec:embedded@cref}{{[subsection][1][7,1]7.1.1}{[1][65][]65}}
\citation{adi_turning_2018}
\citation{zhang_protecting_2018}
\citation{li_piracy_2020}
\citation{wang_neural_2019}
\citation{tramer_stealing_2016}
\citation{namba_robust_2019}
\citation{namba_robust_2019}
\citation{merrer_adversarial_2019}
\citation{guo_watermarking_2018}
\citation{liu_trojaning_2017}
\citation{liu_fine-pruning_2018}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Attacks used in the papers.\relax }}{66}{table.caption.50}\protected@file@percent }
\newlabel{tab:attacks_per_method}{{\M@TitleReference {7.1}{Attacks used in the papers.\relax }}{66}{Attacks used in the papers.\relax }{table.caption.50}{}}
\newlabel{tab:attacks_per_method@cref}{{[table][1][7]7.1}{[1][66][]66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Attacks}{66}{subsection.7.1.2}\protected@file@percent }
\newlabel{sec:implementation_attack}{{\M@TitleReference {7.1.2}{Attacks}}{66}{Attacks}{subsection.7.1.2}{}}
\newlabel{sec:implementation_attack@cref}{{[subsection][2][7,1]7.1.2}{[1][65][]66}}
\@writefile{toc}{\contentsline {subsubsection}{Parameter Pruning}{66}{section*.52}\protected@file@percent }
\newlabel{sec:implementation:pruning}{{\M@TitleReference {7.1.2}{Parameter Pruning}}{66}{Parameter Pruning}{section*.52}{}}
\newlabel{sec:implementation:pruning@cref}{{[subsection][2][7,1]7.1.2}{[1][66][]66}}
\citation{liu_fine-pruning_2018}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning}{67}{section*.53}\protected@file@percent }
\newlabel{sec:finetuning}{{\M@TitleReference {7.1.2}{Fine-Tuning}}{67}{Fine-Tuning}{section*.53}{}}
\newlabel{sec:finetuning@cref}{{[subsection][2][7,1]7.1.2}{[1][67][]67}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Evaluation}{67}{section.7.2}\protected@file@percent }
\newlabel{sec:evaluation}{{\M@TitleReference {7.2}{Evaluation}}{67}{Evaluation}{section.7.2}{}}
\newlabel{sec:evaluation@cref}{{[section][2][7]7.2}{[1][67][]67}}
\newlabel{fig:finetuning_simplenet_lastlayer}{{\M@TitleReference {7.1a}{SimpleNet, only last layers\relax }}{68}{SimpleNet, only last layers\relax }{figure.caption.54}{}}
\newlabel{fig:finetuning_simplenet_lastlayer@cref}{{[subfigure][1][7,1]7.1a}{[1][68][]68}}
\newlabel{sub@fig:finetuning_simplenet_lastlayer}{{\M@TitleReference {a}{SimpleNet, only last layers\relax }}{68}{SimpleNet, only last layers\relax }{figure.caption.54}{}}
\newlabel{sub@fig:finetuning_simplenet_lastlayer@cref}{{[subfigure][1][7,1]7.1a}{[1][68][]68}}
\newlabel{fig:finetuning_simplenet_alllayers}{{\M@TitleReference {7.1b}{SimpleNet, all layers\relax }}{68}{SimpleNet, all layers\relax }{figure.caption.54}{}}
\newlabel{fig:finetuning_simplenet_alllayers@cref}{{[subfigure][2][7,1]7.1b}{[1][68][]68}}
\newlabel{sub@fig:finetuning_simplenet_alllayers}{{\M@TitleReference {b}{SimpleNet, all layers\relax }}{68}{SimpleNet, all layers\relax }{figure.caption.54}{}}
\newlabel{sub@fig:finetuning_simplenet_alllayers@cref}{{[subfigure][2][7,1]7.1b}{[1][68][]68}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Fine-tuning on non-watermarked SimpleNet. The plot on the left side correspond to fine-tuning only the last layer and the one on the right hand side to fine-tuning all layers. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }}{68}{figure.caption.54}\protected@file@percent }
\newlabel{fig:finetuning_all_vs_last_layers}{{\M@TitleReference {7.1}{Fine-tuning on non-watermarked SimpleNet. The plot on the left side correspond to fine-tuning only the last layer and the one on the right hand side to fine-tuning all layers. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }}{68}{Fine-tuning on non-watermarked SimpleNet. The plot on the left side correspond to fine-tuning only the last layer and the one on the right hand side to fine-tuning all layers. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }{figure.caption.54}{}}
\newlabel{fig:finetuning_all_vs_last_layers@cref}{{[figure][1][7]7.1}{[1][68][]68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Evaluation of Fine-Tuning}{68}{subsection.7.2.1}\protected@file@percent }
\newlabel{sec:finetuning-experiments}{{\M@TitleReference {7.2.1}{Evaluation of Fine-Tuning}}{68}{Evaluation of Fine-Tuning}{subsection.7.2.1}{}}
\newlabel{sec:finetuning-experiments@cref}{{[subsection][1][7,2]7.2.1}{[1][68][]68}}
\citation{kirkpatrick_overcoming_2017}
\newlabel{fig:fine-tuning-cinic10}{{\M@TitleReference {7.2a}{Fine-Tuning on CINIC-10\relax }}{69}{Fine-Tuning on CINIC-10\relax }{figure.caption.55}{}}
\newlabel{fig:fine-tuning-cinic10@cref}{{[subfigure][1][7,2]7.2a}{[1][69][]69}}
\newlabel{sub@fig:fine-tuning-cinic10}{{\M@TitleReference {a}{Fine-Tuning on CINIC-10\relax }}{69}{Fine-Tuning on CINIC-10\relax }{figure.caption.55}{}}
\newlabel{sub@fig:fine-tuning-cinic10@cref}{{[subfigure][1][7,2]7.2a}{[1][69][]69}}
\newlabel{fig:fine-tuning-cinic10-imagenet}{{\M@TitleReference {7.2b}{Fine-Tuning on ImageNet part of CINIC-10\relax }}{69}{Fine-Tuning on ImageNet part of CINIC-10\relax }{figure.caption.55}{}}
\newlabel{fig:fine-tuning-cinic10-imagenet@cref}{{[subfigure][2][7,2]7.2b}{[1][69][]69}}
\newlabel{sub@fig:fine-tuning-cinic10-imagenet}{{\M@TitleReference {b}{Fine-Tuning on ImageNet part of CINIC-10\relax }}{69}{Fine-Tuning on ImageNet part of CINIC-10\relax }{figure.caption.55}{}}
\newlabel{sub@fig:fine-tuning-cinic10-imagenet@cref}{{[subfigure][2][7,2]7.2b}{[1][69][]69}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Fine-Tuning on both, CINIC-10 and only on the ImageNet part of CINIC-10. In both cases, 50,000 images are randomly chosen from the corresponding dataset. The underlying model is a ResNet-18 that was trained with \textit  {ProtectingIP-pattern} and 100 trigger images. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model. For clearity reasons, the lines in the plot are smoothed. The original plots are provided in \cref  {fig:fine-tuning-both-cinic10-imagenet-original}.\relax }}{69}{figure.caption.55}\protected@file@percent }
\newlabel{fig:fine-tuning-both-cinic10-imagenet}{{\M@TitleReference {7.2}{Fine-Tuning on both, CINIC-10 and only on the ImageNet part of CINIC-10. In both cases, 50,000 images are randomly chosen from the corresponding dataset. The underlying model is a ResNet-18 that was trained with \textit  {ProtectingIP-pattern} and 100 trigger images. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model. For clearity reasons, the lines in the plot are smoothed. The original plots are provided in \cref  {fig:fine-tuning-both-cinic10-imagenet-original}.\relax }}{69}{Fine-Tuning on both, CINIC-10 and only on the ImageNet part of CINIC-10. In both cases, 50,000 images are randomly chosen from the corresponding dataset. The underlying model is a ResNet-18 that was trained with \textit {ProtectingIP-pattern} and 100 trigger images. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model. For clearity reasons, the lines in the plot are smoothed. The original plots are provided in \cref {fig:fine-tuning-both-cinic10-imagenet-original}.\relax }{figure.caption.55}{}}
\newlabel{fig:fine-tuning-both-cinic10-imagenet@cref}{{[figure][2][7]7.2}{[1][69][]69}}
\newlabel{fig:finetuning_simplenet_smalllr}{{\M@TitleReference {7.3a}{SimpleNet, small learning rates (MNIST)\relax }}{70}{SimpleNet, small learning rates (MNIST)\relax }{figure.caption.56}{}}
\newlabel{fig:finetuning_simplenet_smalllr@cref}{{[subfigure][1][7,3]7.3a}{[1][69][]70}}
\newlabel{sub@fig:finetuning_simplenet_smalllr}{{\M@TitleReference {a}{SimpleNet, small learning rates (MNIST)\relax }}{70}{SimpleNet, small learning rates (MNIST)\relax }{figure.caption.56}{}}
\newlabel{sub@fig:finetuning_simplenet_smalllr@cref}{{[subfigure][1][7,3]7.3a}{[1][69][]70}}
\newlabel{fig:finetuning_simplenet_largelr}{{\M@TitleReference {7.3b}{SimpleNet, large learning rates (MNIST)\relax }}{70}{SimpleNet, large learning rates (MNIST)\relax }{figure.caption.56}{}}
\newlabel{fig:finetuning_simplenet_largelr@cref}{{[subfigure][2][7,3]7.3b}{[1][69][]70}}
\newlabel{sub@fig:finetuning_simplenet_largelr}{{\M@TitleReference {b}{SimpleNet, large learning rates (MNIST)\relax }}{70}{SimpleNet, large learning rates (MNIST)\relax }{figure.caption.56}{}}
\newlabel{sub@fig:finetuning_simplenet_largelr@cref}{{[subfigure][2][7,3]7.3b}{[1][69][]70}}
\newlabel{fig:finetuning_densenet_smalllr}{{\M@TitleReference {7.3c}{DenseNet, small learning rates (CIFAR-10)\relax }}{70}{DenseNet, small learning rates (CIFAR-10)\relax }{figure.caption.56}{}}
\newlabel{fig:finetuning_densenet_smalllr@cref}{{[subfigure][3][7,3]7.3c}{[1][69][]70}}
\newlabel{sub@fig:finetuning_densenet_smalllr}{{\M@TitleReference {c}{DenseNet, small learning rates (CIFAR-10)\relax }}{70}{DenseNet, small learning rates (CIFAR-10)\relax }{figure.caption.56}{}}
\newlabel{sub@fig:finetuning_densenet_smalllr@cref}{{[subfigure][3][7,3]7.3c}{[1][69][]70}}
\newlabel{fig:finetuning_densenet_largelr}{{\M@TitleReference {7.3d}{DenseNet, large learning rates (CIFAR-10)\relax }}{70}{DenseNet, large learning rates (CIFAR-10)\relax }{figure.caption.56}{}}
\newlabel{fig:finetuning_densenet_largelr@cref}{{[subfigure][4][7,3]7.3d}{[1][69][]70}}
\newlabel{sub@fig:finetuning_densenet_largelr}{{\M@TitleReference {d}{DenseNet, large learning rates (CIFAR-10)\relax }}{70}{DenseNet, large learning rates (CIFAR-10)\relax }{figure.caption.56}{}}
\newlabel{sub@fig:finetuning_densenet_largelr@cref}{{[subfigure][4][7,3]7.3d}{[1][69][]70}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Fine-tuning on SimpleNet and DenseNet, watermarked with ProtectingIP-pattern. The plots on the left side correspond to fine-tuning with smaller learning rates and the ones on the right side to fine-tuning with larger learning rates. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }}{70}{figure.caption.56}\protected@file@percent }
\newlabel{fig:finetuning_simplenet_and_densenet}{{\M@TitleReference {7.3}{Fine-tuning on SimpleNet and DenseNet, watermarked with ProtectingIP-pattern. The plots on the left side correspond to fine-tuning with smaller learning rates and the ones on the right side to fine-tuning with larger learning rates. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }}{70}{Fine-tuning on SimpleNet and DenseNet, watermarked with ProtectingIP-pattern. The plots on the left side correspond to fine-tuning with smaller learning rates and the ones on the right side to fine-tuning with larger learning rates. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }{figure.caption.56}{}}
\newlabel{fig:finetuning_simplenet_and_densenet@cref}{{[figure][3][7]7.3}{[1][69][]70}}
\newlabel{fig:frontier_eps0.0001}{{\M@TitleReference {\caption@xref {fig:frontier_eps0.0001}{ on input line 4}}{FrontierStitching}}{71}{FrontierStitching}{figure.caption.58}{}}
\newlabel{fig:frontier_eps0.0001@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\newlabel{fig:frontier_eps0.001}{{\M@TitleReference {\caption@xref {fig:frontier_eps0.001}{ on input line 6}}{FrontierStitching}}{71}{FrontierStitching}{figure.caption.58}{}}
\newlabel{fig:frontier_eps0.001@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\newlabel{fig:frontier_eps0.01}{{\M@TitleReference {\caption@xref {fig:frontier_eps0.01}{ on input line 8}}{FrontierStitching}}{71}{FrontierStitching}{figure.caption.58}{}}
\newlabel{fig:frontier_eps0.01@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\newlabel{fig:frontier_eps0.1}{{\M@TitleReference {\caption@xref {fig:frontier_eps0.1}{ on input line 10}}{FrontierStitching}}{71}{FrontierStitching}{figure.caption.58}{}}
\newlabel{fig:frontier_eps0.1@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\newlabel{fig:frontier_eps0.25}{{\M@TitleReference {\caption@xref {fig:frontier_eps0.25}{ on input line 12}}{FrontierStitching}}{71}{FrontierStitching}{figure.caption.58}{}}
\newlabel{fig:frontier_eps0.25@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\newlabel{fig:frontier_eps0.5}{{\M@TitleReference {\caption@xref {fig:frontier_eps0.5}{ on input line 14}}{FrontierStitching}}{71}{FrontierStitching}{figure.caption.58}{}}
\newlabel{fig:frontier_eps0.5@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\newlabel{fig:frontier_eps1.0}{{\M@TitleReference {\caption@xref {fig:frontier_eps1.0}{ on input line 16}}{FrontierStitching}}{71}{FrontierStitching}{figure.caption.58}{}}
\newlabel{fig:frontier_eps1.0@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Examples for FrontierStitching trigger images for different values of $\epsilon $, created with FGSM on LeNet-1.\relax }}{71}{figure.caption.58}\protected@file@percent }
\newlabel{fig:frontier_trigger-images}{{\M@TitleReference {7.4}{Examples for FrontierStitching trigger images for different values of $\epsilon $, created with FGSM on LeNet-1.\relax }}{71}{Examples for FrontierStitching trigger images for different values of $\epsilon $, created with FGSM on LeNet-1.\relax }{figure.caption.58}{}}
\newlabel{fig:frontier_trigger-images@cref}{{[figure][4][7]7.4}{[1][71][]71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Influence of watermark-specific hyparameters}{71}{subsection.7.2.2}\protected@file@percent }
\newlabel{sec:eval-watermark-spec-param}{{\M@TitleReference {7.2.2}{Influence of watermark-specific hyparameters}}{71}{Influence of watermark-specific hyparameters}{subsection.7.2.2}{}}
\newlabel{sec:eval-watermark-spec-param@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\@writefile{toc}{\contentsline {subsubsection}{FrontierStitching}{71}{section*.57}\protected@file@percent }
\newlabel{sec:eval-param:frontier}{{\M@TitleReference {7.2.2}{FrontierStitching}}{71}{FrontierStitching}{section*.57}{}}
\newlabel{sec:eval-param:frontier@cref}{{[subsection][2][7,2]7.2.2}{[1][71][]71}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces FrontierStitching with various values for $\epsilon $ (strength of perturbation). The plot shows the relative validation loss difference, i.e. the difference between the validation loss of the watermarked model and the non-watermarked benchmark model divided by the validation loss of the benchmark model. For all values the WM Accuracy is $100\%$. The dots in the plot represent the minimal validation loss difference for the respective architecture.\relax }}{72}{figure.caption.59}\protected@file@percent }
\newlabel{fig:frontier_influence_epsilon}{{\M@TitleReference {7.5}{FrontierStitching with various values for $\epsilon $ (strength of perturbation). The plot shows the relative validation loss difference, i.e. the difference between the validation loss of the watermarked model and the non-watermarked benchmark model divided by the validation loss of the benchmark model. For all values the WM Accuracy is $100\%$. The dots in the plot represent the minimal validation loss difference for the respective architecture.\relax }}{72}{FrontierStitching with various values for $\epsilon $ (strength of perturbation). The plot shows the relative validation loss difference, i.e. the difference between the validation loss of the watermarked model and the non-watermarked benchmark model divided by the validation loss of the benchmark model. For all values the WM Accuracy is $100\%$. The dots in the plot represent the minimal validation loss difference for the respective architecture.\relax }{figure.caption.59}{}}
\newlabel{fig:frontier_influence_epsilon@cref}{{[figure][5][7]7.5}{[1][71][]72}}
\newlabel{fig:frontier_influence_epsilon_pruning-a}{{\M@TitleReference {7.6a}{LeNet-1\relax }}{73}{LeNet-1\relax }{figure.caption.60}{}}
\newlabel{fig:frontier_influence_epsilon_pruning-a@cref}{{[subfigure][1][7,6]7.6a}{[1][72][]73}}
\newlabel{sub@fig:frontier_influence_epsilon_pruning-a}{{\M@TitleReference {a}{LeNet-1\relax }}{73}{LeNet-1\relax }{figure.caption.60}{}}
\newlabel{sub@fig:frontier_influence_epsilon_pruning-a@cref}{{[subfigure][1][7,6]7.6a}{[1][72][]73}}
\newlabel{fig:frontier_influence_epsilon_pruning-b}{{\M@TitleReference {7.6b}{LeNet-5\relax }}{73}{LeNet-5\relax }{figure.caption.60}{}}
\newlabel{fig:frontier_influence_epsilon_pruning-b@cref}{{[subfigure][2][7,6]7.6b}{[1][72][]73}}
\newlabel{sub@fig:frontier_influence_epsilon_pruning-b}{{\M@TitleReference {b}{LeNet-5\relax }}{73}{LeNet-5\relax }{figure.caption.60}{}}
\newlabel{sub@fig:frontier_influence_epsilon_pruning-b@cref}{{[subfigure][2][7,6]7.6b}{[1][72][]73}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Model accuracy after pruning attacks with pruning rates from 10\% to 90\%. The black dotted line indicates the threshold for the maximal plausible pruning attack.\relax }}{73}{figure.caption.60}\protected@file@percent }
\newlabel{fig:frontier_influence_epsilon_pruning}{{\M@TitleReference {7.6}{Model accuracy after pruning attacks with pruning rates from 10\% to 90\%. The black dotted line indicates the threshold for the maximal plausible pruning attack.\relax }}{73}{Model accuracy after pruning attacks with pruning rates from 10\% to 90\%. The black dotted line indicates the threshold for the maximal plausible pruning attack.\relax }{figure.caption.60}{}}
\newlabel{fig:frontier_influence_epsilon_pruning@cref}{{[figure][6][7]7.6}{[1][72][]73}}
\@writefile{toc}{\contentsline {subsubsection}{WMEmbeddedSystems}{73}{section*.62}\protected@file@percent }
\newlabel{sec:eval-param:embedded}{{\M@TitleReference {7.2.2}{WMEmbeddedSystems}}{73}{WMEmbeddedSystems}{section*.62}{}}
\newlabel{sec:eval-param:embedded@cref}{{[subsection][2][7,2]7.2.2}{[1][73][]73}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Watermark accuracies after fine-tuning attack on models trained with FrontierStitching.\relax }}{74}{table.caption.61}\protected@file@percent }
\newlabel{tab:frontier-finetuning}{{\M@TitleReference {7.2}{Watermark accuracies after fine-tuning attack on models trained with FrontierStitching.\relax }}{74}{Watermark accuracies after fine-tuning attack on models trained with FrontierStitching.\relax }{table.caption.61}{}}
\newlabel{tab:frontier-finetuning@cref}{{[table][2][7]7.2}{[1][73][]74}}
\@writefile{lot}{\contentsline {table}{\numberline {7.3}{\ignorespaces Results for ranking system for WMEmbeddedSystems. The points are averaged for each dataset and the bold numbers indicate the highest average for each dataset and therefore the winning $\epsilon $.\relax }}{74}{table.caption.63}\protected@file@percent }
\newlabel{tab:embedded_ranking}{{\M@TitleReference {7.3}{Results for ranking system for WMEmbeddedSystems. The points are averaged for each dataset and the bold numbers indicate the highest average for each dataset and therefore the winning $\epsilon $.\relax }}{74}{Results for ranking system for WMEmbeddedSystems. The points are averaged for each dataset and the bold numbers indicate the highest average for each dataset and therefore the winning $\epsilon $.\relax }{table.caption.63}{}}
\newlabel{tab:embedded_ranking@cref}{{[table][3][7]7.3}{[1][73][]74}}
\citation{li_piracy_2020}
\citation{namba_robust_2019}
\citation{adi_turning_2018}
\citation{adi_turning_2018}
\citation{adi_turning_2018}
\citation{zhang_protecting_2018}
\citation{zhang_protecting_2018}
\citation{zhang_protecting_2018}
\@writefile{lot}{\contentsline {table}{\numberline {7.4}{\ignorespaces Fidelity results from Adi et al. (\cite  {adi_turning_2018}, Table 1), and our experiments.\relax }}{75}{table.caption.65}\protected@file@percent }
\newlabel{tab:results-fidelity:weakness:compared}{{\M@TitleReference {7.4}{Fidelity results from Adi et al. (\cite  {adi_turning_2018}, Table 1), and our experiments.\relax }}{75}{Fidelity results from Adi et al. (\cite {adi_turning_2018}, Table 1), and our experiments.\relax }{table.caption.65}{}}
\newlabel{tab:results-fidelity:weakness:compared@cref}{{[table][4][7]7.4}{[1][75][]75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Comparing to State of the Art}{75}{subsection.7.2.3}\protected@file@percent }
\newlabel{sec:compare-results}{{\M@TitleReference {7.2.3}{Comparing to State of the Art}}{75}{Comparing to State of the Art}{subsection.7.2.3}{}}
\newlabel{sec:compare-results@cref}{{[subsection][3][7,2]7.2.3}{[1][75][]75}}
\@writefile{toc}{\contentsline {subsubsection}{WeaknessIntoStrength}{75}{section*.64}\protected@file@percent }
\newlabel{sec:compare-results:weakness}{{\M@TitleReference {7.2.3}{WeaknessIntoStrength}}{75}{WeaknessIntoStrength}{section*.64}{}}
\newlabel{sec:compare-results:weakness@cref}{{[subsection][3][7,2]7.2.3}{[1][75][]75}}
\@writefile{toc}{\contentsline {subsubsection}{ProtectingIP}{75}{section*.67}\protected@file@percent }
\newlabel{sec:compare-results:protecting}{{\M@TitleReference {7.2.3}{ProtectingIP}}{75}{ProtectingIP}{section*.67}{}}
\newlabel{sec:compare-results:protecting@cref}{{[subsection][3][7,2]7.2.3}{[1][75][]75}}
\citation{zhang_protecting_2018}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Behaviour of DenseNet during training with embedding type \textit  {pretrained} and \textit  {fromscratch}.\relax }}{76}{figure.caption.66}\protected@file@percent }
\newlabel{fig:weakness-pretrained-fromscratch}{{\M@TitleReference {7.7}{Behaviour of DenseNet during training with embedding type \textit  {pretrained} and \textit  {fromscratch}.\relax }}{76}{Behaviour of DenseNet during training with embedding type \textit {pretrained} and \textit {fromscratch}.\relax }{figure.caption.66}{}}
\newlabel{fig:weakness-pretrained-fromscratch@cref}{{[figure][7][7]7.7}{[1][75][]76}}
\@writefile{lot}{\contentsline {table}{\numberline {7.5}{\ignorespaces Effectiveness results from Zhang et al. (\cite  {zhang_protecting_2018}, Table 1), and our results.\relax }}{76}{table.caption.68}\protected@file@percent }
\newlabel{fig:results-effectiveness:protecting}{{\M@TitleReference {7.5}{Effectiveness results from Zhang et al. (\cite  {zhang_protecting_2018}, Table 1), and our results.\relax }}{76}{Effectiveness results from Zhang et al. (\cite {zhang_protecting_2018}, Table 1), and our results.\relax }{table.caption.68}{}}
\newlabel{fig:results-effectiveness:protecting@cref}{{[table][5][7]7.5}{[1][75][]76}}
\citation{zhang_protecting_2018}
\citation{zhang_protecting_2018}
\citation{merrer_adversarial_2019}
\citation{merrer_adversarial_2019}
\citation{merrer_adversarial_2019}
\citation{guo_watermarking_2018}
\@writefile{lot}{\contentsline {table}{\numberline {7.6}{\ignorespaces Pruning results from Zhang et al. (\cite  {zhang_protecting_2018}, Table 3 and Table 4), compared with our results. "Test" stands for test accuracy, "WM" for watermark accuracy and "Pr. rate" for pruning rate.\relax }}{77}{table.caption.69}\protected@file@percent }
\newlabel{tab:results-pruning:protecting:compared}{{\M@TitleReference {7.6}{Pruning results from Zhang et al. (\cite  {zhang_protecting_2018}, Table 3 and Table 4), compared with our results. "Test" stands for test accuracy, "WM" for watermark accuracy and "Pr. rate" for pruning rate.\relax }}{77}{Pruning results from Zhang et al. (\cite {zhang_protecting_2018}, Table 3 and Table 4), compared with our results. "Test" stands for test accuracy, "WM" for watermark accuracy and "Pr. rate" for pruning rate.\relax }{table.caption.69}{}}
\newlabel{tab:results-pruning:protecting:compared@cref}{{[table][6][7]7.6}{[1][77][]77}}
\@writefile{toc}{\contentsline {subsubsection}{FrontierStitching}{77}{section*.70}\protected@file@percent }
\newlabel{sec:compare-results:frontier}{{\M@TitleReference {7.2.3}{FrontierStitching}}{77}{FrontierStitching}{section*.70}{}}
\newlabel{sec:compare-results:frontier@cref}{{[subsection][3][7,2]7.2.3}{[1][77][]77}}
\citation{guo_watermarking_2018}
\citation{guo_watermarking_2018}
\@writefile{lot}{\contentsline {table}{\numberline {7.7}{\ignorespaces Pruning results from Merrer et al. (\cite  {merrer_adversarial_2019}, Table 2), and our results. The grey cells indicate a non-plausible pruning attack. For a plausible attack and watermark accuracy above 50\% the cell is green and below it is red.\relax }}{78}{table.caption.71}\protected@file@percent }
\newlabel{fig:compare-results-pruning:frontier}{{\M@TitleReference {7.7}{Pruning results from Merrer et al. (\cite  {merrer_adversarial_2019}, Table 2), and our results. The grey cells indicate a non-plausible pruning attack. For a plausible attack and watermark accuracy above 50\% the cell is green and below it is red.\relax }}{78}{Pruning results from Merrer et al. (\cite {merrer_adversarial_2019}, Table 2), and our results. The grey cells indicate a non-plausible pruning attack. For a plausible attack and watermark accuracy above 50\% the cell is green and below it is red.\relax }{table.caption.71}{}}
\newlabel{fig:compare-results-pruning:frontier@cref}{{[table][7][7]7.7}{[1][77][]78}}
\@writefile{lot}{\contentsline {table}{\numberline {7.8}{\ignorespaces Fidelity results from Guo et al. (\cite  {guo_watermarking_2018}, Table 2), and our results.\relax }}{78}{table.caption.73}\protected@file@percent }
\newlabel{tab:results-fidelity:embedded:compared}{{\M@TitleReference {7.8}{Fidelity results from Guo et al. (\cite  {guo_watermarking_2018}, Table 2), and our results.\relax }}{78}{Fidelity results from Guo et al. (\cite {guo_watermarking_2018}, Table 2), and our results.\relax }{table.caption.73}{}}
\newlabel{tab:results-fidelity:embedded:compared@cref}{{[table][8][7]7.8}{[1][78][]78}}
\@writefile{toc}{\contentsline {subsubsection}{WMEmbeddedSystems}{78}{section*.72}\protected@file@percent }
\newlabel{sec:compare-results:embedded}{{\M@TitleReference {7.2.3}{WMEmbeddedSystems}}{78}{WMEmbeddedSystems}{section*.72}{}}
\newlabel{sec:compare-results:embedded@cref}{{[subsection][3][7,2]7.2.3}{[1][77][]78}}
\@writefile{lot}{\contentsline {table}{\numberline {7.9}{\ignorespaces Watermark accuracy on watermarked models. A checkmark \text  {$\mathsurround \z@ \mathchar "458$}indicates 100\% watermark accuracy.\relax }}{79}{table.caption.74}\protected@file@percent }
\newlabel{tab:effectiveness}{{\M@TitleReference {7.9}{Watermark accuracy on watermarked models. A checkmark \text  {$\mathsurround \z@ \mathchar "458$}indicates 100\% watermark accuracy.\relax }}{79}{Watermark accuracy on watermarked models. A checkmark \checkmark indicates 100\% watermark accuracy.\relax }{table.caption.74}{}}
\newlabel{tab:effectiveness@cref}{{[table][9][7]7.9}{[1][79][]79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Effectiveness}{79}{subsection.7.2.4}\protected@file@percent }
\newlabel{sec:eval-effectiveness}{{\M@TitleReference {7.2.4}{Effectiveness}}{79}{Effectiveness}{subsection.7.2.4}{}}
\newlabel{sec:eval-effectiveness@cref}{{[subsection][4][7,2]7.2.4}{[1][79][]79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.5}Fidelity}{80}{subsection.7.2.5}\protected@file@percent }
\newlabel{sec:eval-fidelity}{{\M@TitleReference {7.2.5}{Fidelity}}{80}{Fidelity}{subsection.7.2.5}{}}
\newlabel{sec:eval-fidelity@cref}{{[subsection][5][7,2]7.2.5}{[1][80][]80}}
\newlabel{fig:fidelity-densenet}{{\M@TitleReference {7.8a}{DenseNet trained on CIFAR-10\relax }}{81}{DenseNet trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-densenet@cref}{{[subfigure][1][7,8]7.8a}{[1][80][]81}}
\newlabel{sub@fig:fidelity-densenet}{{\M@TitleReference {a}{DenseNet trained on CIFAR-10\relax }}{81}{DenseNet trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-densenet@cref}{{[subfigure][1][7,8]7.8a}{[1][80][]81}}
\newlabel{fig:fidelity-simplenet_mnist}{{\M@TitleReference {7.8b}{SimpleNet trained on MNIST\relax }}{81}{SimpleNet trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-simplenet_mnist@cref}{{[subfigure][2][7,8]7.8b}{[1][80][]81}}
\newlabel{sub@fig:fidelity-simplenet_mnist}{{\M@TitleReference {b}{SimpleNet trained on MNIST\relax }}{81}{SimpleNet trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-simplenet_mnist@cref}{{[subfigure][2][7,8]7.8b}{[1][80][]81}}
\newlabel{fig:fidelity-resnet18}{{\M@TitleReference {7.8c}{ResNet-18 trained on CIFAR-10\relax }}{81}{ResNet-18 trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-resnet18@cref}{{[subfigure][3][7,8]7.8c}{[1][80][]81}}
\newlabel{sub@fig:fidelity-resnet18}{{\M@TitleReference {c}{ResNet-18 trained on CIFAR-10\relax }}{81}{ResNet-18 trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-resnet18@cref}{{[subfigure][3][7,8]7.8c}{[1][80][]81}}
\newlabel{fig:fidelity-lenet1}{{\M@TitleReference {7.8d}{LeNet-1 trained on MNIST\relax }}{81}{LeNet-1 trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-lenet1@cref}{{[subfigure][4][7,8]7.8d}{[1][80][]81}}
\newlabel{sub@fig:fidelity-lenet1}{{\M@TitleReference {d}{LeNet-1 trained on MNIST\relax }}{81}{LeNet-1 trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-lenet1@cref}{{[subfigure][4][7,8]7.8d}{[1][80][]81}}
\newlabel{fig:fidelity-resnet34}{{\M@TitleReference {7.8e}{ResNet-34 trained on CIFAR-10\relax }}{81}{ResNet-34 trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-resnet34@cref}{{[subfigure][5][7,8]7.8e}{[1][80][]81}}
\newlabel{sub@fig:fidelity-resnet34}{{\M@TitleReference {e}{ResNet-34 trained on CIFAR-10\relax }}{81}{ResNet-34 trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-resnet34@cref}{{[subfigure][5][7,8]7.8e}{[1][80][]81}}
\newlabel{fig:fidelity-lenet3}{{\M@TitleReference {7.8f}{LeNet-3 trained on MNIST\relax }}{81}{LeNet-3 trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-lenet3@cref}{{[subfigure][6][7,8]7.8f}{[1][80][]81}}
\newlabel{sub@fig:fidelity-lenet3}{{\M@TitleReference {f}{LeNet-3 trained on MNIST\relax }}{81}{LeNet-3 trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-lenet3@cref}{{[subfigure][6][7,8]7.8f}{[1][80][]81}}
\newlabel{fig:fidelity-resnet50}{{\M@TitleReference {7.8g}{ResNet-50 trained on CIFAR-10\relax }}{81}{ResNet-50 trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-resnet50@cref}{{[subfigure][7][7,8]7.8g}{[1][80][]81}}
\newlabel{sub@fig:fidelity-resnet50}{{\M@TitleReference {g}{ResNet-50 trained on CIFAR-10\relax }}{81}{ResNet-50 trained on CIFAR-10\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-resnet50@cref}{{[subfigure][7][7,8]7.8g}{[1][80][]81}}
\newlabel{fig:fidelity-lenet5}{{\M@TitleReference {7.8h}{LeNet-5 trained on MNIST\relax }}{81}{LeNet-5 trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-lenet5@cref}{{[subfigure][8][7,8]7.8h}{[1][80][]81}}
\newlabel{sub@fig:fidelity-lenet5}{{\M@TitleReference {h}{LeNet-5 trained on MNIST\relax }}{81}{LeNet-5 trained on MNIST\relax }{figure.caption.75}{}}
\newlabel{sub@fig:fidelity-lenet5@cref}{{[subfigure][8][7,8]7.8h}{[1][80][]81}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Influence of the trigger set size on \textbf  {fidelity}. Each plot corresponds to one architecture and shows the results for all watermarking methods, on the left models trained on \textbf  {CIFAR-10} and on the right those trained on \textbf  {MNIST}. We plot the relative difference between the test accuracy of the watermarked and non-watermarked model.\relax }}{81}{figure.caption.75}\protected@file@percent }
\newlabel{fig:fidelity-perarch}{{\M@TitleReference {7.8}{Influence of the trigger set size on \textbf  {fidelity}. Each plot corresponds to one architecture and shows the results for all watermarking methods, on the left models trained on \textbf  {CIFAR-10} and on the right those trained on \textbf  {MNIST}. We plot the relative difference between the test accuracy of the watermarked and non-watermarked model.\relax }}{81}{Influence of the trigger set size on \textbf {fidelity}. Each plot corresponds to one architecture and shows the results for all watermarking methods, on the left models trained on \textbf {CIFAR-10} and on the right those trained on \textbf {MNIST}. We plot the relative difference between the test accuracy of the watermarked and non-watermarked model.\relax }{figure.caption.75}{}}
\newlabel{fig:fidelity-perarch@cref}{{[figure][8][7]7.8}{[1][80][]81}}
\newlabel{fig:fidelity-weakness}{{\M@TitleReference {7.9a}{WeaknessIntoStrength\relax }}{82}{WeaknessIntoStrength\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-weakness@cref}{{[subfigure][1][7,9]7.9a}{[1][80][]82}}
\newlabel{sub@fig:fidelity-weakness}{{\M@TitleReference {a}{WeaknessIntoStrength\relax }}{82}{WeaknessIntoStrength\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-weakness@cref}{{[subfigure][1][7,9]7.9a}{[1][80][]82}}
\newlabel{fig:fidelity-ood}{{\M@TitleReference {7.9b}{ProtectingIP-OOD\relax }}{82}{ProtectingIP-OOD\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-ood@cref}{{[subfigure][2][7,9]7.9b}{[1][80][]82}}
\newlabel{sub@fig:fidelity-ood}{{\M@TitleReference {b}{ProtectingIP-OOD\relax }}{82}{ProtectingIP-OOD\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-ood@cref}{{[subfigure][2][7,9]7.9b}{[1][80][]82}}
\newlabel{fig:fidelity-pattern}{{\M@TitleReference {7.9c}{ProtectingIP-pattern\relax }}{82}{ProtectingIP-pattern\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-pattern@cref}{{[subfigure][3][7,9]7.9c}{[1][80][]82}}
\newlabel{sub@fig:fidelity-pattern}{{\M@TitleReference {c}{ProtectingIP-pattern\relax }}{82}{ProtectingIP-pattern\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-pattern@cref}{{[subfigure][3][7,9]7.9c}{[1][80][]82}}
\newlabel{fig:fidelity-noise}{{\M@TitleReference {7.9d}{ProtectingIP-noise\relax }}{82}{ProtectingIP-noise\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-noise@cref}{{[subfigure][4][7,9]7.9d}{[1][80][]82}}
\newlabel{sub@fig:fidelity-noise}{{\M@TitleReference {d}{ProtectingIP-noise\relax }}{82}{ProtectingIP-noise\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-noise@cref}{{[subfigure][4][7,9]7.9d}{[1][80][]82}}
\newlabel{fig:fidelity-piracy}{{\M@TitleReference {7.9e}{PiracyResistant\relax }}{82}{PiracyResistant\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-piracy@cref}{{[subfigure][5][7,9]7.9e}{[1][80][]82}}
\newlabel{sub@fig:fidelity-piracy}{{\M@TitleReference {e}{PiracyResistant\relax }}{82}{PiracyResistant\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-piracy@cref}{{[subfigure][5][7,9]7.9e}{[1][80][]82}}
\newlabel{fig:fidelity-frontier}{{\M@TitleReference {7.9f}{FrontierStitching\relax }}{82}{FrontierStitching\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-frontier@cref}{{[subfigure][6][7,9]7.9f}{[1][80][]82}}
\newlabel{sub@fig:fidelity-frontier}{{\M@TitleReference {f}{FrontierStitching\relax }}{82}{FrontierStitching\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-frontier@cref}{{[subfigure][6][7,9]7.9f}{[1][80][]82}}
\newlabel{fig:fidelity-exponential}{{\M@TitleReference {7.9g}{ExponentialWeighting\relax }}{82}{ExponentialWeighting\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-exponential@cref}{{[subfigure][7][7,9]7.9g}{[1][80][]82}}
\newlabel{sub@fig:fidelity-exponential}{{\M@TitleReference {g}{ExponentialWeighting\relax }}{82}{ExponentialWeighting\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-exponential@cref}{{[subfigure][7][7,9]7.9g}{[1][80][]82}}
\newlabel{fig:fidelity-embedded}{{\M@TitleReference {7.9h}{WMEmbeddedSystems\relax }}{82}{WMEmbeddedSystems\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-embedded@cref}{{[subfigure][8][7,9]7.9h}{[1][80][]82}}
\newlabel{sub@fig:fidelity-embedded}{{\M@TitleReference {h}{WMEmbeddedSystems\relax }}{82}{WMEmbeddedSystems\relax }{figure.caption.76}{}}
\newlabel{sub@fig:fidelity-embedded@cref}{{[subfigure][8][7,9]7.9h}{[1][80][]82}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Influence of the trigger set size on \textbf  {fidelity}. Each plot corresponds to one method and shows the results for all architectures. We plot the relative difference between the test accuracy of the watermarked and non-watermarked model.\relax }}{82}{figure.caption.76}\protected@file@percent }
\newlabel{fig:fidelity-per-method}{{\M@TitleReference {7.9}{Influence of the trigger set size on \textbf  {fidelity}. Each plot corresponds to one method and shows the results for all architectures. We plot the relative difference between the test accuracy of the watermarked and non-watermarked model.\relax }}{82}{Influence of the trigger set size on \textbf {fidelity}. Each plot corresponds to one method and shows the results for all architectures. We plot the relative difference between the test accuracy of the watermarked and non-watermarked model.\relax }{figure.caption.76}{}}
\newlabel{fig:fidelity-per-method@cref}{{[figure][9][7]7.9}{[1][80][]82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.6}Robustness}{83}{subsection.7.2.6}\protected@file@percent }
\newlabel{sec:eval-robustness}{{\M@TitleReference {7.2.6}{Robustness}}{83}{Robustness}{subsection.7.2.6}{}}
\newlabel{sec:eval-robustness@cref}{{[subsection][6][7,2]7.2.6}{[1][83][]83}}
\@writefile{toc}{\contentsline {subsubsection}{Parameter Pruning}{83}{section*.77}\protected@file@percent }
\newlabel{sec:eval-robustness-pruning}{{\M@TitleReference {7.2.6}{Parameter Pruning}}{83}{Parameter Pruning}{section*.77}{}}
\newlabel{sec:eval-robustness-pruning@cref}{{[subsection][6][7,2]7.2.6}{[1][83][]83}}
\newlabel{fig:pruning-0.8-densenet}{{\M@TitleReference {7.10a}{DenseNet, 80\% pruned.\relax }}{84}{DenseNet, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.8-densenet@cref}{{[subfigure][1][7,10]7.10a}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.8-densenet}{{\M@TitleReference {a}{DenseNet, 80\% pruned.\relax }}{84}{DenseNet, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.8-densenet@cref}{{[subfigure][1][7,10]7.10a}{[1][83][]84}}
\newlabel{fig:pruning-0.9-densenet}{{\M@TitleReference {7.10b}{DenseNet, 90\% pruned.\relax }}{84}{DenseNet, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.9-densenet@cref}{{[subfigure][2][7,10]7.10b}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.9-densenet}{{\M@TitleReference {b}{DenseNet, 90\% pruned.\relax }}{84}{DenseNet, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.9-densenet@cref}{{[subfigure][2][7,10]7.10b}{[1][83][]84}}
\newlabel{fig:pruning-0.8-resnet18}{{\M@TitleReference {7.10c}{ResNet-18, 80\% pruned.\relax }}{84}{ResNet-18, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.8-resnet18@cref}{{[subfigure][3][7,10]7.10c}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.8-resnet18}{{\M@TitleReference {c}{ResNet-18, 80\% pruned.\relax }}{84}{ResNet-18, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.8-resnet18@cref}{{[subfigure][3][7,10]7.10c}{[1][83][]84}}
\newlabel{fig:pruning-0.9-resnet18}{{\M@TitleReference {7.10d}{ResNet-18, 90\% pruned.\relax }}{84}{ResNet-18, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.9-resnet18@cref}{{[subfigure][4][7,10]7.10d}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.9-resnet18}{{\M@TitleReference {d}{ResNet-18, 90\% pruned.\relax }}{84}{ResNet-18, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.9-resnet18@cref}{{[subfigure][4][7,10]7.10d}{[1][83][]84}}
\newlabel{fig:pruning-0.8-resnet34}{{\M@TitleReference {7.10e}{ResNet-34, 80\% pruned.\relax }}{84}{ResNet-34, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.8-resnet34@cref}{{[subfigure][5][7,10]7.10e}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.8-resnet34}{{\M@TitleReference {e}{ResNet-34, 80\% pruned.\relax }}{84}{ResNet-34, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.8-resnet34@cref}{{[subfigure][5][7,10]7.10e}{[1][83][]84}}
\newlabel{fig:pruning-0.9-resnet34}{{\M@TitleReference {7.10f}{ResNet-34, 90\% pruned.\relax }}{84}{ResNet-34, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.9-resnet34@cref}{{[subfigure][6][7,10]7.10f}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.9-resnet34}{{\M@TitleReference {f}{ResNet-34, 90\% pruned.\relax }}{84}{ResNet-34, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.9-resnet34@cref}{{[subfigure][6][7,10]7.10f}{[1][83][]84}}
\newlabel{fig:pruning-0.8-resnet50}{{\M@TitleReference {7.10g}{ResNet-50, 80\% pruned.\relax }}{84}{ResNet-50, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.8-resnet50@cref}{{[subfigure][7][7,10]7.10g}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.8-resnet50}{{\M@TitleReference {g}{ResNet-50, 80\% pruned.\relax }}{84}{ResNet-50, 80\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.8-resnet50@cref}{{[subfigure][7][7,10]7.10g}{[1][83][]84}}
\newlabel{fig:pruning-0.9-resnet50}{{\M@TitleReference {7.10h}{ResNet-50, 90\% pruned.\relax }}{84}{ResNet-50, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-0.9-resnet50@cref}{{[subfigure][8][7,10]7.10h}{[1][83][]84}}
\newlabel{sub@fig:pruning-0.9-resnet50}{{\M@TitleReference {h}{ResNet-50, 90\% pruned.\relax }}{84}{ResNet-50, 90\% pruned.\relax }{figure.caption.78}{}}
\newlabel{sub@fig:pruning-0.9-resnet50@cref}{{[subfigure][8][7,10]7.10h}{[1][83][]84}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces Influence of the trigger set size on robustness against pruning on \textbf  {CIFAR-10} models. Each plot on the left corresponds pruning with 80\% and each plot on the right to corresponds pruning with 90\%. Each plot shows the results for all watermarking methods.\relax }}{84}{figure.caption.78}\protected@file@percent }
\newlabel{fig:pruning-cifar10models-perarch}{{\M@TitleReference {7.10}{Influence of the trigger set size on robustness against pruning on \textbf  {CIFAR-10} models. Each plot on the left corresponds pruning with 80\% and each plot on the right to corresponds pruning with 90\%. Each plot shows the results for all watermarking methods.\relax }}{84}{Influence of the trigger set size on robustness against pruning on \textbf {CIFAR-10} models. Each plot on the left corresponds pruning with 80\% and each plot on the right to corresponds pruning with 90\%. Each plot shows the results for all watermarking methods.\relax }{figure.caption.78}{}}
\newlabel{fig:pruning-cifar10models-perarch@cref}{{[figure][10][7]7.10}{[1][83][]84}}
\newlabel{fig:pruning-0.8-simplenet}{{\M@TitleReference {7.11a}{SimpleNet, 80\% pruned.\relax }}{85}{SimpleNet, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.8-simplenet@cref}{{[subfigure][1][7,11]7.11a}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.8-simplenet}{{\M@TitleReference {a}{SimpleNet, 80\% pruned.\relax }}{85}{SimpleNet, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.8-simplenet@cref}{{[subfigure][1][7,11]7.11a}{[1][83][]85}}
\newlabel{fig:pruning-0.9-simplenet}{{\M@TitleReference {7.11b}{SimpleNet, 90\% pruned.\relax }}{85}{SimpleNet, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.9-simplenet@cref}{{[subfigure][2][7,11]7.11b}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.9-simplenet}{{\M@TitleReference {b}{SimpleNet, 90\% pruned.\relax }}{85}{SimpleNet, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.9-simplenet@cref}{{[subfigure][2][7,11]7.11b}{[1][83][]85}}
\newlabel{fig:pruning-0.8-lenet1}{{\M@TitleReference {7.11c}{LeNet-1, 80\% pruned.\relax }}{85}{LeNet-1, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.8-lenet1@cref}{{[subfigure][3][7,11]7.11c}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.8-lenet1}{{\M@TitleReference {c}{LeNet-1, 80\% pruned.\relax }}{85}{LeNet-1, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.8-lenet1@cref}{{[subfigure][3][7,11]7.11c}{[1][83][]85}}
\newlabel{fig:pruning-0.9-lenet1}{{\M@TitleReference {7.11d}{LeNet-1, 90\% pruned.\relax }}{85}{LeNet-1, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.9-lenet1@cref}{{[subfigure][4][7,11]7.11d}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.9-lenet1}{{\M@TitleReference {d}{LeNet-1, 90\% pruned.\relax }}{85}{LeNet-1, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.9-lenet1@cref}{{[subfigure][4][7,11]7.11d}{[1][83][]85}}
\newlabel{fig:pruning-0.8-lenet3}{{\M@TitleReference {7.11e}{LeNet-3, 80\% pruned.\relax }}{85}{LeNet-3, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.8-lenet3@cref}{{[subfigure][5][7,11]7.11e}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.8-lenet3}{{\M@TitleReference {e}{LeNet-3, 80\% pruned.\relax }}{85}{LeNet-3, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.8-lenet3@cref}{{[subfigure][5][7,11]7.11e}{[1][83][]85}}
\newlabel{fig:pruning-0.9-lenet3}{{\M@TitleReference {7.11f}{LeNet-3, 90\% pruned.\relax }}{85}{LeNet-3, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.9-lenet3@cref}{{[subfigure][6][7,11]7.11f}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.9-lenet3}{{\M@TitleReference {f}{LeNet-3, 90\% pruned.\relax }}{85}{LeNet-3, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.9-lenet3@cref}{{[subfigure][6][7,11]7.11f}{[1][83][]85}}
\newlabel{fig:pruning-0.8-lenet5}{{\M@TitleReference {7.11g}{LeNet-5, 80\% pruned.\relax }}{85}{LeNet-5, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.8-lenet5@cref}{{[subfigure][7][7,11]7.11g}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.8-lenet5}{{\M@TitleReference {g}{LeNet-5, 80\% pruned.\relax }}{85}{LeNet-5, 80\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.8-lenet5@cref}{{[subfigure][7][7,11]7.11g}{[1][83][]85}}
\newlabel{fig:pruning-0.9-lenet5}{{\M@TitleReference {7.11h}{LeNet-5, 90\% pruned.\relax }}{85}{LeNet-5, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-0.9-lenet5@cref}{{[subfigure][8][7,11]7.11h}{[1][83][]85}}
\newlabel{sub@fig:pruning-0.9-lenet5}{{\M@TitleReference {h}{LeNet-5, 90\% pruned.\relax }}{85}{LeNet-5, 90\% pruned.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:pruning-0.9-lenet5@cref}{{[subfigure][8][7,11]7.11h}{[1][83][]85}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces Influence of the trigger set size on robustness against pruning on \textbf  {MNIST} models. Each plot on the left corresponds pruning with 80\% and each plot on the right to corresponds pruning with 90\%. Each plot shows the results for all watermarking methods.\relax }}{85}{figure.caption.79}\protected@file@percent }
\newlabel{fig:pruning-mnistmodels-perarch}{{\M@TitleReference {7.11}{Influence of the trigger set size on robustness against pruning on \textbf  {MNIST} models. Each plot on the left corresponds pruning with 80\% and each plot on the right to corresponds pruning with 90\%. Each plot shows the results for all watermarking methods.\relax }}{85}{Influence of the trigger set size on robustness against pruning on \textbf {MNIST} models. Each plot on the left corresponds pruning with 80\% and each plot on the right to corresponds pruning with 90\%. Each plot shows the results for all watermarking methods.\relax }{figure.caption.79}{}}
\newlabel{fig:pruning-mnistmodels-perarch@cref}{{[figure][11][7]7.11}{[1][83][]85}}
\@writefile{lot}{\contentsline {table}{\numberline {7.10}{\ignorespaces Influence of the trigger set size on robustness against pruning with the maximal plausible pruning rate. The values are the watermark accuracy after an pruning attack, the value in the parenthesis is the maximal plausible pruning rate. A checkmark \text  {$\mathsurround \z@ \mathchar "458$}indicates 100\% watermark accuracy.\relax }}{86}{table.caption.80}\protected@file@percent }
\newlabel{tab:max-pruning-rate}{{\M@TitleReference {7.10}{Influence of the trigger set size on robustness against pruning with the maximal plausible pruning rate. The values are the watermark accuracy after an pruning attack, the value in the parenthesis is the maximal plausible pruning rate. A checkmark \text  {$\mathsurround \z@ \mathchar "458$}indicates 100\% watermark accuracy.\relax }}{86}{Influence of the trigger set size on robustness against pruning with the maximal plausible pruning rate. The values are the watermark accuracy after an pruning attack, the value in the parenthesis is the maximal plausible pruning rate. A checkmark \checkmark indicates 100\% watermark accuracy.\relax }{table.caption.80}{}}
\newlabel{tab:max-pruning-rate@cref}{{[table][10][7]7.10}{[1][83][]86}}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning}{87}{section*.81}\protected@file@percent }
\newlabel{sec:eval-robustness-finetuning}{{\M@TitleReference {7.2.6}{Fine-Tuning}}{87}{Fine-Tuning}{section*.81}{}}
\newlabel{sec:eval-robustness-finetuning@cref}{{[subsection][6][7,2]7.2.6}{[1][87][]87}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-densenet}{{\M@TitleReference {7.12a}{DenseNet, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{DenseNet, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-densenet@cref}{{[subfigure][1][7,12]7.12a}{[1][87][]88}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-densenet}{{\M@TitleReference {a}{DenseNet, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{DenseNet, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-densenet@cref}{{[subfigure][1][7,12]7.12a}{[1][87][]88}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-densenet}{{\M@TitleReference {7.12b}{DenseNet, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{DenseNet, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-densenet@cref}{{[subfigure][2][7,12]7.12b}{[1][87][]88}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-densenet}{{\M@TitleReference {b}{DenseNet, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{DenseNet, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-densenet@cref}{{[subfigure][2][7,12]7.12b}{[1][87][]88}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-resnet18}{{\M@TitleReference {7.12c}{ResNet-18, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{ResNet-18, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-resnet18@cref}{{[subfigure][3][7,12]7.12c}{[1][87][]88}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-resnet18}{{\M@TitleReference {c}{ResNet-18, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{ResNet-18, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-resnet18@cref}{{[subfigure][3][7,12]7.12c}{[1][87][]88}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-resnet18}{{\M@TitleReference {7.12d}{ResNet-18, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{ResNet-18, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-resnet18@cref}{{[subfigure][4][7,12]7.12d}{[1][87][]88}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-resnet18}{{\M@TitleReference {d}{ResNet-18, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{ResNet-18, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-resnet18@cref}{{[subfigure][4][7,12]7.12d}{[1][87][]88}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-resnet34}{{\M@TitleReference {7.12e}{ResNet-34, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{ResNet-34, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-resnet34@cref}{{[subfigure][5][7,12]7.12e}{[1][87][]88}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-resnet34}{{\M@TitleReference {e}{ResNet-34, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{ResNet-34, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-resnet34@cref}{{[subfigure][5][7,12]7.12e}{[1][87][]88}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-resnet34}{{\M@TitleReference {7.12f}{ResNet-34, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{ResNet-34, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-resnet34@cref}{{[subfigure][6][7,12]7.12f}{[1][87][]88}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-resnet34}{{\M@TitleReference {f}{ResNet-34, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{ResNet-34, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-resnet34@cref}{{[subfigure][6][7,12]7.12f}{[1][87][]88}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-resnet50}{{\M@TitleReference {7.12g}{ResNet-50, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{ResNet-50, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-resnet50@cref}{{[subfigure][7][7,12]7.12g}{[1][87][]88}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-resnet50}{{\M@TitleReference {g}{ResNet-50, fine-tuned with $\alpha =10^{-4}$\relax }}{88}{ResNet-50, fine-tuned with $\alpha =10^{-4}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-resnet50@cref}{{[subfigure][7][7,12]7.12g}{[1][87][]88}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-resnet50}{{\M@TitleReference {7.12h}{ResNet-50, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{ResNet-50, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-resnet50@cref}{{[subfigure][8][7,12]7.12h}{[1][87][]88}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-resnet50}{{\M@TitleReference {h}{ResNet-50, fine-tuned with $\alpha =10^{-2}$\relax }}{88}{ResNet-50, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.82}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-resnet50@cref}{{[subfigure][8][7,12]7.12h}{[1][87][]88}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces Influence of the trigger set size on robustness against fine-tuning on \textbf  {CIFAR-10} models. Each plot on the right corresponds fine-tuning with a small learning rate and each plot on the left to fine-tuning with a large learning rate, all of them show the results for all watermarking methods.\relax }}{88}{figure.caption.82}\protected@file@percent }
\newlabel{fig:finetuning-cifar10models-perarch}{{\M@TitleReference {7.12}{Influence of the trigger set size on robustness against fine-tuning on \textbf  {CIFAR-10} models. Each plot on the right corresponds fine-tuning with a small learning rate and each plot on the left to fine-tuning with a large learning rate, all of them show the results for all watermarking methods.\relax }}{88}{Influence of the trigger set size on robustness against fine-tuning on \textbf {CIFAR-10} models. Each plot on the right corresponds fine-tuning with a small learning rate and each plot on the left to fine-tuning with a large learning rate, all of them show the results for all watermarking methods.\relax }{figure.caption.82}{}}
\newlabel{fig:finetuning-cifar10models-perarch@cref}{{[figure][12][7]7.12}{[1][87][]88}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-simplenet_mnist}{{\M@TitleReference {7.13a}{SimpleNet, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{SimpleNet, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-simplenet_mnist@cref}{{[subfigure][1][7,13]7.13a}{[1][87][]89}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-simplenet_mnist}{{\M@TitleReference {a}{SimpleNet, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{SimpleNet, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-simplenet_mnist@cref}{{[subfigure][1][7,13]7.13a}{[1][87][]89}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-simplenet_mnist}{{\M@TitleReference {7.13b}{SimpleNet, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{SimpleNet, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-simplenet_mnist@cref}{{[subfigure][2][7,13]7.13b}{[1][87][]89}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-simplenet_mnist}{{\M@TitleReference {b}{SimpleNet, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{SimpleNet, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-simplenet_mnist@cref}{{[subfigure][2][7,13]7.13b}{[1][87][]89}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-lenet1}{{\M@TitleReference {7.13c}{LeNet-1, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{LeNet-1, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-lenet1@cref}{{[subfigure][3][7,13]7.13c}{[1][87][]89}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-lenet1}{{\M@TitleReference {c}{LeNet-1, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{LeNet-1, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-lenet1@cref}{{[subfigure][3][7,13]7.13c}{[1][87][]89}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-lenet1}{{\M@TitleReference {7.13d}{LeNet-1, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{LeNet-1, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-lenet1@cref}{{[subfigure][4][7,13]7.13d}{[1][87][]89}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-lenet1}{{\M@TitleReference {d}{LeNet-1, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{LeNet-1, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-lenet1@cref}{{[subfigure][4][7,13]7.13d}{[1][87][]89}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-lenet3}{{\M@TitleReference {7.13e}{LeNet-3, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{LeNet-3, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-lenet3@cref}{{[subfigure][5][7,13]7.13e}{[1][87][]89}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-lenet3}{{\M@TitleReference {e}{LeNet-3, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{LeNet-3, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-lenet3@cref}{{[subfigure][5][7,13]7.13e}{[1][87][]89}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-lenet3}{{\M@TitleReference {7.13f}{LeNet-3, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{LeNet-3, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-lenet3@cref}{{[subfigure][6][7,13]7.13f}{[1][87][]89}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-lenet3}{{\M@TitleReference {f}{LeNet-3, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{LeNet-3, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-lenet3@cref}{{[subfigure][6][7,13]7.13f}{[1][87][]89}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-lenet5}{{\M@TitleReference {7.13g}{LeNet-5, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{LeNet-5, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-smalllr-allmethods-perarch-lenet5@cref}{{[subfigure][7][7,13]7.13g}{[1][87][]89}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-lenet5}{{\M@TitleReference {g}{LeNet-5, fine-tuned with $\alpha =10^{-5}$\relax }}{89}{LeNet-5, fine-tuned with $\alpha =10^{-5}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-smalllr-allmethods-perarch-lenet5@cref}{{[subfigure][7][7,13]7.13g}{[1][87][]89}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-lenet5}{{\M@TitleReference {7.13h}{LeNet-5, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{LeNet-5, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-largelr-allmethods-perarch-lenet5@cref}{{[subfigure][8][7,13]7.13h}{[1][87][]89}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-lenet5}{{\M@TitleReference {h}{LeNet-5, fine-tuned with $\alpha =10^{-2}$\relax }}{89}{LeNet-5, fine-tuned with $\alpha =10^{-2}$\relax }{figure.caption.83}{}}
\newlabel{sub@fig:finetuning-largelr-allmethods-perarch-lenet5@cref}{{[subfigure][8][7,13]7.13h}{[1][87][]89}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Influence of the trigger set size on robustness against fine-tuning on MNIST models. Each plot on the left corresponds fine-tuning with a small learning rate and each plot on the right to fine-tuning with a large learning rate, all of them show the results for all watermarking methods.\relax }}{89}{figure.caption.83}\protected@file@percent }
\newlabel{fig:finetuning-mnistmodels-perarch}{{\M@TitleReference {7.13}{Influence of the trigger set size on robustness against fine-tuning on MNIST models. Each plot on the left corresponds fine-tuning with a small learning rate and each plot on the right to fine-tuning with a large learning rate, all of them show the results for all watermarking methods.\relax }}{89}{Influence of the trigger set size on robustness against fine-tuning on MNIST models. Each plot on the left corresponds fine-tuning with a small learning rate and each plot on the right to fine-tuning with a large learning rate, all of them show the results for all watermarking methods.\relax }{figure.caption.83}{}}
\newlabel{fig:finetuning-mnistmodels-perarch@cref}{{[figure][13][7]7.13}{[1][87][]89}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces Behaviour of ResNet-18 watermarked with \textit  {ProtectingIP-pattern} during a fine-tuning attack with a small learning rate $\alpha =10^{-4}$. The colors indicate the trigger set size, with which the model was watermarked. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }}{90}{figure.caption.84}\protected@file@percent }
\newlabel{fig:finetuning-content-trgsetsizes}{{\M@TitleReference {7.14}{Behaviour of ResNet-18 watermarked with \textit  {ProtectingIP-pattern} during a fine-tuning attack with a small learning rate $\alpha =10^{-4}$. The colors indicate the trigger set size, with which the model was watermarked. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }}{90}{Behaviour of ResNet-18 watermarked with \textit {ProtectingIP-pattern} during a fine-tuning attack with a small learning rate $\alpha =10^{-4}$. The colors indicate the trigger set size, with which the model was watermarked. The black dash-dotted line corresponds to the benchmark test accuracy of the non-watermarked model.\relax }{figure.caption.84}{}}
\newlabel{fig:finetuning-content-trgsetsizes@cref}{{[figure][14][7]7.14}{[1][87][]90}}
\newlabel{fig:finetuning-smalllr-permethod-weakness}{{\M@TitleReference {7.15a}{WeaknessIntoStrength\relax }}{91}{WeaknessIntoStrength\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-weakness@cref}{{[subfigure][1][7,15]7.15a}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-weakness}{{\M@TitleReference {a}{WeaknessIntoStrength\relax }}{91}{WeaknessIntoStrength\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-weakness@cref}{{[subfigure][1][7,15]7.15a}{[1][87][]91}}
\newlabel{fig:finetuning-smalllr-permethod-ood}{{\M@TitleReference {7.15b}{ProtectingIP-OOD\relax }}{91}{ProtectingIP-OOD\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-ood@cref}{{[subfigure][2][7,15]7.15b}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-ood}{{\M@TitleReference {b}{ProtectingIP-OOD\relax }}{91}{ProtectingIP-OOD\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-ood@cref}{{[subfigure][2][7,15]7.15b}{[1][87][]91}}
\newlabel{fig:finetuning-smalllr-permethod-pattern}{{\M@TitleReference {7.15c}{ProtectingIP-pattern\relax }}{91}{ProtectingIP-pattern\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-pattern@cref}{{[subfigure][3][7,15]7.15c}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-pattern}{{\M@TitleReference {c}{ProtectingIP-pattern\relax }}{91}{ProtectingIP-pattern\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-pattern@cref}{{[subfigure][3][7,15]7.15c}{[1][87][]91}}
\newlabel{fig:finetuning-smalllr-permethod-noise}{{\M@TitleReference {7.15d}{ProtectingIP-noies\relax }}{91}{ProtectingIP-noies\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-noise@cref}{{[subfigure][4][7,15]7.15d}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-noise}{{\M@TitleReference {d}{ProtectingIP-noies\relax }}{91}{ProtectingIP-noies\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-noise@cref}{{[subfigure][4][7,15]7.15d}{[1][87][]91}}
\newlabel{fig:finetuning-smalllr-permethod-piracy}{{\M@TitleReference {7.15e}{PiracyResistant\relax }}{91}{PiracyResistant\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-piracy@cref}{{[subfigure][5][7,15]7.15e}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-piracy}{{\M@TitleReference {e}{PiracyResistant\relax }}{91}{PiracyResistant\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-piracy@cref}{{[subfigure][5][7,15]7.15e}{[1][87][]91}}
\newlabel{fig:finetuning-smalllr-permethod-exponential}{{\M@TitleReference {7.15f}{ExponentialWeighting\relax }}{91}{ExponentialWeighting\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-exponential@cref}{{[subfigure][6][7,15]7.15f}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-exponential}{{\M@TitleReference {f}{ExponentialWeighting\relax }}{91}{ExponentialWeighting\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-exponential@cref}{{[subfigure][6][7,15]7.15f}{[1][87][]91}}
\newlabel{fig:finetuning-smalllr-permethod-frontier}{{\M@TitleReference {7.15g}{FrontierStitching\relax }}{91}{FrontierStitching\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-frontier@cref}{{[subfigure][7][7,15]7.15g}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-frontier}{{\M@TitleReference {g}{FrontierStitching\relax }}{91}{FrontierStitching\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-frontier@cref}{{[subfigure][7][7,15]7.15g}{[1][87][]91}}
\newlabel{fig:finetuning-smalllr-permethod-embedded}{{\M@TitleReference {7.15h}{WMEmbeddedSystems\relax }}{91}{WMEmbeddedSystems\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod-embedded@cref}{{[subfigure][8][7,15]7.15h}{[1][87][]91}}
\newlabel{sub@fig:finetuning-smalllr-permethod-embedded}{{\M@TitleReference {h}{WMEmbeddedSystems\relax }}{91}{WMEmbeddedSystems\relax }{figure.caption.85}{}}
\newlabel{sub@fig:finetuning-smalllr-permethod-embedded@cref}{{[subfigure][8][7,15]7.15h}{[1][87][]91}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces Influence of the trigger set size on robustness against fine-tuning with a small learning rate, $10^{-5}$ for MNIST and $10^{-4}$ for CIFAR-10. Each plot corresponds to one watermarking method and shows the results for all architectures.\relax }}{91}{figure.caption.85}\protected@file@percent }
\newlabel{fig:finetuning-smalllr-permethod}{{\M@TitleReference {7.15}{Influence of the trigger set size on robustness against fine-tuning with a small learning rate, $10^{-5}$ for MNIST and $10^{-4}$ for CIFAR-10. Each plot corresponds to one watermarking method and shows the results for all architectures.\relax }}{91}{Influence of the trigger set size on robustness against fine-tuning with a small learning rate, $10^{-5}$ for MNIST and $10^{-4}$ for CIFAR-10. Each plot corresponds to one watermarking method and shows the results for all architectures.\relax }{figure.caption.85}{}}
\newlabel{fig:finetuning-smalllr-permethod@cref}{{[figure][15][7]7.15}{[1][87][]91}}
\@setckpt{ch7_comparison}{
\setcounter{page}{92}
\setcounter{equation}{1}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{7}
\setcounter{section}{2}
\setcounter{subsection}{6}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{106}
\setcounter{lastsheet}{134}
\setcounter{lastpage}{120}
\setcounter{figure}{15}
\setcounter{lofdepth}{1}
\setcounter{table}{10}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{8}
\setcounter{subtable}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{1}
\setcounter{algocfproc}{1}
\setcounter{algocf}{1}
\setcounter{nag@c}{1}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{Item}{8}
\setcounter{Hfootnote}{7}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{77}
\setcounter{float@type}{4}
\setcounter{r@tfl@t}{2}
\setcounter{ALC@unique}{9}
\setcounter{ALC@line}{9}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{nlinenum}{0}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{section@level}{3}
}
