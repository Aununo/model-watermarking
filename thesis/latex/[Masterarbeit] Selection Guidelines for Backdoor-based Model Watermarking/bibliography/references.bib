
@article{kirkpatrick_overcoming_2017,
	title = {Overcoming catastrophic forgetting in neural networks},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {https://doi.org/10.1073/pnas.1611835114},
	doi = {10.1073/pnas.1611835114},
	abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
	language = {en},
	number = {13},
	urldate = {2021-09-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
	month = mar,
	year = {2017},
	pages = {3521--3526},
}

@article{freund_large_1999,
	title = {Large margin classification using the perceptron algorithm},
	volume = {37},
	issn = {08856125},
	url = {https://doi.org/10.1023/A:1007662407062},
	doi = {10.1023/A:1007662407062},
	number = {3},
	urldate = {2021-09-05},
	journal = {Machine Learning},
	author = {Freund, Yoav and Schapire, Robert E.},
	year = {1999},
	pages = {277--296},
}

@inproceedings{maas_rectifier_2013,
	address = {Atlanta, USA},
	series = {{ICML} '13},
	title = {Rectifier {Nonlinearities} {Improve} {Neural} {Network} {Acoustic} {Models}},
	volume = {28},
	abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectiﬁed linear (ReL) hidden units demonstrates additional gains in ﬁnal system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectiﬁer networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectiﬁer nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify diﬀerences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectiﬁer networks.},
	language = {en},
	booktitle = {30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
	year = {2013},
}

@inproceedings{he_delving_2015,
	address = {Santiago, Chile},
	series = {{ICCV} '15},
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {https://doi.org/10.1109/ICCV.2015.123},
	doi = {10.1109/ICCV.2015.123},
	urldate = {2021-09-05},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	pages = {1026--1034},
}

@inproceedings{huang_densely_2017,
	address = {Honolulu, USA},
	series = {{CVPR} '17},
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {https://doi.org/10.1109/CVPR.2017.243},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	language = {en},
	urldate = {2021-07-26},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {4700--4708},
}

@inproceedings{wang_attacks_2019,
	address = {Brighton, United Kingdom},
	series = {{ICASSP} '19},
	title = {Attacks on {Digital} {Watermarks} for {Deep} {Neural} {Networks}},
	isbn = {978-1-4799-8131-1},
	url = {https://doi.org/10.1109/ICASSP.2019.8682202},
	doi = {10.1109/ICASSP.2019.8682202},
	urldate = {2020-09-14},
	booktitle = {{IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Wang, Tianhao and Kerschbaum, Florian},
	month = apr,
	year = {2019},
	keywords = {ATTACKS WM, done},
	pages = {2622--2626},
}

@inproceedings{isakov_survey_2019,
	address = {Waltham, USA},
	series = {{HPEC} '19},
	title = {Survey of {Attacks} and {Defenses} on {Edge}-{Deployed} {Neural} {Networks}},
	url = {https://doi.org/10.1109/HPEC.2019.8916519},
	doi = {10.1109/HPEC.2019.8916519},
	abstract = {Deep Neural Network (DNN) workloads are quickly moving from datacenters onto edge devices, for latency, privacy, or energy reasons. While datacenter networks can be protected using conventional cybersecurity measures, edge neural networks bring a host of new security challenges. Unlike classic IoT applications, edge neural networks are typically very compute and memory intensive, their execution is data-independent, and they are robust to noise and faults. Neural network models may be very expensive to develop, and can potentially reveal information about the private data they were trained on, requiring special care in distribution. The hidden states and outputs of the network can also be used in reconstructing user inputs, potentially violating users' privacy. Furthermore, neural networks are vulnerable to adversarial attacks, which may cause misclassifications and violate the integrity of the output. These properties add challenges when securing edge-deployed DNNs, requiring new considerations, threat models, priorities, and approaches in securely and privately deploying DNNs to the edge. In this work, we cover the landscape of attacks on, and defenses, of neural networks deployed in edge devices and provide a taxonomy of attacks and defenses targeting edge DNNs.},
	booktitle = {{IEEE} {High} {Performance} {Extreme} {Computing} {Conference}},
	publisher = {IEEE},
	author = {Isakov, Mihailo and Gadepally, Vijay and Gettings, Karen M. and Kinsy, Michel A.},
	month = sep,
	year = {2019},
	keywords = {attacks on DNN, secondary literature},
	pages = {1--8},
}

@inproceedings{juuti_prada_2019,
	address = {Stockholm, Sweden},
	series = {{EuroS}\&{P} '19},
	title = {{PRADA}: {Protecting} {Against} {DNN} {Model} {Stealing} {Attacks}},
	isbn = {978-1-72811-148-3},
	shorttitle = {{PRADA}},
	url = {https://doi.org/10.1109/EuroSP.2019.00044},
	doi = {10.1109/EuroSP.2019.00044},
	abstract = {Machine learning (ML) applications are increasingly prevalent. Protecting the conﬁdentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to ﬁnd transferable adversarial examples that can evade classiﬁcation by the original model. Access to the model can be restricted to be only via well-deﬁned prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API.},
	language = {en},
	urldate = {2020-11-12},
	booktitle = {{IEEE} {European} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE},
	author = {Juuti, Mika and Szyller, Sebastian and Marchal, Samuel and Asokan, N.},
	month = jun,
	year = {2019},
	pages = {512--527},
}

@inproceedings{lach_fpga_1998,
	address = {Santa Clara, USA},
	title = {{FPGA} fingerprinting techniques for protecting intellectual property},
	isbn = {978-0-7803-4292-7},
	url = {http://ieeexplore.ieee.org/document/694986/},
	doi = {10.1109/CICC.1998.694986},
	language = {en},
	urldate = {2020-11-17},
	booktitle = {{IEEE} {Custom} {Integrated} {Circuits} {Conference}},
	publisher = {IEEE},
	author = {Lach, J. and Mangione-Smith, W.H. and Potkonjak, M.},
	year = {1998},
	pages = {299--302},
}

@inproceedings{maini_dataset_2021,
	series = {{ICLR} '21},
	title = {Dataset {Inference}: {Ownership} {Resolution} in {Machine} {Learning}},
	language = {en},
	booktitle = {International {Conference} on {Learning} {Representation}},
	author = {Maini, Pratyush and Yaghini, Mohammad and Papernot, Nicolas},
	year = {2021},
}

@inproceedings{kingma_adam_2015,
	address = {San Diego, USA},
	series = {{ICLR} '15},
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2021-09-05},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2015},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
}

@misc{chen_blackmarks_2019,
	title = {{BlackMarks}: {Blackbox} {Multibit} {Watermarking} for {Deep} {Neural} {Networks}},
	shorttitle = {{BlackMarks}},
	url = {http://arxiv.org/abs/1904.00344},
	abstract = {Deep Neural Networks have created a paradigm shift in our ability to comprehend raw data in various important fields ranging from computer vision and natural language processing to intelligence warfare and healthcare. While DNNs are increasingly deployed either in a white-box setting where the model internal is publicly known, or a black-box setting where only the model outputs are known, a practical concern is protecting the models against Intellectual Property (IP) infringement. We propose BlackMarks, the first end-to-end multi-bit watermarking framework that is applicable in the black-box scenario. BlackMarks takes the pre-trained unmarked model and the owner's binary signature as inputs and outputs the corresponding marked model with a set of watermark keys. To do so, BlackMarks first designs a model-dependent encoding scheme that maps all possible classes in the task to bit '0' and bit '1' by clustering the output activations into two groups. Given the owner's watermark signature (a binary string), a set of key image and label pairs are designed using targeted adversarial attacks. The watermark (WM) is then embedded in the prediction behavior of the target DNN by fine-tuning the model with generated WM key set. To extract the WM, the remote model is queried by the WM key images and the owner's signature is decoded from the corresponding predictions according to the designed encoding scheme. We perform a comprehensive evaluation of BlackMarks's performance on MNIST, CIFAR10, ImageNet datasets and corroborate its effectiveness and robustness. BlackMarks preserves the functionality of the original DNN and incurs negligible WM embedding runtime overhead as low as 2.054\%.},
	urldate = {2020-09-25},
	author = {Chen, Huili and Rouhani, Bita Darvish and Koushanfar, Farinaz},
	month = mar,
	year = {2019},
	note = {arXiv: 1904.00344},
	keywords = {DNN, WATERMARKING, done},
}

@inproceedings{zhong_random_2020,
	address = {New York, USA},
	title = {Random {Erasing} {Data} {Augmentation}},
	volume = {34},
	url = {https://doi.org/10.1609/aaai.v34i07.7000},
	doi = {10.1609/aaai.v34i07.7000},
	abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-ﬁtting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and ﬂipping, and yields consistent improvement over strong baselines in image classiﬁcation, object detection and person re-identiﬁcation. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
	language = {en},
	urldate = {2020-12-01},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	month = apr,
	year = {2020},
	pages = {13001--13008},
}

@inproceedings{zhang_model_2020,
	address = {New York, USA},
	title = {Model {Watermarking} for {Image} {Processing} {Networks}},
	volume = {34},
	url = {https://doi.org/10.1609/aaai.v34i07.6976},
	doi = {10.1609/aaai.v34i07.6976},
	abstract = {Deep learning has achieved tremendous success in numerous industrial applications. As training a good model often needs massive high-quality data and computation resources, the learned models often have signiﬁcant business values. However, these valuable deep models are exposed to a huge risk of infringements. For example, if the attacker has the full information of one target model including the network structure and weights, the model can be easily ﬁnetuned on new datasets. Even if the attacker can only access the output of the target model, he/she can still train another similar surrogate model by generating a large scale of input-output training pairs. How to protect the intellectual property of deep models is a very important but seriously under-researched problem. There are a few recent attempts at classiﬁcation network protection only.},
	language = {en},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Zhang, Jie and Chen, Dongdong and Liao, Jing and Fang, Han and Zhang, Weiming and Zhou, Wenbo and Cui, Hao and Yu, Nenghai},
	month = feb,
	year = {2020},
	keywords = {NEU, WATERMARKING, done},
	pages = {12805--12812},
}

@inproceedings{papernot_practical_2017,
	address = {Abu Dhabi, United Arab Emirates},
	series = {{ASIACCS} '17},
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	isbn = {978-1-4503-4944-4},
	url = {https://dl.acm.org/doi/10.1145/3052973.3053009},
	doi = {10.1145/3052973.3053009},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modiﬁed to yield erroneous model outputs, while appearing unmodiﬁed to human observers. Potential attacks include having malicious content like malware identiﬁed as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the ﬁrst practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and ﬁnd that they are misclassiﬁed by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We ﬁnd that their DNN misclassiﬁes 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassiﬁed by Amazon and Google at rates of 96.19\% and 88.94\%. We also ﬁnd that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	language = {en},
	urldate = {2021-08-27},
	booktitle = {{ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = apr,
	year = {2017},
	pages = {506--519},
}

@inproceedings{namba_robust_2019,
	address = {Auckland, New Zealand},
	series = {{ASIACCS} '19},
	title = {Robust {Watermarking} of {Neural} {Network} with {Exponential} {Weighting}},
	isbn = {978-1-4503-6752-3},
	url = {https://doi.org/10.1145/3321705.3329808},
	doi = {10.1145/3321705.3329808},
	abstract = {Deep learning has been achieving top levels of performance in many tasks. However, since it is costly to train a deep learning model, neural network models must be treated as valuable intellectual properties. One concern arising from our current situation is that malicious users might redistribute proprietary models or provide prediction services using such models without permission. One promising solution to this problem is digital watermarking, which works by embedding a mechanism into the model so that the model owners can verify their ownership of the model externally. In this study, we present a novel attack method against such watermarks known as query modification and demonstrate that all currently existing watermarking methods are vulnerable to either query modification or other existing attack methods (such as model modification). To overcome these vulnerabilities, we then present a novel watermarking method that we have named exponential weighting and experimentally show that our watermarking method achieves high watermark verification performance even under malicious invalidation processing attempts by unauthorized service providers (such as model modification and query modification) without sacrificing the predictive performance of the neural network model itself.},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {{ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Namba, Ryota and Sakuma, Jun},
	month = jul,
	year = {2019},
	keywords = {ATTACKS WM, DNN, WATERMARKING, done, incl implementation},
	pages = {228--240},
}

@inproceedings{zhang_protecting_2018,
	address = {Incheon, Republic of Korea},
	series = {{ASIACCS} '18},
	title = {Protecting {Intellectual} {Property} of {Deep} {Neural} {Networks} with {Watermarking}},
	isbn = {978-1-4503-5576-6},
	url = {https://doi.org/10.1145/3196494.3196550},
	doi = {10.1145/3196494.3196550},
	language = {en},
	urldate = {2020-09-14},
	booktitle = {{ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Zhang, Jialong and Gu, Zhongshu and Jang, Jiyong and Wu, Hui and Stoecklin, Marc Ph. and Huang, Heqing and Molloy, Ian},
	month = jun,
	year = {2018},
	keywords = {DNN, WATERMARKING, black-box access, done},
	pages = {159--172},
}

@inproceedings{xu_adding_2020,
	address = {Wuhan, China},
	series = {{MIPPR} '19},
	title = {Adding identity numbers to deep neural networks},
	volume = {11429},
	url = {https://doi.org/10.1117/12.2540293},
	doi = {10.1117/12.2540293},
	abstract = {Amid the maturity of machine learning, deep neural networks are gradually applied in the business sector rather than be restricted in the laboratory. However, its intellectual property protection encounters a significant challenge. In this paper, we aim at embedding a unique identity number (ID) to the deep neural network for model ownership verification. To this end, a scheme of generating DNN ID is proposed, which is the criterion for model ownership verification. After embedding, the model can complete the original performance and own a unique ID of this model as well. DNN ID can only be generated by the owner to check the model authorship. We evaluate this method on MNIST. Experiment results demonstrate that the DNN ID can accurately verify the ownership of our trained model.},
	urldate = {2020-09-28},
	booktitle = {11th {International} {Symposium} on {Multispectral} {Image} {Processing} and {Pattern} {Recognition}},
	publisher = {SPIE},
	author = {Xu, Xiangrui and Li, Yaqin and Gao, Yunlong and Yuan, Cao},
	month = feb,
	year = {2020},
	keywords = {duplicate},
	pages = {205--209},
}

@inproceedings{xu_show_2015,
	title = {Show, {Attend} and {Tell}: {Neural} {Image} {CaptionGeneration} with {Visual} {Attention}},
	volume = {37},
	abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to ﬁx its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.},
	language = {en},
	booktitle = {32nd {International} {Conference} on {Machine} {Learning}},
	author = {Xu, Kelvin and Lei, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard S and Bengio, Yoshua},
	year = {2015},
	pages = {2048--2057},
}

@inproceedings{wang_watermarking_2020,
	title = {Watermarking in {Deep} {Neural} {Networks} via {Error} {Back}-propagation},
	volume = {2020},
	url = {https://doi.org/10.2352/ISSN.2470-1173.2020.4.MWSF-022},
	doi = {10.2352/ISSN.2470-1173.2020.4.MWSF-022},
	abstract = {Recent advances in deep learning (DL) have led to great success in tasks of computer vision and pattern recognition. Sharing pre-trained DL models has been an important means to promote the rapid progress of research community and development of DL based systems. However, it also raises challenges to model authentication. It is quite necessary to protect the ownership of the DL models to be released. In this paper, we present a digital watermarking technique to deep neural networks (DNNs). We propose to mark a DNN by inserting an independent neural network that allows us to use selective weights for watermarking. The independent neural network is only used in the training phase and watermark veriﬁcation phase, and will not be released publicly. Experiments have shown that, the performance of marked DNN on its original task will not be degraded signiﬁcantly. Meantime, the watermark can be successfully embedded and extracted with a low neural network loss even under the common attacks including model ﬁne-tuning and compression, which has shown the superiority and applicability of the proposed work.},
	language = {en},
	urldate = {2020-11-20},
	booktitle = {{IS}\&{T} {International} {Symposium} on {Electronic} {Imaging} 2020},
	publisher = {Society for Imaging Science and Technology},
	author = {Wang, Jiangfeng and Wu, Hanzhou and Zhang, Xinpeng and Yao, Yuwei},
	month = jan,
	year = {2020},
	keywords = {NEU, WATERMARKING, done, white-box access},
}

@inproceedings{wang_neural_2019,
	address = {San Francisco, USA},
	series = {S\&{P} '19},
	title = {Neural {Cleanse}: {Identifying} and {Mitigating} {Backdoor} {Attacks} in {Neural} {Networks}},
	isbn = {978-1-5386-6660-9},
	shorttitle = {Neural {Cleanse}},
	url = {https://doi.org/10.1109/SP.2019.00031},
	doi = {10.1109/SP.2019.00031},
	abstract = {Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classiﬁcation to produce unexpected results. For example, a model with a backdoor always identiﬁes a face as Bill Gates if a speciﬁc symbol is present in the input. Backdoors can stay hidden indeﬁnitely until activated by an input, and present a serious security risk to many security or safety related applications, e.g., biometric authentication systems or self-driving cars.},
	language = {en},
	urldate = {2020-11-12},
	booktitle = {{IEEE} {Symposium} on {Security} and {Privacy}},
	publisher = {IEEE},
	author = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},
	month = may,
	year = {2019},
	keywords = {ATTACKS WM, backward snowballing},
	pages = {707--723},
}

@inproceedings{uchida_embedding_2017,
	address = {Bucharest, Romania},
	series = {{ICMR} '17},
	title = {Embedding {Watermarks} into {Deep} {Neural} {Networks}},
	isbn = {978-1-4503-4701-3},
	url = {https://doi.org/10.1145/3078971.3078974},
	doi = {10.1145/3078971.3078974},
	abstract = {Significant progress has been made with deep neural networks recently. Sharing trained models of deep neural networks has been a very important in the rapid progress of research and development of these systems. At the same time, it is necessary to protect the rights to shared trained models. To this end, we propose to use digital watermarking technology to protect intellectual property and detect intellectual property infringement in the use of trained models. First, we formulate a new problem: embedding watermarks into deep neural networks. Second, we propose a general framework for embedding a watermark in model parameters, using a parameter regularizer. Our approach does not impair the performance of networks into which a watermark is placed because the watermark is embedded while training the host network. Finally, we perform comprehensive experiments to reveal the potential of watermarking deep neural networks as the basis of this new research effort. We show that our framework can embed a watermark during the training of a deep neural network from scratch, and during fine-tuning and distilling, without impairing its performance. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark remains complete even after 65\% of parameters are pruned.},
	language = {en},
	urldate = {2020-09-22},
	booktitle = {{ACM} on {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Uchida, Yusuke and Nagai, Yuki and Sakazawa, Shigeyuki and Satoh, Shin'ichi},
	month = jun,
	year = {2017},
	keywords = {DNN, WATERMARKING, done, incl implementation, white-box access},
	pages = {269--277},
}

@inproceedings{tramer_stealing_2016,
	address = {Austin, USA},
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	isbn = {978-1-931971-32-4},
	language = {en},
	booktitle = {25th {USENIX} {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Tramèr, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K. and Ristenpart, Thomas},
	month = aug,
	year = {2016},
	pages = {601--618},
}

@inproceedings{szegedy_intriguing_2014,
	address = {Banff, Canada},
	series = {{ICLR} '14},
	title = {Intriguing properties of neural networks},
	booktitle = {2nd {International} {Conference} on {Learning} {Representations}},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian J. and Fergus, Rob},
	month = apr,
	year = {2014},
}

@inproceedings{sun_convolutional_2018,
	address = {Shenzhen, China},
	series = {{CSAI} '18},
	title = {Convolutional {Neural} {Network} {Protection} {Method} of {Lenet}-5-{Like} {Structure}},
	isbn = {978-1-4503-6606-9},
	url = {https://doi.org/10.1145/3297156.3297224},
	doi = {10.1145/3297156.3297224},
	abstract = {In this paper, we build and describe a convolutional neural network protection method of Lenet-5-like structure. The protected convolutional neural network is obtained by adding the protection lock module in forward propagation algorithm in different layers so that unauthorized user can not use protected convolutional neural network of Lenet-5-like structure. The experimental results show that protection lock module be added in first and middle layers would gain the best protection effect. Authorized users can use protected convolutional neural network of Lenet-5-like structure after adding unlocking module. The accuracy of unlocked convolutional neural network is not affected by adding protection lock module and unlocking module. To add protection lock module realize protection on the convolutional neural network of Lenet-5-like structure.},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {2nd {International} {Conference} on {Computer} {Science} and {Artificial} {Intelligence}},
	publisher = {ACM},
	author = {Sun, Lei and Wang, Yuehan and Dai, Leyu},
	month = dec,
	year = {2018},
	keywords = {DNN, PROACTIVE IPP, done rudi},
	pages = {77--80},
}

@inproceedings{song_auditing_2019,
	address = {Anchorage, USA},
	series = {{KDD} '19},
	title = {Auditing {Data} {Provenance} in {Text}-{Generation} {Models}},
	isbn = {978-1-4503-6201-6},
	url = {https://doi.org/10.1145/3292500.3330885},
	doi = {10.1145/3292500.3330885},
	abstract = {To help enforce data-protection regulations such as GDPR and detect unauthorized uses of personal data, we develop a new model auditing technique that helps users check if their data was used to train a machine learning model. We focus on auditing deeplearning models that generate natural-language text, including word prediction and dialog generation. These models are at the core of popular online services and are often trained on personal data such as users’ messages, searches, chats, and comments.},
	language = {en},
	urldate = {2020-12-01},
	booktitle = {25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Song, Congzheng and Shmatikov, Vitaly},
	month = jul,
	year = {2019},
	pages = {196--206},
}

@inproceedings{song_machine_2017,
	address = {Dallas, USA},
	series = {{CCS} '17},
	title = {Machine {Learning} {Models} that {Remember} {Too} {Much}},
	isbn = {978-1-4503-4946-8},
	url = {https://doi.org/10.1145/3133956.3134077},
	doi = {10.1145/3133956.3134077},
	abstract = {Machine learning (ML) is becoming a commodity. Numerous ML frameworks and services are available to data holders who are not ML experts but want to train predictive models on their data. It is important that ML models trained on sensitive inputs (e.g., personal images or documents) not leak too much information about the training data.},
	language = {en},
	urldate = {2020-11-25},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Song, Congzheng and Ristenpart, Thomas and Shmatikov, Vitaly},
	month = oct,
	year = {2017},
	pages = {587--601},
}

@inproceedings{salakhutdinov_learning_2007,
	address = {San Juan, Puerto Rico},
	title = {Learning a {Nonlinear} {Embedding} by {Preserving} {Class} {Neighbourhood} {Structure}},
	abstract = {We show how to pretrain and ﬁne-tune a multilayer neural network to learn a nonlinear transformation from the input space to a lowdimensional feature space in which K-nearest neighbour classiﬁcation performs well. We also show how the non-linear transformation can be improved using unlabeled data. Our method achieves a much lower error rate than Support Vector Machines or standard backpropagation on a widely used version of the MNIST handwritten digit recognition task. If some of the dimensions of the low-dimensional feature space are not used for nearest neighbor classiﬁcation, our method uses these dimensions to explicitly represent transformations of the digits that do not affect their identity.},
	language = {en},
	booktitle = {11th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
	year = {2007},
	pages = {412--419},
}

@inproceedings{rouhani_deepsigns_2019,
	address = {Providence, USA},
	title = {{DeepSigns}: {An} {End}-to-{End} {Watermarking} {Framework} for {Ownership} {Protection} of {Deep} {Neural} {Networks}},
	isbn = {978-1-4503-6240-5},
	shorttitle = {{DeepSigns}},
	url = {https://dl.acm.org/doi/10.1145/3297858.3304051},
	doi = {10.1145/3297858.3304051},
	language = {en},
	urldate = {2020-09-14},
	booktitle = {24th {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Rouhani, Bita Darvish and Chen, Huili and Koushanfar, Farinaz},
	month = apr,
	year = {2019},
	keywords = {DNN, WATERMARKING, black-box access, done, incl implementation, white-box access},
	pages = {485--497},
}

@inproceedings{ribeiro_mlaas_2015,
	address = {Miami, USA},
	title = {{MLaaS}: {Machine} {Learning} as a {Service}},
	url = {https://doi.org/10.1109/ICMLA.2015.152},
	doi = {10.1109/ICMLA.2015.152},
	abstract = {The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-speciﬁc data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a ﬂexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.},
	language = {en},
	booktitle = {{IEEE} 14th {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	publisher = {IEEE},
	author = {Ribeiro, Mauro and Grolinger, Katarina and Capretz, Miriam A M},
	year = {2015},
	pages = {896--902},
}

@inproceedings{quiring_adversarial_2018,
	address = {Rome, Italy},
	series = {{EUSIPCO} '18},
	title = {Adversarial {Machine} {Learning} {Against} {Digital} {Watermarking}},
	isbn = {978-90-827970-1-5},
	url = {https://doi.org/10.23919/EUSIPCO.2018.8553343},
	doi = {10.23919/EUSIPCO.2018.8553343},
	urldate = {2020-11-27},
	booktitle = {26th {European} {Signal} {Processing} {Conference}},
	publisher = {IEEE},
	author = {Quiring, Erwin and Rieck, Konrad},
	month = sep,
	year = {2018},
	pages = {519--523},
}

@inproceedings{potdar_survey_2005,
	address = {Perth, Australia},
	title = {A survey of digital image watermarking techniques},
	isbn = {978-0-7803-9094-2},
	url = {https://doi.org/10.1109/INDIN.2005.1560462},
	doi = {10.1109/INDIN.2005.1560462},
	urldate = {2020-11-20},
	booktitle = {3rd {IEEE} {International} {Conference} on {Industrial} {Informatics}},
	publisher = {IEEE},
	author = {Potdar, V.M. and Han, S. and Chang, E.},
	year = {2005},
	pages = {709--716},
}

@inproceedings{mosafi_stealing_2019,
	address = {Budapest, Hungary},
	title = {Stealing {Knowledge} from {Protected} {Deep} {Neural} {Networks} {Using} {Composite} {Unlabeled} {Data}},
	url = {https://doi.org/10.1109/IJCNN.2019.8851798},
	doi = {10.1109/IJCNN.2019.8851798},
	abstract = {As state-of-the-art deep neural networks are deployed at the core of more advanced Al-based products and services, the incentive for copying them (i.e., their intellectual properties) by rival adversaries is expected to increase considerably over time. The best way to extract or steal knowledge from such networks is by querying them using a large dataset of random samples and recording their output, followed by training a student network to mimic these outputs, without making any assumption about the original networks. The most effective way to protect against such a mimicking attack is to provide only the classification result, without confidence values associated with the softmax layer.In this paper, we present a novel method for generating composite images for attacking a mentor neural network using a student model. Our method assumes no information regarding the mentor's training dataset, architecture, or weights. Further assuming no information regarding the mentor's softmax output values, our method successfully mimics the given neural network and steals all of its knowledge. We also demonstrate that our student network (which copies the mentor) is impervious to watermarking protection methods, and thus would not be detected as a stolen model.Our results imply, essentially, that all current neural networks are vulnerable to mimicking attacks, even if they do not divulge anything but the most basic required output, and that the student model which mimics them cannot be easily detected and singled out as a stolen copy using currently available techniques.},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Mosafi, Itay and David, Eli Omid and Netanyahu, Nathan S.},
	month = jul,
	year = {2019},
	note = {ISSN: 2161-4407},
	keywords = {model extraction},
	pages = {1--8},
}

@inproceedings{milli_model_2019,
	address = {Atlanta, USA},
	series = {{FAT}* '19},
	title = {Model {Reconstruction} from {Model} {Explanations}},
	isbn = {978-1-4503-6125-5},
	url = {https://doi.org/10.1145/3287560.3287562},
	doi = {10.1145/3287560.3287562},
	abstract = {We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations.},
	language = {en},
	urldate = {2020-10-13},
	booktitle = {Conference on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Milli, Smitha and Schmidt, Ludwig and Dragan, Anca D. and Hardt, Moritz},
	year = {2019},
	pages = {1--9},
}

@inproceedings{li_how_2019,
	address = {San Juan, Puerto Rico},
	title = {How to prove your model belongs to you: a blind-watermark based framework to protect intellectual property of {DNN}},
	isbn = {978-1-4503-7628-0},
	shorttitle = {How to prove your model belongs to you},
	url = {https://dl.acm.org/doi/10.1145/3359789.3359801},
	doi = {10.1145/3359789.3359801},
	abstract = {Deep learning techniques have made tremendous progress in a variety of challenging tasks, such as image recognition and machine translation, during the past decade. Training deep neural networks is computationally expensive and requires both human and intellectual resources. Therefore, it is necessary to protect the intellectual property of the model and externally verify the ownership of the model. However, previous studies either fail to defend against the evasion attack or have not explicitly dealt with fraudulent claims of ownership by adversaries. Furthermore, they can not establish a clear association between the model and the creator’s identity.},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {35th {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Li, Zheng and Hu, Chengyu and Zhang, Yang and Guo, Shanqing},
	month = dec,
	year = {2019},
	keywords = {DNN, WATERMARKING, black-box access, done, incl implementation},
	pages = {126--137},
}

@inproceedings{kupek_difficulty_2020,
	address = {Denver, USA},
	series = {{IH}\&{MMSec} '20},
	title = {On the {Difficulty} of {Hiding} {Keys} in {Neural} {Networks}},
	isbn = {978-1-4503-7050-9},
	url = {https://doi.org/10.1145/3369412.3395076},
	doi = {10.1145/3369412.3395076},
	abstract = {In order to defend neural networks against malicious attacks, recent approaches propose the use of secret keys in the training or inference pipelines of learning systems. While this concept is innovative and the results are promising in terms of attack mitigation and classification accuracy, the effectiveness relies on the secrecy of the key. However, this aspect is often not discussed. In this short paper, we explore this issue for the case of a recently proposed key-based deep neural network. White-box experiments on multiple models and datasets, using the original key-based method and our own extensions, show that it is currently possible to extract secret key bits with relatively limited effort.},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {{ACM} {Workshop} on {Information} {Hiding} and {Multimedia} {Security}},
	publisher = {ACM},
	author = {Kupek, Tobias and Pasquini, Cecilia and Böhme, Rainer},
	month = jun,
	year = {2020},
	keywords = {DNN, done rudi, not sure},
	pages = {73--78},
}

@inproceedings{kahng_watermarking_1998,
	address = {San Francisco, USA},
	series = {{DAC} '98},
	title = {Watermarking {Techniques} for {Intellectual} {Property} {Protection}},
	isbn = {0-89791-964-5},
	url = {https://doi.org/10.1145/277044.277240},
	doi = {10.1145/277044.277240},
	abstract = {Digital system designs are the product of valuable effort and knowhow. Their embodiments, from software and HDL program down to device-level netlist and mask data, represent carefully guarded intellectual property (IP). Hence, design methodologies based on IP reuse require new mechanisms to protect the rights of IP producers and owners. This paper establishes principles of watermarkingbased IP protection, where a watermark is a mechanism for identiﬁcation that is (i) nearly invisible to human and machine inspection, (ii) difﬁcult to remove, and (iii) permanently embedded as an integral part of the design. We survey related work in cryptography and design methodology, then develop desiderata, metrics and example approaches – centering on constraint-based techniques – for watermarking at various stages of the VLSI design process.},
	language = {en},
	booktitle = {35th {Annual} {Design} {Automation} {Conference}},
	author = {Kahng, A B and Lach, J and Mangione-Smith, W H and Mantik, S and Markov, I L and Potkonjak, M and Tucker, P and Wang, H and Wolfe, G},
	year = {1998},
	pages = {776--781},
}

@inproceedings{hitaj_evasion_2019,
	address = {Rome, Italy},
	title = {Evasion {Attacks} {Against} {Watermarking} {Techniques} found in {MLaaS} {Systems}},
	isbn = {978-1-72810-722-6},
	url = {https://doi.org/10.1109/SDS.2019.8768572},
	doi = {10.1109/SDS.2019.8768572},
	urldate = {2020-09-14},
	booktitle = {6th {International} {Conference} on {Software} {Defined} {Systems} ({SDS})},
	publisher = {IEEE},
	author = {Hitaj, Dorjan and Hitaj, Briland and Mancini, Luigi V.},
	month = jun,
	year = {2019},
	keywords = {ATTACKS WM, done},
	pages = {55--63},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, USA},
	series = {{CVPR} '16},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {https://doi.org/10.1109/CVPR.2016.90},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2021-07-06},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@inproceedings{guo_watermarking_2018,
	address = {San Diego, California},
	series = {{ICCAD} '18},
	title = {Watermarking deep neural networks for embedded systems},
	isbn = {978-1-4503-5950-4},
	url = {https://doi.org/10.1145/3240765.3240862},
	doi = {10.1145/3240765.3240862},
	language = {en},
	urldate = {2020-09-14},
	booktitle = {International {Conference} on {Computer}-{Aided} {Design}},
	publisher = {ACM},
	author = {Guo, Jia and Potkonjak, Miodrag},
	month = nov,
	year = {2018},
	keywords = {DNN, WATERMARKING, black-box access, done},
}

@inproceedings{guan_reversible_2020,
	address = {Seattle, USA},
	series = {{MM} '20},
	title = {Reversible {Watermarking} in {Deep} {Convolutional} {Neural} {Networks} for {Integrity} {Authentication}},
	isbn = {978-1-4503-7988-5},
	url = {https://doi.org/10.1145/3394171.3413729},
	doi = {10.1145/3394171.3413729},
	abstract = {Deep convolutional neural networks have made outstanding contributions in many fields such as computer vision in the past few years and many researchers published well-trained network for downloading. But recent studies have shown serious concerns about integrity due to model-reuse attacks and backdoor attacks. In order to protect these open-source networks, many algorithms have been proposed such as watermarking. However, these existing algorithms modify the contents of the network permanently and are not suitable for integrity authentication. In this paper, we propose a reversible watermarking algorithm for integrity authentication. Specifically, we present the reversible watermarking problem of deep convolutional neural networks and utilize the pruning theory of model compression technology to construct a host sequence used for embedding watermarking information by histogram shift. As shown in the experiments, the influence of embedding reversible watermarking on the classification performance is less than ±0.5\% and the parameters of the model can be fully recovered after extracting the watermarking. At the same time, the integrity of the model can be verified by applying the reversible watermarking: if the model is modified illegally, the authentication information generated by original model will be absolutely different from the extracted watermarking information.},
	language = {en},
	urldate = {2020-11-20},
	booktitle = {28th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Guan, Xiquan and Feng, Huamin and Zhang, Weiming and Zhou, Hang and Zhang, Jie and Yu, Nenghai},
	month = oct,
	year = {2020},
	keywords = {NEU, WATERMARKING, done},
	pages = {2273--2280},
}

@inproceedings{goodfellow_generative_2014,
	address = {Montreal, Canada},
	title = {Generative {Adversarial} {Nets}},
	volume = {2},
	language = {en},
	booktitle = {27th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
	pages = {2671--2680},
}

@inproceedings{gomez_security_2019,
	address = {Prague, Czech Republic},
	title = {Security for {Distributed} {Deep} {Neural} {Networks}: {Towards} {Data} {Confidentiality} \& {Intellectual} {Property} {Protection}},
	isbn = {978-989-758-378-0},
	shorttitle = {Security for {Distributed} {Deep} {Neural} {Networks}},
	url = {https://doi.org/10.5220/0007922404390447},
	doi = {10.5220/0007922404390447},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {16th {International} {Joint} {Conference} on e-{Business} and {Telecommunications}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Gomez, Laurent and Wilhelm, Marcus and Márquez, José and Duverger, Patrick},
	month = jul,
	year = {2019},
	keywords = {DNN, PROACTIVE IPP, done rudi},
	pages = {439--447},
}

@inproceedings{ganju_property_2018,
	address = {Toronto, Canada},
	series = {{CCS}’18},
	title = {Property {Inference} {Attacks} on {Fully} {Connected} {Neural} {Networks} using {Permutation} {Invariant} {Representations}},
	url = {https://doi.org/10.1145/3243734.3243834},
	doi = {10.1145/3243734.3243834},
	abstract = {With the growing adoption of machine learning, sharing of learned models is becoming popular. However, in addition to the prediction properties the model producer aims to share, there is also a risk that the model consumer can infer other properties of the training data the model producer did not intend to share. In this paper, we focus on the inference of global properties of the training data, such as the environment in which the data was produced, or the fraction of the data that comes from a certain class, as applied to white-box Fully Connected Neural Networks (FCNNs).},
	language = {en},
	booktitle = {{ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Ganju, Karan and Wang, Qi and Yang, Wei and Gunter, Carl A and Borisov, Nikita},
	month = oct,
	year = {2018},
	pages = {619--633},
}

@inproceedings{fan_rethinking_2019,
	title = {Rethinking {Deep} {Neural} {Network} {Ownership} {Verification}: {Embedding} {Passports} to {Defeat} {Ambiguity} {Attacks}},
	volume = {32},
	abstract = {With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR},
	booktitle = {33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Neural information processing systems foundation},
	author = {Fan, Lixin and Ng, Kam Woh and Chan, Chee Seng},
	month = dec,
	year = {2019},
	keywords = {DNN, PROACTIVE IPP, done, incl implementation},
	pages = {4714--4723},
}

@inproceedings{eckersley_how_2010,
	title = {How {Unique} {Is} {Your} {Web} {Browser}?},
	isbn = {978-3-642-14526-1 978-3-642-14527-8},
	url = {http://link.springer.com/10.1007/978-3-642-14527-8_1},
	urldate = {2020-11-21},
	booktitle = {Symposium on {Privacy} {Enhancing} {Technologies} {Symposium}},
	publisher = {Springer Berlin Heidelberg},
	author = {Eckersley, Peter},
	year = {2010},
	doi = {10.1007/978-3-642-14527-8_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {1--18},
}

@inproceedings{deeba_protecting_2019,
	address = {Shanghai, China},
	series = {{ICISE} '19},
	title = {Protecting the {Intellectual} {Properties} of {Digital} {Watermark} {Using} {Deep} {Neural} {Network}},
	isbn = {978-1-72812-558-9},
	url = {https://doi.org/10.1109/ICISE.2019.00025},
	doi = {10.1109/ICISE.2019.00025},
	urldate = {2020-09-14},
	booktitle = {4th {International} {Conference} on {Information} {Systems} {Engineering}},
	publisher = {IEEE},
	author = {Deeba, Farah and Tefera, Getenet and Kun, She and Memon, Hira},
	month = may,
	year = {2019},
	keywords = {duplicate, secondary literature},
	pages = {91--95},
}

@inproceedings{correia-silva_copycat_2018,
	address = {Rio de Janeiro, Brazil},
	series = {{IJCNN} '18},
	title = {Copycat {CNN}: {Stealing} {Knowledge} by {Persuading} {Confession} with {Random} {Non}-{Labeled} {Data}},
	shorttitle = {Copycat {CNN}},
	url = {https://doi.org/10.1109/IJCNN.2018.8489592},
	doi = {10.1109/IJCNN.2018.8489592},
	abstract = {In the past few years, Convolutional Neural Networks (CNNs) have been achieving state-of-the-art performance on a variety of problems. Many companies employ resources and money to generate these models and provide them as an API, therefore it is in their best interest to protect them, i.e., to avoid that someone else copy them. Recent studies revealed that stateof-the-art CNNs are vulnerable to adversarial examples attacks, and this weakness indicates that CNNs do not need to operate in the problem domain (PD). Therefore, we hypothesize that they also do not need to be trained with examples of the PD in order to operate in it.},
	language = {en},
	urldate = {2020-10-06},
	booktitle = {International {Joint} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Correia-Silva, Jacson Rodrigues and Berriel, Rodrigo F. and Badue, Claudine and de Souza, Alberto F. and Oliveira-Santos, Thiago},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	pages = {1--8},
}

@inproceedings{coates_analysis_2011,
	title = {An {Analysis} of {Single}-{Layer} {Networks} in {Unsupervised} {Feature} {Learning}},
	abstract = {A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Speciﬁcally, we will apply several oﬀthe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the eﬀect of changes in the model setup: the receptive ﬁeld size, number of hidden nodes (features), the step-size (“stride”) between extracted features, and the eﬀect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance—so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6\% and 97.2\% respectively).},
	language = {en},
	booktitle = {14th {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Coates, Adam and Lee, Honglak and Ng, Andrew Y},
	year = {2011},
	pages = {215--223},
}

@inproceedings{chen_leveraging_2019,
	title = {Leveraging {Unlabeled} {Data} for {Watermark} {Removal} of {Deep} {Neural} {Networks}},
	abstract = {Deep neural networks have achieved tremendous success in various ﬁelds; however, training these models from scratch could be computationally expensive and requires a lot of training data. Therefore, recent work has explored different watermarking techniques to protect the pre-trained deep neural networks from potential copyright infringements. Although several existing techniques could effectively embed such watermarks into the DNNs, they could be vulnerable to adversaries who aim at removing the watermarks. In this work, we demonstrate that a carefullydesigned ﬁne-tuning method enables the adversary with limited training data to effectively remove the watermarks, without compromising the model functionality. In particular, leveraging auxiliary unlabeled data signiﬁcantly decreases the amount of labeled training data needed for effective watermark removal, even if the unlabeled data samples are not drawn from the same distribution as the benign data for model evaluation.},
	language = {en},
	booktitle = {{ICML} {Workshop} on {Security} and {Privacy} of {Machine} {Learning}},
	author = {Chen, Xinyun and Wang, Wenxiao and Ding, Yiming and Bender, Chris and Jia, Ruoxi and Li, Bo and Song, Dawn},
	month = jun,
	year = {2019},
	keywords = {ATTACKS WM, done},
}

@inproceedings{chen_protect_2018,
	address = {Hong Kong, Hong Kong},
	series = {{WIFS} '18},
	title = {Protect {Your} {Deep} {Neural} {Networks} from {Piracy}},
	isbn = {978-1-5386-6536-7},
	url = {https://doi.org/10.1109/WIFS.2018.8630791},
	doi = {10.1109/WIFS.2018.8630791},
	abstract = {Building an effective DNN model requires massive human-labeled training data, powerful computing hardware and researchers’ skills and efforts. Successful DNN models are becoming important intellectual properties for the model owners and should be protected from unauthorized access and piracy. This paper proposes a novel framework to provide access control to the trained deep neural networks so that only authorized users can utilize them properly. The proposed framework is capable of keeping the DNNs functional to authorized access while dysfunctional to unauthorized access or illicit use. The proposed framework is evaluated on the MNIST, Fashion, and CIFAR10 datasets to demonstrate its effectiveness to protect the trained DNNs from unauthorized access. The security of the proposed framework is examined against the potential attacks from unauthorized users. The experimental results show that the trained DNN models under the proposed framework maintain high accuracy to authorized access while having a low accuracy to unauthorized users, and they are resistant to several types of attacks.},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {{IEEE} {International} {Workshop} on {Information} {Forensics} and {Security}},
	publisher = {IEEE},
	author = {Chen, Mingliang and Wu, Min},
	year = {2018},
	keywords = {DNN, PROACTIVE IPP, done rudi},
	pages = {1--7},
}

@inproceedings{chen_deepmarks_2019,
	address = {Ottawa, Canada},
	series = {{ICMR} '19},
	title = {{DeepMarks}: {A} {Secure} {Fingerprinting} {Framework} for {Digital} {Rights} {Management} of {Deep} {Learning} {Models}},
	isbn = {978-1-4503-6765-3},
	shorttitle = {{DeepMarks}},
	url = {https://doi.org/10.1145/3323873.3325042},
	doi = {10.1145/3323873.3325042},
	abstract = {Deep Neural Networks (DNNs) are revolutionizing various critical fields by providing an unprecedented leap in terms of accuracy and functionality. Due to the costly training procedure, highperformance DNNs are typically considered as the Intellectual Property (IP) of the model builder and need to be protected. While DNNs are increasingly commercialized, the pre-trained models might be illegally copied or redistributed after they are delivered to malicious users. In this paper, we introduce DeepMarks, the first endto-end collusion-secure fingerprinting framework that enables the owner to retrieve model authorship information and identification of unique users in the context of deep learning (DL). DeepMarks consists of two main modules: (i) Designing unique fingerprints using anti-collusion codebooks for individual users; and (ii) Encoding each constructed fingerprint (FP) in the probability density function (pdf) of the weights by incorporating an FP-specific regularization loss during DNN re-training. We investigate the performance of DeepMarks on various datasets and DNN architectures. Experimental results show that the embedded FP preserves the accuracy of the host DNN and is robust against different model modifications that might be conducted by the malicious user. Furthermore, our framework is scalable and yields perfect detection rates and no false alarms when identifying the participants of FP collusion attacks under theoretical guarantee. The runtime overhead of retrieving the embedded FP from the marked DNN can be as low as 0.056\%.},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {International {Conference} on {Multimedia} {Retrieval}},
	publisher = {ACM},
	author = {Chen, Huili and Rouhani, Bita Darvish and Fu, Cheng and Zhao, Jishen and Koushanfar, Farinaz},
	year = {2019},
	keywords = {DNN, FINGERPRINTING, done},
	pages = {105--113},
}

@inproceedings{chen_specmark_2020,
	title = {{SpecMark}: {A} {Spectral} {Watermarking} {Framework} for {IP} {Protection} of {Speech} {Recognition} {Systems}},
	shorttitle = {{SpecMark}},
	url = {https://doi.org/10.21437/Interspeech.2020-2787},
	doi = {10.21437/Interspeech.2020-2787},
	abstract = {Automatic Speech Recognition (ASR) systems are widely deployed in various applications due to their superior performance. However, obtaining a highly accurate ASR model is non-trivial since it requires the availability of a massive amount of proprietary training data and enormous computational resources. As such, pre-trained ASR models shall be considered as the intellectual property (IP) of the model designer and protected against copyright infringement attacks. In this paper, we propose SpecMark, the ﬁrst spectral watermarking framework that seamlessly embeds a watermark (WM) in the spectrum of the ASR model for ownership proof. SpecMark identiﬁes the signiﬁcant frequency components of the model parameters and encodes the owner’s WM in the corresponding spectrum region before sharing the model with end-users. The model builder can later extract the spectral WM to verify his ownership of the marked ASR system. We evaluate SpecMark’s performance using DeepSpeech model with three different speech datasets. Empirical results corroborate that SpecMark incurs negligible overhead and preserves the recognition accuracy of the original system. Furthermore, SpecMark sustains diverse model modiﬁcations, including parameter pruning and transfer learning.},
	language = {en},
	urldate = {2020-11-20},
	booktitle = {Interspeech 2020},
	publisher = {ISCA},
	author = {Chen, Huili and Darvish, Bita and Koushanfar, Farinaz},
	month = oct,
	year = {2020},
	keywords = {NEU, WATERMARKING, done},
	pages = {2312--2316},
}

@inproceedings{chakraborty_hardware-assisted_2020,
	title = {Hardware-{Assisted} {Intellectual} {Property} {Protection} of {Deep} {Learning} {Models}},
	url = {https://eprint.iacr.org/2020/1016.pdf},
	abstract = {The protection of intellectual property (IP) rights of welltrained deep learning (DL) models has become a matter of major concern, especially with the growing trend of deployment of Machine Learning as a Service (MLaaS). In this work, we demonstrate the utilization of a hardware root-of-trust to safeguard the IPs of such DL models which potential attackers have access to. We propose an obfuscation framework called Hardware Protected Neural Network (HPNN) in which a deep neural network is trained as a function of a secret key and then, the obfuscated DL model is hosted on a public model sharing platform. This framework ensures that only an authorized end-user who possesses a trustworthy hardware device (with the secret key embedded on-chip) is able to run intended DL applications using the published model. Extensive experimental evaluations show that any unauthorized usage of such obfuscated DL models result in signiﬁcant accuracy drops ranging from 73.22 to 80.17\% across different neural network architectures and benchmark datasets. In addition, we also demonstrate the robustness of proposed HPNN framework against a model ﬁne-tuning type of attack.},
	language = {en},
	booktitle = {57th {ACM}/{IEEE} {Annual} {Design} {Automation} {Conference} 2020},
	publisher = {IEEE},
	author = {Chakraborty, Abhishek and Mondal, Ankit and Srivastava, Ankur},
	month = jun,
	year = {2020},
	keywords = {DNN, PROACTIVE IPP, done rudi},
	pages = {1--6},
}

@inproceedings{batina_csi_nodate,
	address = {Santa Clara, USA},
	title = {{CSI} {NN}: {Reverse} {Engineering} of {Neural} {Network} {Architectures} {Through} {Electromagnetic} {Side} {Channel}},
	isbn = {978-1-939133-06-9},
	abstract = {Machine learning has become mainstream across industries. Numerous examples prove the validity of it for security applications. In this work, we investigate how to reverse engineer a neural network by using side-channel information such as timing and electromagnetic (EM) emanations. To this end, we consider multilayer perceptron and convolutional neural networks as the machine learning architectures of choice and assume a non-invasive and passive attacker capable of measuring those kinds of leakages.},
	language = {en},
	booktitle = {28th {USENIX} {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Batina, Lejla and Bhasin, Shivam and Jap, Dirmanto and Picek, Stjepan},
	pages = {515--532},
}

@inproceedings{adi_turning_2018,
	address = {Baltimore, USA},
	series = {{SEC}'18},
	title = {Turning {Your} {Weakness} {Into} a {Strength}: {Watermarking} {Deep} {Neural} {Networks} by {Backdooring}},
	isbn = {978-1-939133-04-5},
	shorttitle = {Turning {Your} {Weakness} {Into} a {Strength}},
	abstract = {Deep Neural Networks have recently gained lots of success after enabling several breakthroughs in notoriously challenging problems. Training these networks is computationally expensive and requires vast amounts of training data. Selling such pre-trained models can, therefore, be a lucrative business model. Unfortunately, once the models are sold they can be easily copied and redistributed. To avoid this, a tracking mechanism to identify models as the intellectual property of a particular vendor is necessary. In this work, we present an approach for watermarking Deep Neural Networks in a black-box way. Our scheme works for general classification tasks and can easily be combined with current learning algorithms. We show experimentally that such a watermark has no noticeable impact on the primary task that the model is designed for and evaluate the robustness of our proposal against a multitude of practical attacks. Moreover, we provide a theoretical analysis, relating our approach to previous work on backdooring.},
	booktitle = {27th {USENIX} {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Adi, Yossi and Baum, Carsten and Cisse, Moustapha and Pinkas, Benny and Keshet, Joseph},
	month = aug,
	year = {2018},
	keywords = {DNN, WATERMARKING, black-box access, done, incl implementation},
	pages = {1615--1631},
}

@article{gu_badnets_2019,
	title = {{BadNets}: {Evaluating} {Backdooring} {Attacks} on {Deep} {Neural} {Networks}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{BadNets}},
	url = {https://doi.org/10.1109/ACCESS.2019.2909068},
	doi = {10.1109/ACCESS.2019.2909068},
	urldate = {2020-12-03},
	journal = {IEEE Access},
	author = {Gu, Tianyu and Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
	year = {2019},
	pages = {47230--47244},
}

@article{quan_watermarking_2020,
	title = {Watermarking {Deep} {Neural} {Networks} in {Image} {Processing}},
	issn = {2162-2388},
	url = {https://doi.org/10.1109/TNNLS.2020.2991378},
	doi = {10.1109/TNNLS.2020.2991378},
	abstract = {Publishing/sharing pretrained deep neural network (DNN) models is a common practice in the community of computer vision. The increasing popularity of pretrained models has made it a serious concern: how to protect the intellectual properties of model owners and avert illegal usages by malicious attackers. This article aims at developing a framework for watermarking DNNs, with a particular focus on low-level image processing tasks that map images to images. Using image denoising and superresolution as case studies, we develop a black-box watermarking method for pretrained models, which exploits the overparameterization of the DNNs in image processing. In addition, an auxiliary module for visualizing the watermark information is proposed for further verification. Extensive experiments show that the proposed watermarking framework has no noticeable impact on model performance and enjoys the robustness against the often-seen attacks.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Quan, Yuhui and Teng, Huan and Chen, Yixin and Ji, Hui},
	month = may,
	year = {2020},
	keywords = {WATERMARKING, done, image processing, other ML},
	pages = {1852--1865},
}

@article{wu_watermarking_2020,
	title = {Watermarking {Neural} {Networks} with {Watermarked} {Images}},
	issn = {1051-8215, 1558-2205},
	url = {https://doi.org/10.1109/TCSVT.2020.3030671},
	doi = {10.1109/TCSVT.2020.3030671},
	abstract = {Watermarking neural networks is a quite important means to protect the intellectual property (IP) of neural networks. In this paper, we introduce a novel digital watermarking framework suitable for deep neural networks that output images as the results, in which any image outputted from a watermarked neural network must contain a certain watermark. Here, the host neural network to be protected and a watermark-extraction network are trained together, so that, by optimizing a combined loss function, the trained neural network can accomplish the original task while embedding a watermark into the outputted images. This work is totally different from previous schemes carrying a watermark by network weights or classiﬁcation labels of the trigger set. By detecting watermarks in the outputted images, this technique can be adopted to identify the ownership of the host network and ﬁnd whether an image is generated from a certain neural network or not. We demonstrate that this technique is effective and robust on a variety of image processing tasks, including image colorization, super-resolution, image editing, semantic segmentation and so on.},
	language = {en},
	urldate = {2020-11-24},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Wu, Hanzhou and Liu, Gen and Yao, Yuwei and Zhang, Xinpeng},
	month = oct,
	year = {2020},
	keywords = {NEU, WATERMARKING, done},
	pages = {2591--2601},
}

@article{zhang_deeptrigger_2020,
	title = {{DeepTrigger}: {A} {Watermarking} {Scheme} of {Deep} {Learning} {Models} based on {Chaotic} {Automatic} {Data} {Annotation}},
	url = {https://doi.org/10.1109/ACCESS.2020.3039323},
	doi = {10.1109/ACCESS.2020.3039323},
	abstract = {With the rapid development of artificial intelligence, the intellectual property protection of deep learning models appeals widespread concerns of scientists and engineers. The black-box watermarking protection scheme has been favored by many scholars due to its many advantages. The trigger set containing data content and data annotation is the key of black-box watermarking technology. However, most of the trigger sets in literates were constructed by comprehensible features, such as Gaussian noise and badges on original data content. Then, the attacks based on machine learning can obtain the watermarking features and generate fake trigger set. Therefore, fraudulent ownership claim attacks may occur. In this paper, we turn our attention to data annotation and propose a black-box watermarking scheme based on chaotic automatic data annotation. Chaos has superior features, such as the sensitivity of initial value, aperiodic behavior and unpredictability of the chaotic sequence. We applies these chaotic features on data annotation so as to against the fraudulent ownership claim attacks. Firstly, this scheme applies chaotic automatic data annotation, which is time-saving and non-manual labeling. Secondly, chaotic sequences are unpredictable for long-terms, which can break the principle of empirical or statistical machine learning based attacks when chaotic labeling the trigger samples. Thirdly, the initial value and parameters in chaos offer a large range of key space, which can facilitate the commercialization of the intelligent models. The key formulation also guarantees the separation of the secret key and the trigger set. In addition, experiments and simulations show that the scheme is effective, secure and robust. It can resist fine-tuning attacks, compression attacks, fraudulent ownership claim attacks and overwriting attacks.},
	language = {en},
	journal = {IEEE Access},
	author = {Zhang, Ying-Qian and Jia, Yi-Ran and Wang, Xing-Yuan and Niu, Qiong and Chen, Nian-Dong},
	month = nov,
	year = {2020},
	keywords = {WATERMARKING, neu neu},
	pages = {213296 -- 213305},
}

@article{zhong_automated_2020,
	title = {An {Automated} and {Robust} {Image} {Watermarking} {Scheme} {Based} on {Deep} {Neural} {Networks}},
	issn = {1520-9210, 1941-0077},
	url = {https://doi.org/10.1109/TMM.2020.3006415},
	doi = {10.1109/TMM.2020.3006415},
	urldate = {2020-11-20},
	journal = {IEEE Transactions on Multimedia},
	author = {Zhong, Xin and Huang, Pei-Chi and Mastorakis, Spyridon and Shih, Frank Y.},
	year = {2020},
	pages = {1951--1961},
}

@book{mohri_foundations_2018,
	edition = {2},
	series = {Adaptive computation and machine learning},
	title = {Foundations of machine learning},
	isbn = {978-0-262-03940-6},
	language = {en},
	publisher = {MIT Press},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2018},
	keywords = {Computer algorithms, Machine learning},
}

@article{zhu_secure_2020,
	title = {Secure neural network watermarking protocol against forging attack},
	volume = {2020},
	issn = {1687-5281},
	url = {https://doi.org/10.1186/s13640-020-00527-1},
	doi = {10.1186/s13640-020-00527-1},
	abstract = {In order to protect the intellectual property of neural network, an owner may select a set of trigger samples and their corresponding labels to train a network, and prove the ownership by the trigger set without revealing the inner mechanism and parameters of the network. However, if an attacker is allowed to access the neural network, he can forge a matching relationship between fake trigger samples and fake labels to confuse the ownership. In this paper, we propose a novel neural network watermarking protocol against the forging attack. By introducing one-way hash function, the trigger samples used to prove ownership must form a one-way chain, and their labels are also assigned. By this way, an attacker without the right of network training is impossible to construct a chain of trigger samples or the matching relationship between the trigger samples and the assigned labels. Our experiments show that the proposed protocol can resist the watermark forgery without sacrificing the network performance.},
	language = {en},
	number = {1},
	urldate = {2020-09-24},
	journal = {EURASIP Journal on Image and Video Processing},
	author = {Zhu, Renjie and Zhang, Xinpeng and Shi, Mengte and Tang, Zhenjun},
	month = sep,
	year = {2020},
	keywords = {DNN, WATERMARKING, done},
}

@article{zhao_afa_2019,
	title = {{AFA}: {Adversarial} fingerprinting authentication for deep neural networks},
	volume = {150},
	issn = {01403664},
	shorttitle = {{AFA}},
	url = {https://doi.org/10.1016/j.comcom.2019.12.016},
	doi = {10.1016/j.comcom.2019.12.016},
	abstract = {With the vigorous development of deep learning, sharing trained deep neural network (DNN) models has become a common trend in various fields. An urgent problem is to protect the intellectual property (IP) rights of the model owners and detect IP infringement. DNN watermarking technology, which embeds signature information into the protected model and tries to extract it from the plagiarism model, has been the main approach of IP verification. However, the existing DNN watermarking methods have to be robust to various removal attacks since their watermarks are single in form or limited in quantity. Meanwhile, the process of adding watermarks to the DNN models will affect their original prediction abilities. Moreover, if the model has been distributed before embedding the watermarks, its IP cannot be correctly recognized and protected.},
	language = {en},
	urldate = {2020-09-15},
	journal = {Computer Communications},
	author = {Zhao, Jingjing and Hu, Qingyue and Liu, Gaoyang and Ma, Xiaoqiang and Chen, Fei and Hassan, Mohammad Mehedi},
	month = dec,
	year = {2019},
	keywords = {DNN, FINGERPRINTING, done},
	pages = {488--497},
}

@article{yingjiu_li_fingerprinting_2005,
	title = {Fingerprinting relational databases: schemes and specialties},
	volume = {2},
	issn = {1545-5971},
	shorttitle = {Fingerprinting relational databases},
	url = {https://doi.org/10.1109/TDSC.2005.12},
	doi = {10.1109/TDSC.2005.12},
	abstract = {In this paper, we present a technique for fingerprinting relational data by extending Agrawal et al.’s watermarking scheme. The primary new capability provided by our scheme is that, under reasonable assumptions, it can embed and detect arbitrary bit-string marks in relations. This capability, which is not provided by prior techniques, permits our scheme to be used as a fingerprinting scheme. We then present quantitative models of the robustness properties of our scheme. These models demonstrate that fingerprints embedded by our scheme are detectable and robust against a wide variety of attacks including collusion attacks.},
	language = {en},
	number = {1},
	urldate = {2021-08-26},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {{Yingjiu Li} and Swarup, V. and Jajodia, S.},
	month = jan,
	year = {2005},
	pages = {34--45},
}

@article{ying_overview_2019,
	title = {An {Overview} of {Overfitting} and its {Solutions}},
	volume = {1168},
	issn = {1742-6588, 1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1168/2/022022},
	doi = {10.1088/1742-6596/1168/2/022022},
	abstract = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.},
	language = {en},
	urldate = {2021-08-31},
	journal = {Journal of Physics: Conference Series},
	author = {Ying, Xue},
	month = feb,
	year = {2019},
	pages = {022022},
}

@article{yang_federated_2019,
	title = {Federated {Machine} {Learning}: {Concept} and {Applications}},
	volume = {10},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Federated {Machine} {Learning}},
	url = {https://doi.org/10.1145/3298981},
	doi = {10.1145/3298981},
	abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
	language = {en},
	number = {2},
	urldate = {2021-08-26},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
	month = feb,
	year = {2019},
	pages = {1--19},
}

@article{xu_identity_2020,
	title = {“{Identity} {Bracelets}” for {Deep} {Neural} {Networks}},
	volume = {8},
	issn = {2169-3536},
	url = {https://doi.org/10.1109/ACCESS.2020.2998784},
	doi = {10.1109/ACCESS.2020.2998784},
	abstract = {The power of deep learning and the enormous effort and money required to build a deep learning model makes stealing them a hugely worthwhile and highly lucrative endeavor. Worse still, model theft requires little more than a high-school understanding of computer functions, which ensures a healthy and vibrant black market full of choice for any would-be pirate. As such, estimating how many neural network models are likely to be illegally reproduced and distributed in future is almost impossible. Therefore, we propose an embedded ‘identity bracelet’ for deep neural networks that acts as proof of a model’s owner. Our solution is an extension to the existing trigger-set watermarking techniques that embeds a post-cryptographic-style serial number into the base deep neural network (DNN). Called a DNN-SN, this identiﬁer works like an identity bracelet that proves a network’s rightful owner. Further, a novel training method based on non-related multitask learning ensures that embedding the DNN-SN does not compromise model performance. Experimental evaluations of the framework conﬁrm that a DNN-SN can be embedded into a model when training from scratch or in the student network component of Net2Net.},
	language = {en},
	urldate = {2020-09-24},
	journal = {IEEE Access},
	author = {Xu, Xiangrui and Li, Yaqin and Yuan, Cao},
	month = jun,
	year = {2020},
	keywords = {DNN, WATERMARKING, black-box access},
	pages = {102065--102074},
}

@article{sharma_robust_2020,
	title = {A robust hybrid digital watermarking technique against a powerful {CNN}-based adversarial attack},
	volume = {79},
	issn = {1380-7501, 1573-7721},
	url = {https://doi.org/10.1007/s11042-020-09555-5},
	doi = {10.1007/s11042-020-09555-5},
	language = {en},
	number = {43-44},
	urldate = {2020-11-20},
	journal = {Multimedia Tools and Applications},
	author = {Sharma, Sai Shyam and Chandrasekaran, V.},
	month = nov,
	year = {2020},
	pages = {32769--32790},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {0920-5691, 1573-1405},
	url = {https://doi.org/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classiﬁcation and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than ﬁfty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the ﬁeld of large-scale image classiﬁcation and object detection, and compare the state-ofthe-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
	language = {en},
	number = {3},
	urldate = {2021-08-29},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	pages = {211--252},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	url = {https://doi.org/10.1038/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	journal = {Nature},
	author = {Rumelhart, David E and Hintont, Geoffrey E and Williams, Ronald J},
	year = {1986},
	pages = {533--536},
}

@article{pan_survey_2010,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	url = {https://doi.org/10.1109/TKDE.2009.191},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	language = {en},
	number = {10},
	urldate = {2021-08-31},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	month = oct,
	year = {2010},
	pages = {1345--1359},
}

@misc{wang_evilmodel_2021,
	title = {{EvilModel}: {Hiding} {Malware} {Inside} of {Neural} {Network} {Models}},
	shorttitle = {{EvilModel}},
	url = {http://arxiv.org/abs/2107.08590},
	abstract = {Delivering malware covertly and detection-evadingly is critical to advanced malware campaigns. In this paper, we present a method that delivers malware covertly and detectionevadingly through neural network models. Neural network models are poorly explainable and have a good generalization ability. By embedding malware into the neurons, malware can be delivered covertly with minor or even no impact on the performance of neural networks. Meanwhile, since the structure of the neural network models remains unchanged, they can pass the security scan of antivirus engines. Experiments show that 36.9MB of malware can be embedded into a 178MB-AlexNet model within 1\% accuracy loss, and no suspicious are raised by antivirus engines in VirusTotal, which veriﬁes the feasibility of this method. With the widespread application of artiﬁcial intelligence, utilizing neural networks becomes a forwarding trend of malware. We hope this work could provide a referenceable scenario for the defense on neural network-assisted attacks.},
	language = {en},
	urldate = {2021-07-28},
	author = {Wang, Zhi and Liu, Chaoge and Cui, Xiang},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.08590},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{song_robust_2020,
	title = {Robust {Membership} {Encoding}: {Inference} {Attacks} and {Copyright} {Protection} for {Deep} {Learning}},
	shorttitle = {Robust {Membership} {Encoding}},
	url = {http://arxiv.org/abs/1909.12982},
	abstract = {Machine learning as a service (MLaaS), and algorithm marketplaces are on a rise. Data holders can easily train complex models on their data using third party provided learning codes. Training accurate ML models requires massive labeled data and advanced learning algorithms. The resulting models are considered as intellectual property of the model owners and their copyright should be protected. Also, MLaaS needs to be trusted not to embed secret information about the training data into the model, such that it could be later retrieved when the model is deployed.},
	language = {en},
	urldate = {2021-07-28},
	author = {Song, Congzheng and Shokri, Reza},
	month = mar,
	year = {2020},
	note = {arXiv: 1909.12982},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{orekondy_knockoff_2018,
	title = {Knockoff {Nets}: {Stealing} {Functionality} of {Black}-{Box} {Models}},
	shorttitle = {Knockoff {Nets}},
	url = {http://arxiv.org/abs/1812.02766},
	abstract = {Machine Learning (ML) models are increasingly deployed in the wild to perform a wide range of tasks. In this work, we ask to what extent can an adversary steal functionality of such “victim” models based solely on blackbox interactions: image in, predictions out. In contrast to prior work, we present an adversary lacking knowledge of train/test data used by the model, its internals, and semantics over model outputs. We formulate model functionality stealing as a two-step approach: (i) querying a set of input images to the blackbox model to obtain predictions; and (ii) training a “knockoff” with queried image-prediction pairs. We make multiple remarkable observations: (a) querying random images from a different distribution than that of the blackbox training data results in a well-performing knockoff; (b) this is possible even when the knockoff is represented using a different architecture; and (c) our reinforcement learning approach additionally improves query sample efﬁciency in certain settings and provides performance gains. We validate model functionality stealing on a range of datasets and tasks, as well as on a popular image analysis API where we create a reasonable knockoff for as little as \$30.},
	language = {en},
	urldate = {2020-10-06},
	author = {Orekondy, Tribhuvanesh and Schiele, Bernt and Fritz, Mario},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.02766},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{pal_framework_2019,
	title = {A framework for the extraction of {Deep} {Neural} {Networks} by leveraging public data},
	url = {http://arxiv.org/abs/1905.09165},
	abstract = {Machine learning models trained on conﬁdential datasets are increasingly being deployed for proﬁt. Machine Learning as a Service (MLaaS) has made such models easily accessible to end-users. Prior work has developed model extraction attacks, in which an adversary extracts an approximation of MLaaS models by making black-box queries to it. However, none of these works is able to satisfy all the three essential criteria for practical model extraction: (i) the ability to work on deep learning models, (ii) the non-requirement of domain knowledge and (iii) the ability to work with a limited query budget. We design a model extraction framework that makes use of active learning and large public datasets to satisfy them. We demonstrate that it is possible to use this framework to steal deep classiﬁers trained on a variety of datasets from image and text domains. By querying a model via black-box access for its top prediction, our framework improves performance on an average over a uniform noise baseline by 4.70× for image tasks and 2.11× for text tasks respectively, while using only 30\% (30,000 samples) of the public dataset at its disposal.},
	language = {en},
	urldate = {2020-10-13},
	author = {Pal, Soham and Gupta, Yash and Shukla, Aditya and Kanade, Aditya and Shevade, Shirish and Ganapathy, Vinod},
	month = may,
	year = {2019},
	note = {arXiv: 1905.09165},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{molchanov_pruning_2017,
	title = {Pruning {Convolutional} {Neural} {Networks} for {Resource} {Efficient} {Inference}},
	url = {http://arxiv.org/abs/1611.06440},
	abstract = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical (5x practical) reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.},
	urldate = {2020-09-25},
	author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
	month = jun,
	year = {2017},
	note = {arXiv: 1611.06440},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{mitchell_machine_1997,
	title = {Machine {Learning}},
	volume = {45},
	isbn = {0-07-042807-7},
	language = {en},
	publisher = {McGraw-Hill Education Ltd},
	author = {Mitchell, Tom M},
	year = {1997},
}

@article{merrer_adversarial_2019,
	title = {Adversarial {Frontier} {Stitching} for {Remote} {Neural} {Network} {Watermarking}},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	url = {https://doi.org/10.1007/s00521-019-04434-z},
	doi = {10.1007/s00521-019-04434-z},
	abstract = {The state of the art performance of deep learning models comes at a high cost for companies and institutions, due to the tedious data collection and the heavy processing requirements. Recently, [35, 22] proposed to watermark convolutional neural networks for image classification, by embedding information into their weights. While this is a clear progress towards model protection, this technique solely allows for extracting the watermark from a network that one accesses locally and entirely. Instead, we aim at allowing the extraction of the watermark from a neural network (or any other machine learning model) that is operated remotely, and available through a service API. To this end, we propose to mark the model's action itself, tweaking slightly its decision frontiers so that a set of specific queries convey the desired information. In the present paper, we formally introduce the problem and propose a novel zero-bit watermarking algorithm that makes use of adversarial model examples. While limiting the loss of performance of the protected model, this algorithm allows subsequent extraction of the watermark using only few queries. We experimented the approach on three neural networks designed for image classification, in the context of MNIST digit recognition task.},
	number = {13},
	journal = {Neural Computing and Applications},
	author = {Merrer, Erwan Le and Perez, Patrick and Trédan, Gilles},
	month = aug,
	year = {2019},
	keywords = {DNN, WATERMARKING, black-box access, incl implementation},
	pages = {9233--9244},
}

@article{mahood_searching_2014,
	title = {Searching for grey literature for systematic reviews: challenges and benefits},
	volume = {5},
	issn = {17592879},
	shorttitle = {Searching for grey literature for systematic reviews},
	url = {doi.org/10.1002/jrsm.1106},
	doi = {10.1002/jrsm.1106},
	language = {en},
	number = {3},
	urldate = {2020-11-20},
	journal = {Research Synthesis Methods},
	author = {Mahood, Quenby and Van Eerd, Dwayne and Irvin, Emma},
	month = sep,
	year = {2014},
	pages = {221--234},
}

@article{lin_chaotic_2020,
	title = {Chaotic {Weights}: {A} {Novel} {Approach} to {Protect} {Intellectual} {Property} of {Deep} {Neural} {Networks}},
	issn = {1937-4151},
	shorttitle = {Chaotic {Weights}},
	url = {https://doi.org/10.1109/TCAD.2020.3018403},
	doi = {10.1109/TCAD.2020.3018403},
	abstract = {Despite the high accuracy achieved by the Deep Neural Network (DNN) technique, there is still a lack of satisfying methodologies to protect the intellectual property (IP) of DNNs, which involves extensive valuable training data, abundant hardware training resources, and fine-tuning skills of experienced experts. Existing solutions based on watermarking cannot prevent malicious/unauthorized users from using well-trained DNNs. This paper proposes Chaotic Weights (ChaoW), a novel framework based on the Chaotic Map theory, to protect the IP of DNN providers with very low overhead. Specifically, in order to alleviate the storage overhead and abridge the decryption time, our method makes convolutional or fully-connected kernels chaotic by exchanging the weight positions to obtain a satisfying encryption effect, instead of using the conventional idea of encrypting the weight values. Comprehensive experimental evaluations on image classification, semantic segmentation and name generation demonstrate that ChaoW can effectively protect the IP of DNNs without damaging the inference accuracy, and the impact on the inference speed is negligible.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Lin, Ning and Chen, Xiaoming and Lu, Hang and Li, Xiaowei},
	month = aug,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {PROACTIVE IPP, done rudi},
	pages = {1327--1339},
}

@misc{li_pruning_2017,
	title = {Pruning {Filters} for {Efficient} {ConvNets}},
	url = {http://arxiv.org/abs/1608.08710},
	abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
	urldate = {2020-09-21},
	author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
	month = mar,
	year = {2017},
	note = {arXiv: 1608.08710},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {https://doi.org/10.1109/5.726791},
	doi = {10.1109/5.726791},
	language = {en},
	number = {11},
	urldate = {2021-07-06},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@article{kohno_remote_2005,
	title = {Remote {Physical} {Device} {Fingerprinting}},
	volume = {2},
	issn = {0361-1434},
	url = {https://doi.org/10.1109/TDSC.2005.26},
	doi = {10.1109/TDSC.2005.26},
	language = {en},
	number = {2},
	urldate = {2020-11-21},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Kohno, T. and Broido, A. and Claffy, K.C.},
	month = feb,
	year = {2005},
	pages = {93--108},
}

@article{deeba_digital_2020,
	title = {Digital {Watermarking} {Using} {Deep} {Neural} {Network}},
	volume = {10},
	issn = {20103700},
	url = {https://doi.org/10.18178/ijmlc.2020.10.2.932},
	doi = {10.18178/ijmlc.2020.10.2.932},
	abstract = {Recently in the vast advancement of Artificial Intelligence, Machine learning and Deep Neural Network (DNN) driven us to the robust applications. Such as Image processing, speech recognition, and natural language processing, DNN Algorithms has succeeded in many drawbacks; especially the trained DNN models have made easy to the researchers to produce state-of-art results. However, sharing these trained models are always a challenging task, i.e. security, and protection. We performed extensive experiments to present some analysis of watermark in DNN. We proposed a DNN model for Digital watermarking which investigate the intellectual property of Deep Neural Network, Embedding watermarks, and owner verification. This model can generate the watermarks to deal with possible attacks (fine-tuning and train to embed). This approach is tested on the standard dataset. Hence this model is robust to above counter-watermark attacks. Our model accurately and instantly verifies the ownership of all the remotely expanded deep learning models without aﬀecting the model accuracy for standard information data.},
	language = {en},
	number = {2},
	urldate = {2020-09-28},
	journal = {International Journal of Machine Learning and Computing},
	author = {Deeba, Farah and Kun, She and Dharejo, Fayaz Ali and Langah, Hameer and Memon, Hira and Memon, Hira},
	month = feb,
	year = {2020},
	pages = {277--282},
}

@incollection{li_watermarking_2020,
	title = {Watermarking {Neural} {Network} with {Compensation} {Mechanism}},
	volume = {12275},
	isbn = {978-3-030-55392-0 978-3-030-55393-7},
	url = {http://link.springer.com/10.1007/978-3-030-55393-7_33},
	abstract = {In recent years, the rapid development of neural networks has also brought his intellectual property (IP) protection. Embedding a watermark in a neural network is an eﬀective scheme to protect its IP. In this paper, we propose a new watermark embedding scheme with compensation mechanism that is diﬀerent from the previous regularization embedding. First, we select the weights of the watermark to be embedded pseudo-randomly. Then, we perform an orthogonal transformation on the selected weights, and embed the watermark by the binarization method in the obtained coeﬃcients, and use the inverse orthogonal transformation on the watermarked coeﬃcients to obtain the watermarked weights. Finally, we propose a model ﬁne-tuning scheme with compensation mechanism, which can eliminate the slight accuracy degradation caused by binarization without destroying the watermark in the model. In our scheme, due to the concealment of watermark embedding location, it can overcome the defects of previous schemes which cannot resist watermark overwriting attack. Moreover, compared with the regularization embedding method, our scheme uses the ﬁne-tuning with compensation mechanism, which requires less embedding cost and is more stable. In addition, it has achieved favorable performance in resisting weight pruning attack, weight ﬁne-tuning and ﬁdelity evaluation.},
	language = {en},
	urldate = {2020-09-24},
	booktitle = {Knowledge {Science}, {Engineering} and {Management}},
	publisher = {Springer International Publishing},
	author = {Feng, Le and Zhang, Xinpeng},
	editor = {Li, Gang and Shen, Heng Tao and Yuan, Ye and Wang, Xiaoyang and Liu, Huawen and Zhao, Xiang},
	month = aug,
	year = {2020},
	doi = {10.1007/978-3-030-55393-7_33},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {DNN, WATERMARKING, done, white-box access},
	pages = {363--375},
}

@incollection{liu_fine-pruning_2018,
	title = {Fine-{Pruning}: {Defending} {Against} {Backdooring} {Attacks} on {Deep} {Neural} {Networks}},
	isbn = {978-3-030-00469-9 978-3-030-00470-5},
	shorttitle = {Fine-{Pruning}},
	url = {http://link.springer.com/10.1007/978-3-030-00470-5_13},
	abstract = {Deep neural networks (DNNs) provide excellent performance across a wide range of classiﬁcation tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassiﬁcations or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the ﬁrst eﬀective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and ﬁne-tuning. We show that neither, by itself, is suﬃcient to defend against sophisticated attackers. We then evaluate ﬁne-pruning, a combination of pruning and ﬁne-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0\% with only a 0.4\% drop in accuracy for clean (non-triggering) inputs. Our work provides the ﬁrst step toward defenses against backdoor attacks in deep neural networks.},
	language = {en},
	urldate = {2020-10-14},
	booktitle = {Research in {Attacks}, {Intrusions}, and {Defenses}},
	publisher = {Springer International Publishing},
	author = {Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
	year = {2018},
	doi = {10.1007/978-3-030-00470-5_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {273--294},
}

@incollection{lauw_protecting_2020,
	title = {Protecting {IP} of {Deep} {Neural} {Networks} with {Watermarking}: {A} {New} {Label} {Helps}},
	isbn = {978-3-030-47435-5 978-3-030-47436-2},
	shorttitle = {Protecting {IP} of {Deep} {Neural} {Networks} with {Watermarking}},
	url = {http://link.springer.com/10.1007/978-3-030-47436-2_35},
	language = {en},
	urldate = {2020-09-14},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Zhong, Qi and Zhang, Leo Yu and Zhang, Jun and Gao, Longxiang and Xiang, Yong},
	editor = {Lauw, Hady W. and Wong, Raymond Chi-Wing and Ntoulas, Alexandros and Lim, Ee-Peng and Ng, See-Kiong and Pan, Sinno Jialin},
	month = may,
	year = {2020},
	doi = {10.1007/978-3-030-47436-2_35},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {DNN, WATERMARKING, black-box access, done},
	pages = {462--474},
}

@incollection{ferrari_hidden_2018,
	title = {{HiDDeN}: {Hiding} {Data} {With} {Deep} {Networks}},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	shorttitle = {{HiDDeN}},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_40},
	abstract = {Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneﬁcial. We ﬁnd that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixelwise dropout, cropping, and JPEG compression. Even though JPEG is non-diﬀerentiable, we show that a robust model can be trained using diﬀerentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images.},
	language = {en},
	urldate = {2020-12-07},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Zhu, Jiren and Kaplan, Russell and Johnson, Justin and Fei-Fei, Li},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01267-0_40},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {rudis input},
	pages = {682--697},
}

@inproceedings{arai_preventing_2020,
	series = {{AISC}},
	title = {Preventing {Neural} {Network} {Weight} {Stealing} via {Network} {Obfuscation}},
	volume = {1230},
	isbn = {978-3-030-52242-1 978-3-030-52243-8},
	url = {https://doi.org/10.1007/978-3-030-52243-8_1},
	doi = {10.1007/978-3-030-52243-8_1},
	abstract = {Deep Neural Networks are robust to minor perturbations of the learned network parameters and their minor modiﬁcations do not change the overall network response signiﬁcantly. This allows space for model stealing, where a malevolent attacker can steal an already trained network, modify the weights and claim the new network his own intellectual property. In certain cases this can prevent the free distribution and application of networks in the embedded domain. In this paper, we propose a method for creating an equivalent version of an already trained fully connected deep neural network that can prevent network stealing, namely, it produces the same responses and classiﬁcation accuracy, but it is extremely sensitive to weight changes.},
	language = {en},
	urldate = {2020-12-15},
	booktitle = {{SAI} 2020: {Intelligent} {Computing}},
	publisher = {Springer International Publishing},
	author = {Szentannai, Kálmán and Al-Afandi, Jalal and Horváth, András},
	editor = {Arai, Kohei and Kapoor, Supriya and Bhatia, Rahul},
	month = jul,
	year = {2020},
	note = {preprint on arXiv: 1907.01650},
	keywords = {PROACTIVE IPP},
	pages = {1--11},
}

@misc{szentannai_mimosanet_2019,
	title = {{MimosaNet}: {An} {Unrobust} {Neural} {Network} {Preventing} {Model} {Stealing}},
	author = {Szentannai, Kálmán and Al-Afandi, Jalal and Horváth, András},
	month = jun,
	year = {2019},
	note = {arXiv: 1907.01650},
	keywords = {PROACTIVE IPP},
}

@misc{liu_delving_2017,
	title = {Delving into {Transferable} {Adversarial} {Examples} and {Black}-{Box} {Attacks}},
	url = {http://arxiv.org/abs/1611.02770},
	abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the ﬁrst to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the ﬁrst to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to ﬁnd, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the ﬁrst time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classiﬁcation system.},
	language = {en},
	author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
	year = {2017},
	note = {arXiv: 1611.02770},
}

@article{netzer_reading_2011,
	title = {Reading {Digits} in {Natural} {Images} with {Unsupervised} {Feature} {Learning}},
	abstract = {Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difﬁcult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difﬁculty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and ﬁnd that they are convincingly superior on our benchmarks.},
	language = {en},
	journal = {NIPS Workshop on Deep Learning and Unsupervised Feature Learning},
	author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
	year = {2011},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	url = {https://doi.org/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = jun,
	year = {2017},
	pages = {84--90},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high conﬁdence. Early attempts at explaining this phenomenon focused on nonlinearity and overﬁtting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the ﬁrst explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	language = {en},
	urldate = {2021-08-02},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = {2015},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{liu_trojaning_2017,
	title = {Trojaning {Attack} on {Neural} {Networks}},
	abstract = {With the fast spread of machine learning techniques, sharing and adopting public machine learning models become very popular. This gives attackers many new opportunities. In this paper, we propose a trojaning attack on neuron networks. As the models are not intuitive for human to understand, the attack features stealthiness. Deploying trojaned models can cause various severe consequences including endangering human lives (in applications like auto driving). We first inverse the neuron network to generate a general trojan trigger, and then retrain the model with external datasets to inject malicious behaviors to the model. The malicious behaviors are only activated by inputs stamped with the trojan trigger. In our attack, we do not need to tamper with the original training process, which usually takes weeks to months. Instead, it takes minutes to hours to apply our attack. Also, we do not require the datasets that are used to train the model. In practice, the datasets are usually not shared due to privacy or copyright concerns. We use five different applications to demonstrate the power of our attack, and perform a deep analysis on the possible factors that affect the attack. The results show that our attack is highly effective and efficient. The trojaned behaviors can be successfully triggered (with nearly 100\% possibility) without affecting its test accuracy for normal input data. Also, it only takes a small amount of time to attack a complex neuron network model. In the end, we also discuss possible defense against such attacks.},
	language = {en},
	author = {Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
	year = {2017},
}

@misc{krizhevsky_learning_2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	language = {en},
	publisher = {Citeseer},
	author = {Krizhevsky, Alex},
	year = {2009},
}

@misc{kamran_comprehensive_2018,
	title = {A {Comprehensive} {Survey} of {Watermarking} {Relational} {Databases} {Research}},
	url = {https://arxiv.org/abs/1801.08271},
	abstract = {Watermarking and ﬁngerprinting of relational databases are quite proﬁcient for ownership protection, tamper prooﬁng, and proving data integrity. In past few years several such techniques have been proposed. A survey of almost all the work done, till date, in these ﬁelds has been presented in this paper. The techniques have been classiﬁed on the basis of how and where they embed the watermark. The analysis and comparison of these techniques on different merits has also been provided. In the end, this paper points out the direction of future research in these ﬁelds.},
	language = {en},
	author = {Kamran, Muhammad and Farooq, Muddassar},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.08271},
}

@misc{abdelnabi_adversarial_2020,
	title = {Adversarial {Watermarking} {Transformer}: {Towards} {Tracing} {Text} {Provenance} with {Data} {Hiding}},
	shorttitle = {Adversarial {Watermarking} {Transformer}},
	url = {http://arxiv.org/abs/2009.03015},
	abstract = {Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.},
	language = {en},
	urldate = {2020-09-28},
	author = {Abdelnabi, Sahar and Fritz, Mario},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.03015},
	keywords = {WATERMARKING, done, other ML, text},
}

@misc{han_learning_2015,
	title = {Learning both {Weights} and {Connections} for {Efficient} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.02626},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
	urldate = {2020-09-21},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
	month = oct,
	year = {2015},
	note = {arXiv: 1506.02626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{cohen_emnist_2017,
	title = {{EMNIST}: an extension of {MNIST} to handwritten letters},
	shorttitle = {{EMNIST}},
	url = {http://arxiv.org/abs/1702.05373},
	abstract = {The MNIST dataset has become a standard benchmark for learning, classiﬁcation and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classiﬁcation tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classiﬁers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classiﬁcation results on converted NIST digits and the MNIST digits.},
	language = {en},
	urldate = {2021-08-26},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
	month = mar,
	year = {2017},
	note = {arXiv: 1702.05373},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	journal = {Journal of Machine Learning Research 15},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@misc{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2021-08-31},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2020-09-25},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = feb,
	year = {2016},
	note = {arXiv: 1510.00149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@misc{boenisch_survey_2020,
	title = {A {Survey} on {Model} {Watermarking} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2009.12153},
	abstract = {Machine learning (ML) models are applied in an increasing variety of domains. The availability of large amounts of data and computational resources encourages the development of ever more complex and valuable models. These models are considered intellectual property of the legitimate parties who have trained them, which makes their protection against stealing, illegitimate redistribution, and unauthorized application an urgent need. Digital watermarking presents a strong mechanism for marking model ownership and, thereby, offers protection against those threats. The emergence of numerous watermarking schemes and attacks against them is pushed forward by both academia and industry, which motivates a comprehensive survey on this ﬁeld. This document at hand provides the ﬁrst extensive literature review on ML model watermarking schemes and attacks against them. It offers a taxonomy of existing approaches and systemizes general knowledge around them. Furthermore, it assembles the security requirements to watermarking approaches and evaluates schemes published by the scientiﬁc community according to them in order to present systematic shortcomings and vulnerabilities. Thus, it can not only serve as valuable guidance in choosing the appropriate scheme for speciﬁc scenarios, but also act as an entry point into developing new mechanisms that overcome presented shortcomings, and thereby contribute in advancing the ﬁeld.},
	language = {en},
	urldate = {2020-11-19},
	author = {Boenisch, Franziska},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12153},
	keywords = {NEU},
}

@misc{kitchenham_guidelines_2007,
	title = {Guidelines for performing {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	publisher = {Citeseer},
	author = {Kitchenham, Barbara and Charters, Stuart},
	year = {2007},
}

@misc{hasanpour_lets_2018,
	title = {Lets keep it simple, {Using} simple architectures to outperform deeper and more complex architectures},
	url = {http://arxiv.org/abs/1608.06037},
	abstract = {Major winning Convolutional Neural Networks (CNNs), such as AlexNet, VGGNet, ResNet, GoogleNet, include tens to hundreds of millions of parameters, which impose considerable computation and memory overhead. This limits their practical use for training, optimization and memory efficiency. On the contrary, light-weight architectures, being proposed to address this issue, mainly suffer from low accuracy. These inefficiencies mostly stem from following an ad hoc procedure. We propose a simple architecture, called SimpleNet, based on a set of designing principles, with which we empirically show, a well-crafted yet simple and reasonably deep architecture can perform on par with deeper and more complex architectures. SimpleNet provides a good tradeoff between the computation/memory efficiency and the accuracy. Our simple 13-layer architecture outperforms most of the deeper and complex architectures to date such as VGGNet, ResNet, and GoogleNet on several well-known benchmarks while having 2 to 25 times fewer number of parameters and operations. This makes it very handy for embedded system or system with computational and memory limitations. We achieved state-of-the-art result on CIFAR10 outperforming several heavier architectures, near state of the art on MNIST and competitive results on CIFAR100 and SVHN. Models are made available at: https://github.com/Coderx7/SimpleNet},
	language = {en},
	urldate = {2021-07-26},
	author = {Hasanpour, Seyyed Hossein and Rouhani, Mohammad and Fayyaz, Mohsen and Sabokrou, Mohammad},
	month = feb,
	year = {2018},
	note = {arXiv: 1608.06037},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
}

@misc{zhu_prune_2017,
	title = {To prune, or not to prune: exploring the efficacy of pruning for model compression},
	shorttitle = {To prune, or not to prune},
	url = {http://arxiv.org/abs/1710.01878},
	abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
	urldate = {2020-09-25},
	author = {Zhu, Michael and Gupta, Suyog},
	month = nov,
	year = {2017},
	note = {arXiv: 1710.01878},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{behzadan_sequential_2019,
	title = {Sequential {Triggers} for {Watermarking} of {Deep} {Reinforcement} {Learning} {Policies}},
	url = {http://arxiv.org/abs/1906.01126},
	abstract = {This paper proposes a novel scheme for the watermarking of Deep Reinforcement Learning (DRL) policies. This scheme provides a mechanism for the integration of a unique identiﬁer within the policy in the form of its response to a designated sequence of state transitions, while incurring minimal impact on the nominal performance of the policy. The applications of this watermarking scheme include detection of unauthorized replications of proprietary policies, as well as enabling the graceful interruption or termination of DRL activities by authorized entities. We demonstrate the feasibility of our proposal via experimental evaluation of watermarking a DQN policy trained in the Cartpole environment.},
	language = {en},
	urldate = {2020-12-01},
	author = {Behzadan, Vahid and Hsu, William},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01126},
	keywords = {WATERMARKING, neu neu},
}

@misc{xu_deepobfuscation_2018,
	title = {{DeepObfuscation}: {Securing} the {Structure} of {Convolutional} {Neural} {Networks} via {Knowledge} {Distillation}},
	shorttitle = {{DeepObfuscation}},
	url = {http://arxiv.org/abs/1806.10313},
	abstract = {This paper investigates the piracy problem of deep learning models. Designing and training a well-performing model is generally expensive. However, when releasing them, attackers may reverse engineer the models and pirate their design. This paper, therefore, proposes deep learning obfuscation, aiming at obstructing attackers from pirating a deep learning model. In particular, we focus on obfuscating convolutional neural networks (CNN), a widely employed type of deep learning architectures for image recognition. Our approach obfuscates a CNN model eventually by simulating its feature extractor with a shallow and sequential convolutional block. To this end, we employ a recursive simulation method and a joint training method to train the simulation network. The joint training method leverages both the intermediate knowledge generated by a feature extractor and data labels to train a simulation network. In this way, we can obtain an obfuscated model without accuracy loss. We have verified the feasibility of our approach with three prevalent CNNs, i.e., GoogLeNet, ResNet, and DenseNet. Although these networks are very deep with tens or hundreds of layers, we can simulate them in a shallow network including only five or seven convolutional layers. The obfuscated models are even more efficient than the original models. Our obfuscation approach is very effective to protect the critical structure of a deep learning model from being exposed to attackers. Moreover, it can also thwart attackers from pirating the model with transfer learning or incremental learning techniques because the shallow simulation network bears poor learning ability. To our best knowledge, this paper serves as a first attempt to obfuscate deep learning models, which may shed light on more future studies.},
	urldate = {2020-11-24},
	author = {Xu, Hui and Su, Yuxin and Zhao, Zirui and Zhou, Yangfan and Lyu, Michael R. and King, Irwin},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.10313},
	keywords = {NEU, PROACTIVE IPP, backward snowballing},
}

@article{nagai_digital_2018,
	title = {Digital watermarking for deep neural networks},
	volume = {7},
	issn = {2192-662X},
	url = {https://doi.org/10.1007/s13735-018-0147-1},
	doi = {10.1007/s13735-018-0147-1},
	abstract = {Although deep neural networks have made tremendous progress in the area of multimedia representation, training neural models requires a large amount of data and time. It is well known that utilizing trained models as initial weights often achieves lower training error than neural networks that are not pre-trained. A fine-tuning step helps to both reduce the computational cost and improve the performance. Therefore, sharing trained models has been very important for the rapid progress of research and development. In addition, trained models could be important assets for the owner(s) who trained them; hence, we regard trained models as intellectual property. In this paper, we propose a digital watermarking technology for ownership authorization of deep neural networks. First, we formulate a new problem: embedding watermarks into deep neural networks. We also define requirements, embedding situations, and attack types on watermarking in deep neural networks. Second, we propose a general framework for embedding a watermark in model parameters, using a parameter regularizer. Our approach does not impair the performance of networks into which a watermark is placed because the watermark is embedded while training the host network. Finally, we perform comprehensive experiments to reveal the potential of watermarking deep neural networks as the basis of this new research effort. We show that our framework can embed a watermark during the training of a deep neural network from scratch, and during fine-tuning and distilling, without impairing its performance. The embedded watermark does not disappear even after fine-tuning or parameter pruning; the watermark remains complete even after 65\% of parameters are pruned.},
	language = {en},
	number = {1},
	urldate = {2020-09-29},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Nagai, Yuki and Uchida, Yusuke and Sakazawa, Shigeyuki and Satoh, Shin’ichi},
	month = mar,
	year = {2018},
	pages = {3--16},
}

@misc{guo_evolutionary_2019,
	title = {Evolutionary {Trigger} {Set} {Generation} for {DNN} {Black}-{Box} {Watermarking}},
	url = {http://arxiv.org/abs/1906.04411},
	abstract = {The commercialization of deep learning creates a compelling need for intellectual property (IP) protection. Deep neural network (DNN) watermarking has been proposed as a promising tool to help model owners prove ownership and ﬁght piracy. A popular approach of watermarking is to train a DNN to recognize images with certain trigger patterns. In this paper, we propose a novel evolutionary algorithm-based method to generate and optimize trigger patterns. Our method brings siginificant reduction in false positive rates, leading to compelling proof of ownership. At the same time, it maintains the robustness of the watermark against attacks. We compare our method with prior art and demonstrate its effectiveness on popular models and datasets.},
	language = {en},
	urldate = {2020-09-28},
	author = {Guo, Jia and Potkonjak, Miodrag},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04411},
	keywords = {DNN, WATERMARKING, done},
}

@phdthesis{xia_watermarking_2020,
	address = {Espoo, Finland},
	title = {Watermarking {Federated} {Deep} {Neural} {Network} {Models}},
	abstract = {Training DNN models is expensive in terms of computational power, collection of a large amount of labeled data, and human expertise. Thus, DNN models constitute intellectual property (IP) and business value for their owners. Embedding digital watermarks into model training allows model owners to later demonstrate ownership, which can effectively protect the IP of their models. Recently, federated learning has been proposed as a new framework for machine learning development, which distributes the training of a global deep neural network (DNN) model over a large number of participants. Therefore, federated learning is advantageous than traditional DNN training in terms of data privacy, computational resources and a distributed optimization. However, there is no prior work investigating a solution for watermarking federated DNN models. The main challenge is that the distributed training causes the separation of training data (on participants’ side) and watermark set (on aggregator’s side), which does not satisfy the condition of traditional watermarking techniques that requires both training data and watermark set to be stored in the same place. In this thesis, we introduce two novel federated watermarking approaches which can embed watermark into federated DNN models by backdooring with low communication and computational overhead. In our approaches, the embedding of watermark is completed by the aggregator while the training is done by participants. We prove that our approaches embed a watermark with a high accuracy (100\%) while keeping the functionality of the model. Moreover, the embedded watermarks in DNN models are resistant to post-processing techniques. We also propose a new watermark generation method and evaluate its efficacy in terms of unremovability, model utility and computational cost aspects.},
	language = {en},
	school = {Aalto University School of Science},
	author = {Xia, Yuxi},
	month = jan,
	year = {2020},
	keywords = {duplicate, master thesis},
}

@misc{aprilpyone_training_2020,
	title = {Training {DNN} {Model} with {Secret} {Key} for {Model} {Protection}},
	url = {http://arxiv.org/abs/2008.02450},
	abstract = {In this paper, we propose a model protection method by using block-wise pixel shufﬂing with a secret key as a preprocessing technique to input images for the ﬁrst time. The protected model is built by training with such preprocessed images. Experiment results show that the performance of the protected model is close to that of non-protected models when the key is correct, while the accuracy is severely dropped when an incorrect key is given, and the proposed model protection is robust against not only brute-force attacks but also ﬁne-tuning attacks, while maintaining almost the same performance accuracy as that of using a non-protected model.},
	language = {en},
	urldate = {2020-09-28},
	author = {AprilPyone, MaungMaung and Kiya, Hitoshi},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.02450},
	keywords = {DNN, PROACTIVE IPP, done rudi},
}

@misc{skripniuk_black-box_2020,
	title = {Black-{Box} {Watermarking} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2007.08457},
	abstract = {As companies start using deep learning to provide value to their customers, the demand for solutions to protect the ownership of trained models becomes evident. Several watermarking approaches have been proposed for protecting discriminative models. However, rapid progress in the task of photorealistic image synthesis, boosted by Generative Adversarial Networks (GANs), raises an urgent need for extending protection to generative models. We propose the ﬁrst watermarking solution for GAN models. We leverage steganography techniques to watermark GAN training dataset, transfer the watermark from dataset to GAN models, and then verify the watermark from generated images. In the experiments, we show that the hidden encoding characteristic of steganography allows preserving generation quality and supports the watermark secrecy against steganalysis attacks. We validate that our watermark veriﬁcation is robust in wide ranges against several image and model perturbation attacks. Critically, our solution treats GAN models as an independent component: watermark embedding is agnostic to GAN details and watermark veriﬁcation relies only on accessing the APIs of black-box GANs. We further extend our watermarking applications to generated image detection and attribution, which delivers a practical potential to facilitate forensics against deep fakes and responsibility tracking of GAN misuse.},
	language = {en},
	urldate = {2020-09-28},
	author = {Skripniuk, Vladislav and Yu, Ning and Abdelnabi, Sahar and Fritz, Mario},
	month = aug,
	year = {2020},
	note = {arXiv: 2007.08457},
	keywords = {GANs, WATERMARKING, done, other ML},
}

@misc{xu_novel_2019,
	title = {A novel method for identifying the deep neural network model with the {Serial} {Number}},
	url = {http://arxiv.org/abs/1911.08053},
	abstract = {Deep neural network (DNN) with the state of art performance has emerged as a viable and lucrative business service. However, those impressive performances require a large number of computational resources, which comes at a high cost for the model creators. The necessity for protecting DNN models from illegal reproducing and distribution appears salient now. Recently, trigger-set watermarking, breaking the white-box restriction, relying on adversarial training pre-deﬁned (incorrect) labels for crafted inputs, and subsequently using them to verify the model authenticity, has been the main topic of DNN ownership veriﬁcation. While these methods have successfully demonstrated robustness against removal attacks, few are effective against the tampering attacks from competitors forging the fake watermarks and dogging in the manager. In this paper, we put forth a new framework of the trigger-set watermark by embedding a unique Serial Number (relatedness less original labels) to the deep neural network for model ownership identiﬁcation, which is both robust to model pruning and resist to tampering attacks. Experiment results demonstrate that the DNN Serial Number only incurs slight accuracy degradation of the original performance and is valid for ownership veriﬁcation.},
	language = {en},
	urldate = {2020-09-28},
	author = {Xu, XiangRui and Li, YaQin and Yuan, Cao},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.08053},
	keywords = {duplicate},
}

@misc{guo_hidden_2020,
	title = {The {Hidden} {Vulnerability} of {Watermarking} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2009.08697},
	abstract = {Watermarking has shown its effectiveness in protecting the intellectual property of Deep Neural Networks (DNNs). Existing techniques usually embed a set of carefully-crafted sample-label pairs into the target model during the training process. Then ownership veriﬁcation is performed by querying a suspicious model with those watermark samples and checking the prediction results. These watermarking solutions claim to be robustness against model transformations, which is challenged by this paper. We design a novel watermark removal attack, which can defeat state-of-the-art solutions without any prior knowledge of the adopted watermarking technique and training samples. We make two contributions in the design of this attack. First, we propose a novel preprocessing function, which embeds imperceptible patterns and performs spatial-level transformations over the input. This function can make the watermark sample unrecognizable by the watermarked model, while still maintaining the correct prediction results of normal samples. Second, we introduce a ﬁne-tuning strategy using unlabelled and out-ofdistribution samples, which can improve the model usability in an efﬁcient manner. Extensive experimental results indicate that our proposed attack can effectively bypass existing watermarking solutions with very high success rates.},
	language = {en},
	urldate = {2020-09-28},
	author = {Guo, Shangwei and Zhang, Tianwei and Qiu, Han and Zeng, Yi and Xiang, Tao and Liu, Yang},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.08697},
	keywords = {ATTACKS WM, done},
}

@misc{liu_removing_2020,
	title = {Removing {Backdoor}-{Based} {Watermarks} in {Neural} {Networks} with {Limited} {Data}},
	url = {http://arxiv.org/abs/2008.00407},
	abstract = {Deep neural networks have been widely applied and achieved great success in various ﬁelds. As training deep models usually consumes massive data and computational resources, trading the trained deep models is highly-demanded and lucrative nowadays. Unfortunately, the naive trading schemes typically involves potential risks related to copyright and trustworthiness issues, e.g., a sold model can be illegally resold to others without further authorization to reap huge proﬁts. To tackle this problem, various watermarking techniques are proposed to protect the model intellectual property, amongst which the backdoorbased watermarking is the most commonly-used one. However, the robustness of these watermarking approaches is not well evaluated under realistic settings, such as limited in-distribution data availability and agnostic of watermarking patterns. In this paper, we benchmark the robustness of watermarking, and propose a novel backdoor-based watermark removal framework using limited data, dubbed WILD. The proposed WILD removes the watermarks of deep models with only a small portion of training data, and the output model can perform the same as models trained from scratch without watermarks injected. In particular, a novel data augmentation method is utilized to mimic the behavior of watermark triggers. Combining with the distribution alignment between the normal and perturbed (e.g., occluded) data in the feature space, our approach generalizes well on all typical types of trigger contents. The experimental results demonstrate that our approach can effectively remove the watermarks without compromising the deep model performance for the original task with the limited access to training data.},
	language = {en},
	urldate = {2020-09-28},
	author = {Liu, Xuankai and Li, Fengting and Wen, Bihan and Li, Qi},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.00407},
	keywords = {ATTACKS WM, done},
}

@misc{lukas_deep_2020,
	title = {Deep {Neural} {Network} {Fingerprinting} by {Conferrable} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1912.00888},
	abstract = {In Machine Learning as a Service, a provider trains a deep neural network and provides many users access to it. However, the hosted (source) model is susceptible to model stealing attacks where an adversary derives a surrogate model from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model or not. We propose a ﬁngerprinting method for deep neural networks that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classiﬁcation of such inputs. These inputs are a speciﬁcally crafted subclass of targeted transferable adversarial examples which we call conferrable adversarial examples that transfer exclusively from a source model to its surrogates. We propose new methods to generate these conferrable adversarial examples and use them as our ﬁngerprint. Our ﬁngerprint is the ﬁrst to be successfully tested as robust against distillation attacks, and our experiments show that this robustness extends to robustness against weaker removal attacks such as ﬁne-tuning, ensemble attacks, adversarial training and stronger adaptive attacks speciﬁcally designed against our ﬁngerprint. We even protect against a powerful adversary with white-box access to the source model, whereas the defender only needs black-box access to the surrogate model. We conduct our experiments on the CINIC dataset, which is a superset of CIFAR-10, and a subset of ImageNet32 with 100 classes. Our experiments show that our ﬁngerprint perfectly separates surrogate and reference models. We measure a ﬁngerprint retention of 100\% in all evaluated attacks for surrogate models that have at most a difference in test accuracy of ﬁve percentage points to the source model.},
	language = {en},
	urldate = {2020-09-28},
	author = {Lukas, Nils and Zhang, Yuxuan and Kerschbaum, Florian},
	month = feb,
	year = {2020},
	note = {arXiv: 1912.00888},
	keywords = {DNN, FINGERPRINTING, done},
}

@misc{cao_ipguard_2020,
	title = {{IPGuard}: {Protecting} {Intellectual} {Property} of {Deep} {Neural} {Networks} via {Fingerprinting} the {Classification} {Boundary}},
	shorttitle = {{IPGuard}},
	url = {http://arxiv.org/abs/1910.12903},
	abstract = {A deep neural network (DNN) classiﬁer represents a model owner’s intellectual property as training a DNN classiﬁer often requires lots of resource. Watermarking was recently proposed to protect the intellectual property of DNN classiﬁers. However, watermarking suﬀers from a key limitation: it sacriﬁces the utility/accuracy of the model owner’s classiﬁer because it tampers the classiﬁer’s training or ﬁne-tuning process. In this work, we propose IPGuard, the ﬁrst method to protect intellectual property of DNN classiﬁers that provably incurs no accuracy loss for the classiﬁers. Our key observation is that a DNN classiﬁer can be uniquely represented by its classiﬁcation boundary. Based on this observation, IPGuard extracts some data points near the classiﬁcation boundary of the model owner’s classiﬁer and uses them to ﬁngerprint the classiﬁer. A DNN classiﬁer is said to be a pirated version of the model owner’s classiﬁer if they predict the same labels for most ﬁngerprinting data points. IPGuard is qualitatively diﬀerent from watermarking. Speciﬁcally, IPGuard extracts ﬁngerprinting data points near the classiﬁcation boundary of a classiﬁer that is already trained, while watermarking embeds watermarks into a classiﬁer during its training or ﬁne-tuning process. We extensively evaluate IPGuard on CIFAR-10, CIFAR-100, and ImageNet datasets. Our results show that IPGuard can robustly identify post-processed versions of the model owner’s classiﬁer as pirated versions of the classiﬁer, and IPGuard can identify classiﬁers, which are not the model owner’s classiﬁer nor its post-processed versions, as non-pirated versions of the classiﬁer.},
	language = {en},
	urldate = {2020-09-28},
	author = {Cao, Xiaoyu and Jia, Jinyuan and Gong, Neil Zhenqiang},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.12903},
	keywords = {done, model verification - no IP protection},
}

@misc{lim_protect_2020,
	title = {Protect, {Show}, {Attend} and {Tell}: {Image} {Captioning} {Model} with {Ownership} {Protection}},
	shorttitle = {Protect, {Show}, {Attend} and {Tell}},
	url = {http://arxiv.org/abs/2008.11009},
	abstract = {By and large, existing Intellectual Property Right (IPR) protection on deep neural networks typically i) focus on image classiﬁcation task only, and ii) follow a standard digital watermarking framework that were conventionally used to protect the ownership of multimedia and video content. This paper demonstrates that current digital watermarking framework is insufﬁcient to protect image captioning task that often regarded as one of the frontier A.I. problems. As a remedy, this paper studies and proposes two different embedding schemes in the hidden memory state of a recurrent neural network to protect image captioning model. From both theoretically and empirically points, we prove that a forged key will yield an unusable image captioning model, defeating the purpose on infringement. To the best of our knowledge, this work is the ﬁrst to propose ownership protection on image captioning task. Also, extensive experiments show that the proposed method does not compromise the original image captioning performance on all common captioning metrics on Flickr30k and MS-COCO datasets, and at the same time it is able to withstand both removal and ambiguity attacks.},
	language = {en},
	urldate = {2020-09-28},
	author = {Lim, Jian Han and Chan, Chee Seng and Ng, Kam Woh and Fan, Lixin and Yang, Qiang},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.11009},
	keywords = {WATERMARKING, done, image captioning, other ML},
}

@misc{alam_deep-lock_2020,
	title = {Deep-{Lock}: {Secure} {Authorization} for {Deep} {Neural} {Networks}},
	shorttitle = {Deep-{Lock}},
	url = {http://arxiv.org/abs/2008.05966},
	abstract = {Trained Deep Neural Network (DNN) models are considered valuable Intellectual Properties (IP) in several business models. Prevention of IP theft and unauthorized usage of such DNN models has been raised as of significant concern by industry. In this paper, we address the problem of preventing unauthorized usage of DNN models by proposing a generic and lightweight key-based model-locking scheme, which ensures that a locked model functions correctly only upon applying the correct secret key. The proposed scheme, known as Deep-Lock, utilizes S-Boxes with good security properties to encrypt each parameter of a trained DNN model with secret keys generated from a master key via a key scheduling algorithm. The resulting dense network of encrypted weights is found robust against model fine-tuning attacks. Finally, Deep-Lock does not require any intervention in the structure and training of the DNN models, making it applicable for all existing software and hardware implementations of DNN.},
	language = {en},
	urldate = {2020-09-28},
	author = {Alam, Manaar and Saha, Sayandeep and Mukhopadhyay, Debdeep and Kundu, Sandip},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.05966},
	keywords = {DNN, PROACTIVE IPP, done rudi},
}

@misc{shafieinejad_robustness_2019,
	title = {On the {Robustness} of the {Backdoor}-based {Watermarking} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.07745},
	abstract = {Obtaining the state of the art performance of deep learning models imposes a high cost to model generators, due to the tedious data preparation and the substantial processing requirements. To protect the model from unauthorized re-distribution, watermarking approaches have been introduced in the past couple of years. We investigate the robustness and reliability of state-of-the-art deep neural network watermarking schemes. We focus on backdoor-based watermarking and propose two – a black-box and a white-box – attacks that remove the watermark. Our black-box attack steals the model and removes the watermark with minimum requirements; it just relies on public unlabeled data and a black-box access to the classiﬁcation label. It does not need classiﬁcation conﬁdences or access to the model’s sensitive information such as the training data set, the trigger set or the model parameters. The white-box attack, proposes an efﬁcient watermark removal when the parameters of the marked model are available; our white-box attack does not require access to the labeled data or the trigger set and improves the runtime of the black-box attack up to seventeen times. We as well prove the security inadequacy of the backdoor-based watermarking in keeping the watermark undetectable by proposing an attack that detects whether a model contains a watermark. Our attacks show that a recipient of a marked model can remove a backdoor-based watermark with signiﬁcantly less effort than training a new model and some other techniques are needed to protect against re-distribution by a motivated attacker.},
	language = {en},
	urldate = {2020-09-28},
	author = {Shafieinejad, Masoumeh and Wang, Jiaqi and Lukas, Nils and Li, Xinda and Kerschbaum, Florian},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.07745},
	keywords = {ATTACKS WM, done},
}

@misc{szyller_dawn_2020,
	title = {{DAWN}: {Dynamic} {Adversarial} {Watermarking} of {Neural} {Networks}},
	shorttitle = {{DAWN}},
	url = {http://arxiv.org/abs/1906.00830},
	abstract = {Training machine learning (ML) models is expensive in terms of computational power, amounts of labeled data and human expertise. Thus, ML models constitute intellectual property (IP) and business value for their owners. Embedding digital watermarks during model training allows a model owner to later identify their models in case of theft or misuse. However, model functionality can also be stolen via model extraction, where an adversary trains a surrogate model using results returned from a prediction API of the original model. Recent work has shown that model extraction is a realistic threat. Existing watermarking schemes are ineffective against IP theft via model extraction since it is the adversary who trains the surrogate model. In this paper, we introduce DAWN (Dynamic Adversarial Watermarking of Neural Networks), the first approach to use watermarking to deter model extraction IP theft. Unlike prior watermarking schemes, DAWN does not impose changes to the training process but it operates at the prediction API of the protected model, by dynamically changing the responses for a small subset of queries (e.g., {\textless}0.5\%) from API clients. This set is a watermark that will be embedded in case a client uses its queries to train a surrogate model. We show that DAWN is resilient against two state-of-the-art model extraction attacks, effectively watermarking all extracted surrogate models, allowing model owners to reliably demonstrate ownership (with confidence \${\textgreater}1- 2{\textasciicircum}\{-64\}\$), incurring negligible loss of prediction accuracy (0.03-0.5\%).},
	urldate = {2020-09-25},
	author = {Szyller, Sebastian and Atli, Buse Gul and Marchal, Samuel and Asokan, N.},
	month = jun,
	year = {2020},
	note = {arXiv: 1906.00830},
	keywords = {DNN, WATERMARKING, done},
}

@misc{chen_performance_2018,
	title = {Performance {Comparison} of {Contemporary} {DNN} {Watermarking} {Techniques}},
	url = {http://arxiv.org/abs/1811.03713},
	abstract = {DNNs shall be considered as the intellectual property (IP) of the model builder due to the impeding cost of designing/training a highly accurate model. Research attempts have been made to protect the authorship of the trained model and prevent IP infringement using DNN watermarking techniques. In this paper, we provide a comprehensive performance comparison of the state-of-the-art DNN watermarking methodologies according to the essential requisites for an effective watermarking technique. We identify the pros and cons of each scheme and provide insights into the underlying rationale. Empirical results corroborate that DeepSigns framework proposed in [4] has the best overall performance in terms of the evaluation metrics. Our comparison facilitates the development of pending watermarking approaches and enables the model owner to deploy the watermarking scheme that satisfying her requirements.},
	language = {en},
	urldate = {2020-09-25},
	author = {Chen, Huili and Rouhani, Bita Darvish and Fan, Xinwei and Kilinc, Osman Cihan and Koushanfar, Farinaz},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.03713},
	keywords = {DNN, done, secondary literature, watermarking},
}

@misc{fan_digital_2019,
	title = {Digital {Passport}: {A} {Novel} {Technological} {Strategy} for {Intellectual} {Property} {Protection} of {Convolutional} {Neural} {Networks}},
	shorttitle = {Digital {Passport}},
	url = {http://arxiv.org/abs/1905.04368},
	abstract = {In order to prevent deep neural networks from being infringed by unauthorized parties, we propose a generic solution which embeds a designated digital passport into a network, and subsequently, either paralyzes the network functionalities for unauthorized usages or maintain its functionalities in the presence of a verified passport. Such a desired network behavior is successfully demonstrated in a number of implementation schemes, which provide reliable, preventive and timely protections against tens of thousands of fake-passport deceptions. Extensive experiments also show that the deep neural network performance under unauthorized usages deteriorate significantly (e.g. with 33\% to 82\% reductions of CIFAR10 classification accuracies), while networks endorsed with valid passports remain intact.},
	urldate = {2020-09-14},
	author = {Fan, Lixin and Ng, KamWoh and Chan, Chee Seng},
	month = may,
	year = {2019},
	note = {arXiv: 1905.04368},
	keywords = {DNN, done rudi, duplicate},
}

@misc{li_piracy_2020,
	title = {Piracy {Resistant} {Watermarks} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1910.01226},
	abstract = {As companies continue to invest heavily in larger, more accurate and more robust deep learning models, they are exploring approaches to monetize their models while protecting their intellectual property. Model licensing is promising, but requires a robust tool for owners to claim ownership of models, i.e. a watermark. Unfortunately, current designs have not been able to address piracy attacks, where third parties falsely claim model ownership by embedding their own "pirate watermarks" into an already-watermarked model. We observe that resistance to piracy attacks is fundamentally at odds with the current use of incremental training to embed watermarks into models. In this work, we propose null embedding, a new way to build piracy-resistant watermarks into DNNs that can only take place at a model's initial training. A null embedding takes a bit string (watermark value) as input, and builds strong dependencies between the model's normal classification accuracy and the watermark. As a result, attackers cannot remove an embedded watermark via tuning or incremental training, and cannot add new pirate watermarks to already watermarked models. We empirically show that our proposed watermarks achieve piracy resistance and other watermark properties, over a wide range of tasks and models. Finally, we explore a number of adaptive counter-measures, and show our watermark remains robust against a variety of model modifications, including model fine-tuning, compression, and existing methods to detect/remove backdoors. Our watermarked models are also amenable to transfer learning without losing their watermark properties.},
	urldate = {2020-09-14},
	author = {Li, Huiying and Wenger, Emily and Zhao, Ben Y. and Zheng, Haitao},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.01226},
	keywords = {ATTACKS WM, DNN, WATERMARKING, black-box access, done, incl implementation},
}

@book{brownlee_basics_2018,
	title = {Basics of {Linear} {Algebra} for {Machine} {Learning}},
	language = {en},
	author = {Brownlee, Jason},
	year = {2018},
}

@misc{goodfellow_empirical_2015,
	title = {An {Empirical} {Investigation} of {Catastrophic} {Forgetting} in {Gradient}-{Based} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1312.6211},
	abstract = {Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models “forget” how to perform the ﬁrst task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the eﬀect of the relationship between the ﬁrst task and the second task on catastrophic forgetting. We ﬁnd that it is always best to train using the dropout algorithm–the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoﬀ curve between these two extremes. We ﬁnd that different tasks and relationships between tasks result in very diﬀerent rankings of activation function performance. This suggests that the choice of activation function should always be cross-validated.},
	language = {en},
	urldate = {2020-12-01},
	author = {Goodfellow, Ian J. and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
	month = mar,
	year = {2015},
	note = {arXiv: 1312.6211},
}

@article{storn_differential_1997,
	title = {Differential {Evolution} – {A} {Simple} and {Efﬁcient} {Heuristic} for {Global} {Optimization} over {Continuous} {Spaces}},
	volume = {11},
	abstract = {A new heuristic approach for minimizing possibly nonlinear and non-differentiable continuous space functions is presented. By means of an extensive testbed it is demonstrated that the new method converges faster and with more certainty than many other acclaimed global optimization methods. The new method requires few control variables, is robust, easy to use, and lends itself very well to parallel computation.},
	language = {en},
	journal = {Journal of global optimization},
	author = {Storn, Rainer},
	year = {1997},
	pages = {341--359},
}

@misc{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	url = {http://arxiv.org/abs/1905.00414},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	language = {en},
	urldate = {2020-12-01},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = jul,
	year = {2019},
	note = {arXiv: 1905.00414},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can signiﬁcantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ﬁne-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	language = {en},
	urldate = {2020-12-01},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
}

@article{halder_watermarking_2010,
	title = {Watermarking {Techniques} for {Relational} {Databases}: {Survey}, {Classiﬁcation} and {Comparison}},
	volume = {16},
	abstract = {Digital watermarking for relational databases emerged as a candidate solution to provide copyright protection, tamper detection, traitor tracing, maintaining integrity of relational data. Many watermarking techniques have been proposed in the literature to address these purposes. In this paper, we survey the current state-of-theart and we classify them according to their intent, the way they express the watermark, the cover type, the granularity level, and their veriﬁability.},
	language = {en},
	number = {21},
	journal = {Journal of Universal Computer Science},
	author = {Halder, Raju and Pal, Shantanu and Cortesi, Agostino},
	year = {2010},
	pages = {3164--3190},
}

@misc{zagoruyko_wide_2017,
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and signiﬁcant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
	language = {en},
	urldate = {2021-08-02},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = jun,
	year = {2017},
	note = {arXiv: 1605.07146},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{darlow_cinic-10_2018,
	title = {{CINIC}-10 is not {ImageNet} or {CIFAR}-10},
	url = {http://arxiv.org/abs/1810.03505},
	abstract = {In this brief technical report we introduce the CINIC-10 dataset as a plug-in extended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with images selected and downsampled from the ImageNet database. We present the approach to compiling the dataset, illustrate the example images for different classes, give pixel distributions for each part of the repository, and give some standard benchmarks for well known models. Details for download, usage, and compilation can be found in the associated github repository. 1.},
	language = {en},
	urldate = {2021-08-18},
	author = {Darlow, Luke N. and Crowley, Elliot J. and Antoniou, Antreas and Storkey, Amos J.},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.03505},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2021-07-06},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhao_watermarking_2020,
	title = {Watermarking {Graph} {Neural} {Networks} by {Random} {Graphs}},
	url = {http://arxiv.org/abs/2011.00512},
	abstract = {Many learning tasks require us to deal with graph data which contains rich relational information among elements, leading increasing graph neural network (GNN) models to be deployed in industrial products for improving the quality of service. However, they also raise challenges to model authentication. It is necessary to protect the ownership of the GNN models, which motivates us to present a watermarking method to GNN models in this paper. In the proposed method, an Erdos-Renyi (ER) random graph with random node feature vectors and labels is randomly generated as a trigger to train the GNN to be protected together with the normal samples. During model training, the secret watermark is embedded into the label predictions of the ER graph nodes. During model veriﬁcation, by activating a marked GNN with the trigger ER graph, the watermark can be reconstructed from the output to verify the ownership. Since the ER graph was randomly generated, by feeding it to a non-marked GNN, the label predictions of the graph nodes are random, resulting in a low false alarm rate (of the proposed work). Experimental results have also shown that, the performance of a marked GNN on its original task will not be impaired. Moreover, it is robust against model compression and ﬁne-tuning, which has shown the superiority and applicability.},
	language = {en},
	urldate = {2020-12-01},
	author = {Zhao, Xiangyu and Wu, Hanzhou and Zhang, Xinpeng},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.00512},
	keywords = {WATERMARKING, neu neu},
}

@misc{sablayrolles_radioactive_2020,
	title = {Radioactive data: tracing through training},
	shorttitle = {Radioactive data},
	url = {http://arxiv.org/abs/2002.00937},
	abstract = {We want to detect whether a particular image dataset has been used to train a model. We propose a new technique, radioactive data, that makes imperceptible changes to this dataset such that any model trained on it will bear an identiﬁable mark. The mark is robust to strong variations such as different architectures or optimization methods. Given a trained model, our technique detects the use of radioactive data and provides a level of conﬁdence (p-value).},
	language = {en},
	urldate = {2020-11-06},
	author = {Sablayrolles, Alexandre and Douze, Matthijs and Schmid, Cordelia and Jégou, Hervé},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.00937},
	keywords = {rudis input},
}

@misc{wang_riga_2020,
	title = {{RIGA}: {Covert} and {Robust} {White}-{Box} {Watermarking} of {Deep} {Neural} {Networks}},
	shorttitle = {{RIGA}},
	url = {http://arxiv.org/abs/1910.14268},
	abstract = {Watermarking of deep neural networks (DNN) can enable their tracing once released by a data owner. In this paper, we generalize white-box watermarking algorithms for DNNs, where the data owner needs white-box access to the model to extract the watermark. White-box watermarking algorithms have the advantage that they do not impact the accuracy of the watermarked model. We propose Robust whIte-box GAn watermarking (RIGA), a novel white-box watermarking algorithm that uses adversarial training. Our extensive experiments demonstrate that the proposed watermarking algorithm not only does not impact accuracy, but also significantly improves the covertness and robustness over the current state-of-art.},
	language = {en},
	urldate = {2020-12-01},
	author = {Wang, Tianhao and Kerschbaum, Florian},
	month = oct,
	year = {2020},
	note = {arXiv: 1910.14268 [v3]},
	keywords = {NEU, WATERMARKING, neu neu},
}

@misc{tang_deep_2020,
	title = {Deep {Serial} {Number}: {Computational} {Watermarking} for {DNN} {Intellectual} {Property} {Protection}},
	shorttitle = {Deep {Serial} {Number}},
	url = {http://arxiv.org/abs/2011.08960},
	abstract = {In this paper, we introduce DSN (Deep Serial Number), a new watermarking approach that can prevent the stolen model from being deployed by unauthorized parties. Recently, watermarking in DNNs has emerged as a new research direction for owners to claim ownership of DNN models. However, the veriﬁcation schemes of existing watermarking approaches are vulnerable to various watermark attacks. Different from existing work that embeds identiﬁcation information into DNNs, we explore a new DNN Intellectual Property Protection mechanism that can prevent adversaries from deploying the stolen deep neural networks. Motivated by the success of serial number in protecting conventional software IP, we introduce the ﬁrst attempt to embed a serial number into DNNs. Speciﬁcally, the proposed DSN is implemented in the knowledge distillation framework, where a private teacher DNN is ﬁrst trained, then its knowledge is distilled and transferred to a series of customized student DNNs. During the distillation process, each customer DNN is augmented with a unique serial number, i.e., an encrypted 0/1 bit trigger pattern. Customer DNN works properly only when a potential customer enters the valid serial number. The embedded serial number could be used as a strong watermark for ownership veriﬁcation. Experiments on various applications indicate that DSN is effective in terms of preventing unauthorized application while not sacriﬁcing the original DNN performance. The experimental analysis further shows that DSN is resistant to different categories of attacks.},
	language = {en},
	urldate = {2020-12-01},
	author = {Tang, Ruixiang and Du, Mengnan and Hu, Xia},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.08960},
	keywords = {PROACTIVE IPP, neu neu},
}

@misc{atli_waffle_2020,
	title = {{WAFFLE}: {Watermarking} in {Federated} {Learning}},
	shorttitle = {{WAFFLE}},
	url = {http://arxiv.org/abs/2008.07298},
	abstract = {Creators of machine learning models can use watermarking as a technique to demonstrate their ownership if their models are stolen. Several recent proposals watermark deep neural network (DNN) models using backdooring: training them with additional mislabeled data. Backdooring requires full access to the training data and control of the training process. This is feasible when a single party trains the model in a centralized manner, but not in a federated learning setting where the training process and training data are distributed among several parties. In this paper, we introduce WAFFLE, the ﬁrst approach to watermark DNN models in federated learning. It introduces a re-training step after each aggregation of local models into the global model. We show that WAFFLE efﬁciently embeds a resilient watermark into models with a negligible test accuracy degradation (−0.17\%), and does not require access to the training data. We introduce a novel technique to generate the backdoor used as a watermark. It outperforms prior techniques, imposing no communication, and low computational (+2.8\%) overhead.},
	language = {en},
	urldate = {2020-09-28},
	author = {Atli, Buse Gul and Xia, Yuxi and Marchal, Samuel and Asokan, N.},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.07298},
	keywords = {WATERMARKING, done, federated learning, other ML},
}

@misc{aiken_neural_2020,
	title = {Neural {Network} {Laundering}: {Removing} {Black}-{Box} {Backdoor} {Watermarks} from {Deep} {Neural} {Networks}},
	shorttitle = {Neural {Network} {Laundering}},
	url = {http://arxiv.org/abs/2004.11368},
	abstract = {Creating a state-of-the-art deep-learning system requires vast amounts of data, expertise, and hardware, yet research into embedding copyright protection for neural networks has been limited. One of the main methods for achieving such protection involves relying on the susceptibility of neural networks to backdoor attacks, but the robustness of these tactics has been primarily evaluated against pruning, ﬁne-tuning, and model inversion attacks. In this work, we propose a neural network “laundering” algorithm to remove black-box backdoor watermarks from neural networks even when the adversary has no prior knowledge of the structure of the watermark. We are able to effectively remove watermarks used for recent defense or copyright protection mechanisms while achieving test accuracies above 97\% and 80\% for both MNIST and CIFAR-10, respectively. For all backdoor watermarking methods addressed in this paper, we ﬁnd that the robustness of the watermark is significantly weaker than the original claims. We also demonstrate the feasibility of our algorithm in more complex tasks as well as in more realistic scenarios where the adversary is able to carry out efﬁcient laundering attacks using less than 1\% of the original training set size, demonstrating that existing backdoor watermarks are not sufﬁcient to reach their claims.},
	language = {en},
	urldate = {2020-09-28},
	author = {Aiken, William and Kim, Hyoungshick and Woo, Simon},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.11368},
	keywords = {ATTACKS WM, done},
}

@misc{jia_entangled_2020,
	title = {Entangled {Watermarks} as a {Defense} against {Model} {Extraction}},
	url = {http://arxiv.org/abs/2002.12200},
	abstract = {Machine learning involves expensive data collection and training procedures. Model owners may be concerned that valuable intellectual property can be leaked if adversaries mount model extraction attacks. Because it is difﬁcult to defend against model extraction without sacriﬁcing signiﬁcant prediction accuracy, watermarking leverages unused model capacity to have the model overﬁt to outlier input-output pairs, which are not sampled from the task distribution and are only known to the defender. The defender then demonstrates knowledge of the input-output pairs to claim ownership of the model at inference. The effectiveness of watermarks remains limited because they are distinct from the task distribution and can thus be easily removed through compression or other forms of knowledge transfer.},
	language = {en},
	urldate = {2020-09-28},
	author = {Jia, Hengrui and Choquette-Choo, Christopher A. and Papernot, Nicolas},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.12200},
	keywords = {DNN, WATERMARKING, done, incl implementation},
}

@misc{chen_refit_2020,
	title = {{REFIT}: a {Unified} {Watermark} {Removal} {Framework} for {Deep} {Learning} {Systems} with {Limited} {Data}},
	shorttitle = {{REFIT}},
	url = {http://arxiv.org/abs/1911.07205},
	abstract = {Deep neural networks (DNNs) have achieved tremendous success in various fields; however, training these models from scratch could be computationally expensive and requires a lot of training data. Recent work has explored different watermarking techniques to protect the pre-trained deep neural networks from potential copyright infringements. Although several existing techniques could effectively embed such watermarks into the DNNs, they could be vulnerable to adversaries who aim at removing the watermarks.},
	language = {en},
	urldate = {2020-09-28},
	author = {Chen, Xinyun and Wang, Wenxiao and Bender, Chris and Ding, Yiming and Jia, Ruoxi and Li, Bo and Song, Dawn},
	month = jan,
	year = {2020},
	note = {arXiv: 1911.07205},
	keywords = {ATTACKS WM, done},
}

@misc{yang_effectiveness_2019,
	title = {Effectiveness of {Distillation} {Attack} and {Countermeasure} on {Neural} {Network} {Watermarking}},
	url = {http://arxiv.org/abs/1906.06046},
	abstract = {The rise of machine learning as a service and model sharing platforms has raised the need of traitor-tracing the models and proof of authorship. Watermarking technique is the main component of existing methods for protecting copyright of models. In this paper, we show that distillation, a widely used transformation technique, is a quite effective attack to remove watermark embedded by existing algorithms. The fragility is due to the fact that distillation does not retain the watermark embedded in the model that is redundant and independent to the main learning task. We design ingrain in response to the destructive distillation. It regularizes a neural network with an ingrainer model, which contains the watermark, and forces the model to also represent the knowledge of the ingrainer. Our extensive evaluations show that ingrain is more robust to distillation attack and its robustness against other widely used transformation techniques is comparable to existing methods.},
	language = {en},
	urldate = {2020-09-28},
	author = {Yang, Ziqi and Dang, Hung and Chang, Ee-Chien},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06046},
	keywords = {ATTACKS WM, WATERMARKING, done},
}

@misc{wang_robust_2020,
	title = {Robust and {Undetectable} {White}-{Box} {Watermarks} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1910.14268},
	abstract = {Watermarking of deep neural networks (DNN) can enable their tracing once released by a data owner. In this paper we generalize white-box watermarking algorithms for DNNs, where the data owner needs white-box access to the model to extract the watermark, and attack and defend them using DNNs. White-box watermarking algorithms have the advantage that they do not impact the accuracy of the watermarked model. We demonstrate a new property inference attack using a DNN that can detect watermarking by any existing, manually designed algorithm regardless of training data set and model architecture. We then use a new training architecture and a further DNN to create a new white-box watermarking algorithm that does not impact accuracy, is undetectable and robust against moderate model transformation attacks.},
	language = {en},
	urldate = {2020-09-28},
	author = {Wang, Tianhao and Kerschbaum, Florian},
	month = mar,
	year = {2020},
	note = {arXiv: 1910.14268 [v2]},
	keywords = {ATTACKS WM, DNN, WATERMARKING, done, white-box access},
}

@misc{hitaj_have_2018,
	title = {Have {You} {Stolen} {My} {Model}? {Evasion} {Attacks} {Against} {Deep} {Neural} {Network} {Watermarking} {Techniques}},
	shorttitle = {Have {You} {Stolen} {My} {Model}?},
	url = {http://arxiv.org/abs/1809.00615},
	abstract = {Deep neural networks have had enormous impact on various domains of computer science, considerably outperforming previous state of the art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increases their construction costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information(watermark) in a given model. The watermark allows the legitimate owner to detect copyright infringements of his model. This paper focuses on verifying the robustness and reliability of state-of- the-art deep neural network watermarking schemes. We show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification by the legitimate owners, thus avoiding the detection of model theft.},
	urldate = {2020-09-14},
	author = {Hitaj, Dorjan and Mancini, Luigi V.},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.00615},
	keywords = {duplicate},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}
