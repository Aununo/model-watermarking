\begin{table}
\small
\centering
\caption{Hyperparameters configuration for the architectures. \textbf{lr} stands for learning rate, \textbf{bs} for batch size, \textbf{wm\_bs} for watermarking batch size, i.e. the batch size for the trigger set, and \textbf{epochs} the number of training iterations.}

\setlength\tabcolsep{4pt}

%\rowcolors{2}{white}{gray!15}
\begin{tabular}{|l|l|c|l|c|c|c|}
\hline
\textbf{Architecture} & \textbf{optimizer} & \textbf{lr} & \textbf{scheduler} & \textbf{bs} & \textbf{wm\_bs} & \textbf{epochs} \\ \hline
DenseNet-121   & \gape{\makecell[l]{SGD \\ mom.=0.9 \\ decay=0.0005}} & 0.1     & \gape{\makecell[l]{CosineAnnealingLR \\ T\_max=200}} & 64 & 32 & 200      \\ \hline
ResNet-18      & \gape{\makecell[l]{SGD \\ mom.=0.9 \\ decay=0.0005}} & 0.1     & \gape{\makecell[l]{CosineAnnealingLR \\ T\_max=200}} & 64 & 32 & 192      \\ \hline
ResNet-34      & \gape{\makecell[l]{SGD \\ mom.=0.9 \\ decay=0.0005}} & 0.1     & \gape{\makecell[l]{CosineAnnealingLR \\ T\_max=200}} & 64 & 32 & 195      \\ \hline
ResNet-50      & \gape{\makecell[l]{SGD \\ mom.=0.9 \\ decay=0.0005}} & 0.1     & \gape{\makecell[l]{CosineAnnealingLR \\ T\_max=200}} & 64 & 32 & 192      \\ \hline
SimpleNet      & \gape{\makecell[l]{SGD \\ mom.=0.9 \\ decay=0.0005}} & 0.1     & \gape{\makecell[l]{MultiStepLR \\ n=20, gamma=0.1}}  & 64 & 32 & 24       \\ \hline
LeNet-1        & ADAM                                                 & 0.001   & \gape{\makecell[l]{MultiStepLR \\ n=20, gamma=0.1}}  & 64 & 32 & 48       \\ \hline
LeNet-3        & \gape{\makecell[l]{SGD \\ mom.=0.9 \\ decay=0.0005}} & 0.1     & \gape{\makecell[l]{MultiStepLR \\ n=20, gamma=0.1}}  & 64 & 32 & 55       \\ \hline
LeNet-5        & \gape{\makecell[l]{SGD \\ mom.=0.9 \\ decay=0.0005}} & 0.1     & \gape{\makecell[l]{MultiStepLR \\ n=20, gamma=0.1}}  & 64 & 32 & 42       \\ \hline
\end{tabular}
\label{tab:hyperparameter_config}
\end{table}