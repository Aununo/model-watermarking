\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Defining research questions and study setting}{49}{chapter.6}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:study_setting}{{\M@TitleReference {6}{Defining research questions and study setting}}{49}{Defining research questions and study setting}{chapter.6}{}}
\newlabel{ch:study_setting@cref}{{[chapter][6][]6}{[1][49][]49}}
\citation{namba_robust_2019}
\citation{merrer_adversarial_2019}
\citation{li_piracy_2020}
\citation{zhang_protecting_2018}
\citation{adi_turning_2018}
\citation{guo_evolutionary_2019}
\citation{chen_performance_2018}
\citation{uchida_embedding_2017}
\citation{rouhani_deepsigns_2019}
\citation{zhang_protecting_2018}
\citation{adi_turning_2018}
\citation{merrer_adversarial_2019}
\citation{rouhani_deepsigns_2019}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Study settings in selected papers.\relax }}{51}{table.caption.32}\protected@file@percent }
\newlabel{tab:models-and-datasets}{{\M@TitleReference {6.1}{Study settings in selected papers.\relax }}{51}{Study settings in selected papers.\relax }{table.caption.32}{}}
\newlabel{tab:models-and-datasets@cref}{{[table][1][6]6.1}{[1][50][]51}}
\citation{lecun_gradient-based_1998}
\citation{krizhevsky_learning_2009}
\citation{cohen_emnist_2017}
\citation{grother_NIST_1970}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Datasets}{52}{section.6.1}\protected@file@percent }
\citation{darlow_cinic-10_2018}
\citation{russakovsky_imagenet_2015}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Characteristics of the datasets used in the evaluation\relax }}{53}{table.caption.33}\protected@file@percent }
\newlabel{tab:datasets}{{\M@TitleReference {6.2}{Characteristics of the datasets used in the evaluation\relax }}{53}{Characteristics of the datasets used in the evaluation\relax }{table.caption.33}{}}
\newlabel{tab:datasets@cref}{{[table][2][6]6.2}{[1][53][]53}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Representative examples for each class from the training sets of MNIST, CIFAR-10, EMNIST and CINIC-10.\relax }}{53}{figure.caption.34}\protected@file@percent }
\newlabel{fig:datasets}{{\M@TitleReference {6.1}{Representative examples for each class from the training sets of MNIST, CIFAR-10, EMNIST and CINIC-10.\relax }}{53}{Representative examples for each class from the training sets of MNIST, CIFAR-10, EMNIST and CINIC-10.\relax }{figure.caption.34}{}}
\newlabel{fig:datasets@cref}{{[figure][1][6]6.1}{[1][53][]53}}
\citation{lecun_gradient-based_1998}
\citation{he_deep_2016}
\citation{huang_densely_2017}
\citation{hasanpour_lets_2018}
\citation{almezhghwi_improved_2020}
\citation{almezhghwi_improved_2020}
\citation{huang_densely_2017}
\citation{huang_densely_2017}
\citation{lecun_gradient-based_1998}
\citation{lecun_gradient-based_1998}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Neural Networks}{54}{section.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces ResNet-18 architecture. Source: \cite  {almezhghwi_improved_2020}\relax }}{55}{figure.caption.35}\protected@file@percent }
\newlabel{fig:arch-resnet18}{{\M@TitleReference {6.2}{ResNet-18 architecture. Source: \cite  {almezhghwi_improved_2020}\relax }}{55}{ResNet-18 architecture. Source: \cite {almezhghwi_improved_2020}\relax }{figure.caption.35}{}}
\newlabel{fig:arch-resnet18@cref}{{[figure][2][6]6.2}{[1][54][]55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces A 5-layer dense block. A DenseNet consists of several dense blocks. Source: \cite  {huang_densely_2017}\relax }}{55}{figure.caption.36}\protected@file@percent }
\newlabel{fig:arch-densenet}{{\M@TitleReference {6.3}{A 5-layer dense block. A DenseNet consists of several dense blocks. Source: \cite  {huang_densely_2017}\relax }}{55}{A 5-layer dense block. A DenseNet consists of several dense blocks. Source: \cite {huang_densely_2017}\relax }{figure.caption.36}{}}
\newlabel{fig:arch-densenet@cref}{{[figure][3][6]6.3}{[1][54][]55}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Architecture of LeNet-5. Source: \cite  {lecun_gradient-based_1998}\relax }}{55}{figure.caption.37}\protected@file@percent }
\newlabel{fig:lenet5}{{\M@TitleReference {6.4}{Architecture of LeNet-5. Source: \cite  {lecun_gradient-based_1998}\relax }}{55}{Architecture of LeNet-5. Source: \cite {lecun_gradient-based_1998}\relax }{figure.caption.37}{}}
\newlabel{fig:lenet5@cref}{{[figure][4][6]6.4}{[1][54][]55}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Amount of trainable parameters and the state-of-the-art test accuracy, as well as, our test accuracy of the trained models.\relax }}{56}{table.caption.39}\protected@file@percent }
\newlabel{tab:trainable_parameters}{{\M@TitleReference {6.3}{Amount of trainable parameters and the state-of-the-art test accuracy, as well as, our test accuracy of the trained models.\relax }}{56}{Amount of trainable parameters and the state-of-the-art test accuracy, as well as, our test accuracy of the trained models.\relax }{table.caption.39}{}}
\newlabel{tab:trainable_parameters@cref}{{[table][3][6]6.3}{[1][54][]56}}
\newlabel{note:lenet}{{\M@TitleReference {6.3}{Neural Networks}}{56}{Neural Networks}{table.caption.39}{}}
\newlabel{note:lenet@cref}{{[table][3][6]6.3}{[1][54][]56}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Trigger set sizes used for training models with various watermarking methods.\relax }}{56}{table.caption.40}\protected@file@percent }
\newlabel{tab:trg_set_sizes}{{\M@TitleReference {6.4}{Trigger set sizes used for training models with various watermarking methods.\relax }}{56}{Trigger set sizes used for training models with various watermarking methods.\relax }{table.caption.40}{}}
\newlabel{tab:trg_set_sizes@cref}{{[table][4][6]6.4}{[1][56][]56}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces Embedding and fine-tuning time (with learning rate 0.01) for \textit  {WeaknessIntoStrength} with 100 trigger images. The time is given in the format (hh:mm:ss).\relax }}{57}{table.caption.41}\protected@file@percent }
\newlabel{tab:training_time}{{\M@TitleReference {6.5}{Embedding and fine-tuning time (with learning rate 0.01) for \textit  {WeaknessIntoStrength} with 100 trigger images. The time is given in the format (hh:mm:ss).\relax }}{57}{Embedding and fine-tuning time (with learning rate 0.01) for \textit {WeaknessIntoStrength} with 100 trigger images. The time is given in the format (hh:mm:ss).\relax }{table.caption.41}{}}
\newlabel{tab:training_time@cref}{{[table][5][6]6.5}{[1][56][]57}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces Training times for additional experiments in format (dd:hh:mm:ss).\relax }}{57}{table.caption.42}\protected@file@percent }
\newlabel{tab:extra_exp_time}{{\M@TitleReference {6.6}{Training times for additional experiments in format (dd:hh:mm:ss).\relax }}{57}{Training times for additional experiments in format (dd:hh:mm:ss).\relax }{table.caption.42}{}}
\newlabel{tab:extra_exp_time@cref}{{[table][6][6]6.6}{[1][57][]57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Training time}{57}{subsection.6.2.1}\protected@file@percent }
\citation{guo_watermarking_2018}
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces Hyperparameters configuration for the architectures. \textbf  {lr} stands for learning rate, \textbf  {bs} for batch size, \textbf  {wm\_bs} for watermarking batch size, i.e. the batch size for the trigger set, and \textbf  {epochs} the number of training iterations.\relax }}{58}{table.caption.43}\protected@file@percent }
\newlabel{tab:hyperparameter_config}{{\M@TitleReference {6.7}{Hyperparameters configuration for the architectures. \textbf  {lr} stands for learning rate, \textbf  {bs} for batch size, \textbf  {wm\_bs} for watermarking batch size, i.e. the batch size for the trigger set, and \textbf  {epochs} the number of training iterations.\relax }}{58}{Hyperparameters configuration for the architectures. \textbf {lr} stands for learning rate, \textbf {bs} for batch size, \textbf {wm\_bs} for watermarking batch size, i.e. the batch size for the trigger set, and \textbf {epochs} the number of training iterations.\relax }{table.caption.43}{}}
\newlabel{tab:hyperparameter_config@cref}{{[table][7][6]6.7}{[1][58][]58}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Setting hyperparameters}{58}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Setting watermark-specific hyperparameters}{59}{subsection.6.3.1}\protected@file@percent }
\newlabel{sec:studysettings:watermark-spec}{{\M@TitleReference {6.3.1}{Setting watermark-specific hyperparameters}}{59}{Setting watermark-specific hyperparameters}{subsection.6.3.1}{}}
\newlabel{sec:studysettings:watermark-spec@cref}{{[subsection][1][6,3]6.3.1}{[1][58][]59}}
\@setckpt{ch6_studysettings}{
\setcounter{page}{60}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{3}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{74}
\setcounter{lastsheet}{134}
\setcounter{lastpage}{120}
\setcounter{figure}{4}
\setcounter{lofdepth}{1}
\setcounter{table}{7}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{nag@c}{1}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{Item}{8}
\setcounter{Hfootnote}{6}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{50}
\setcounter{float@type}{4}
\setcounter{r@tfl@t}{2}
\setcounter{ALC@unique}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{nlinenum}{0}
\setcounter{su@anzahl}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{section@level}{2}
}
