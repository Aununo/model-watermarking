\babel@toc {naustrian}{}
\babel@toc {naustrian}{}
\babel@toc {english}{}
\babel@toc {naustrian}{}
\babel@toc {english}{}
\babel@toc {naustrian}{}
\babel@toc {naustrian}{}
\addvspace {10pt}
\babel@toc {naustrian}{}
\babel@toc {english}{}
\addvspace {10pt}
\babel@toc {naustrian}{}
\babel@toc {english}{}
\addvspace {10pt}
\addvspace {10pt}
\addvspace {10pt}
\contentsline {table}{\numberline {3.1}{\ignorespaces Confusion matrix of a two-class problem.\relax }}{10}{table.caption.9}% 
\addvspace {10pt}
\addvspace {10pt}
\contentsline {table}{\numberline {5.1}{\ignorespaces Requirements for Watermarking techniques. The notation is not consistent throughout the papers, but the terms in the left column are the most prominent ones. These requirements mostly apply also to Fingerprinting methods\relax }}{29}{table.caption.24}% 
\contentsline {table}{\numberline {5.2}{\ignorespaces Requirements met by watermarking and fingerprinting schemes. We distinguish two degrees: $\sim $ indicates: the respective authors claim the scheme fulfils this property; \text {$\mathsurround \z@ \mathchar "458$}indicates: the authors show empirically that the property is fulfilled. \relax }}{30}{table.caption.25}% 
\contentsline {table}{\numberline {5.3}{\ignorespaces Which attack defeats which watermarking technique based on the evaluation of the papers. A $\sim $ denotes that the authors claim that their attack can be extended easily to defeat this watermarking technique but did not provide an evaluation for that.\relax }}{43}{table.caption.30}% 
\addvspace {10pt}
\contentsline {table}{\numberline {6.1}{\ignorespaces Study settings in selected papers.\relax }}{51}{table.caption.32}% 
\contentsline {table}{\numberline {6.2}{\ignorespaces Characteristics of the datasets used in the evaluation\relax }}{53}{table.caption.33}% 
\contentsline {table}{\numberline {6.3}{\ignorespaces Amount of trainable parameters and the state-of-the-art test accuracy, as well as, our test accuracy of the trained models.\relax }}{56}{table.caption.39}% 
\contentsline {table}{\numberline {6.4}{\ignorespaces Trigger set sizes used for training models with various watermarking methods.\relax }}{56}{table.caption.40}% 
\contentsline {table}{\numberline {6.5}{\ignorespaces Embedding and fine-tuning time (with learning rate 0.01) for \textit {WeaknessIntoStrength} with 100 trigger images. The time is given in the format (hh:mm:ss).\relax }}{57}{table.caption.41}% 
\contentsline {table}{\numberline {6.6}{\ignorespaces Training times for additional experiments in format (dd:hh:mm:ss).\relax }}{57}{table.caption.42}% 
\contentsline {table}{\numberline {6.7}{\ignorespaces Hyperparameters configuration for the architectures. \textbf {lr} stands for learning rate, \textbf {bs} for batch size, \textbf {wm\_bs} for watermarking batch size, i.e. the batch size for the trigger set, and \textbf {epochs} the number of training iterations.\relax }}{58}{table.caption.43}% 
\addvspace {10pt}
\contentsline {table}{\numberline {7.1}{\ignorespaces Attacks used in the papers.\relax }}{66}{table.caption.50}% 
\contentsline {table}{\numberline {7.2}{\ignorespaces Watermark accuracies after fine-tuning attack on models trained with FrontierStitching.\relax }}{74}{table.caption.61}% 
\contentsline {table}{\numberline {7.3}{\ignorespaces Results for ranking system for WMEmbeddedSystems. The points are averaged for each dataset and the bold numbers indicate the highest average for each dataset and therefore the winning $\epsilon $.\relax }}{74}{table.caption.63}% 
\contentsline {table}{\numberline {7.4}{\ignorespaces Fidelity results from Adi et al. (\cite {adi_turning_2018}, Table 1), and our experiments.\relax }}{75}{table.caption.65}% 
\contentsline {table}{\numberline {7.5}{\ignorespaces Effectiveness results from Zhang et al. (\cite {zhang_protecting_2018}, Table 1), and our results.\relax }}{76}{table.caption.68}% 
\contentsline {table}{\numberline {7.6}{\ignorespaces Pruning results from Zhang et al. (\cite {zhang_protecting_2018}, Table 3 and Table 4), compared with our results. "Test" stands for test accuracy, "WM" for watermark accuracy and "Pr. rate" for pruning rate.\relax }}{77}{table.caption.69}% 
\contentsline {table}{\numberline {7.7}{\ignorespaces Pruning results from Merrer et al. (\cite {merrer_adversarial_2019}, Table 2), and our results. The grey cells indicate a non-plausible pruning attack. For a plausible attack and watermark accuracy above 50\% the cell is green and below it is red.\relax }}{78}{table.caption.71}% 
\contentsline {table}{\numberline {7.8}{\ignorespaces Fidelity results from Guo et al. (\cite {guo_watermarking_2018}, Table 2), and our results.\relax }}{78}{table.caption.73}% 
\contentsline {table}{\numberline {7.9}{\ignorespaces Watermark accuracy on watermarked models. A checkmark \text {$\mathsurround \z@ \mathchar "458$}indicates 100\% watermark accuracy.\relax }}{79}{table.caption.74}% 
\contentsline {table}{\numberline {7.10}{\ignorespaces Influence of the trigger set size on robustness against pruning with the maximal plausible pruning rate. The values are the watermark accuracy after an pruning attack, the value in the parenthesis is the maximal plausible pruning rate. A checkmark \text {$\mathsurround \z@ \mathchar "458$}indicates 100\% watermark accuracy.\relax }}{86}{table.caption.80}% 
\addvspace {10pt}
\addvspace {10pt}
